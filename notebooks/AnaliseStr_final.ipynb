{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGLjzGqCqVOq",
        "outputId": "6d093788-fb43-45cc-f5d5-c9cbb76dcc56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xCa_qLXzgOG8"
      },
      "outputs": [],
      "source": [
        "==> aki so pra gerar erro de clicar em executar tudo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VOx1fdLrTLaH"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/pesquisa')\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from str_utils import arquivos_amostras, busca_arquivos, quebra_nome_arquivo, gravar_arquivo_saida, df_amostras, tipo_nucleotideo_map, labels_amostras, find_str, shannon_entropy, gravar_arquivo_saida, analises, pasta_analise, hamming_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdAbl5eIVj3x"
      },
      "source": [
        "1 gerar diagrama de sequencia do processo\n",
        "\n",
        "2 Por que STRs s√£o t√£o eficazes?\n",
        "\n",
        "    Alta variabilidade entre indiv√≠duos\n",
        "\n",
        "    Baixa taxa de muta√ß√£o\n",
        "\n",
        "    Facilidade de amplifica√ß√£o por PCR\n",
        "\n",
        "    Compatibilidade com sistemas multiplex, que analisam v√°rios loci simultaneamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eYo_maD5zx8",
        "outputId": "3ab62778-6de6-4718-ba71-d0c23d43cf5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/pesquisa/str_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/drive/MyDrive/pesquisa/str_utils.py\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import math\n",
        "import datetime\n",
        "\n",
        "analises = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "diretorio_saida = \"/content/drive/MyDrive/pesquisa/\"\n",
        "pasta_analise = \"\"\n",
        "pasta_analise = \"/content/drive/MyDrive/pesquisa/2025-09-04/\"\n",
        "\n",
        "labels_amostras = ['ASM285v2', 'ASM1928827v1', 'ASM2978390v1', 'ASM2978392v1', 'ASM4765177v1']\n",
        "arquivos_amostras = [\n",
        "  '/content/drive/MyDrive/fasta/aspergillus/ASM285v2/ncbi_dataset/data/GCA_000002855.2/GCA_000002855.2_ASM285v2_genomic.fna',\n",
        "  '/content/drive/MyDrive/fasta/aspergillus/ASM1928827v1/ncbi_dataset/data/GCA_019288275.1/GCA_019288275.1_ASM1928827v1_genomic.fna',\n",
        "  '/content/drive/MyDrive/fasta/aspergillus/ASM2978390v1/ncbi_dataset/data/GCA_029783905.1/GCA_029783905.1_ASM2978390v1_genomic.fna',\n",
        "  '/content/drive/MyDrive/fasta/aspergillus/ASM2978392v1/ncbi_dataset/data/GCA_029783925.1/GCA_029783925.1_ASM2978392v1_genomic.fna',\n",
        "  '/content/drive/MyDrive/fasta/aspergillus/ASM4765177v1/ncbi_dataset/data/GCA_047651775.1/GCA_047651775.1_ASM4765177v1_genomic.fna'\n",
        "]\n",
        "\n",
        "# dicion√°rio Montagem -> Refer√™ncia GenBank\n",
        "tipo_nucleotideo_map = {\n",
        "    \"mononucleot√≠deo\": 1,\n",
        "    \"dinucleot√≠deo\": 2,\n",
        "    \"trinucleot√≠deo\": 3,\n",
        "    \"tetranucleot√≠deo\": 4,\n",
        "    \"pentanucleot√≠deo\": 5,\n",
        "    \"hexanucleot√≠deo\": 6,\n",
        "    \"heptanucleot√≠deo\": 7,\n",
        "    \"octanucleot√≠deo\": 8,\n",
        "    \"nonanucleot√≠deo\": 9\n",
        "}\n",
        "\n",
        "ASM285v2_scaffolds_validos = [\n",
        "    \"NT_166518\", \"NT_166519\", \"NT_166520\", \"NT_166521\",\n",
        "    \"NT_166522\", \"NT_166523\", \"NT_166524\", \"NT_166525\",\n",
        "    \"NT_166526\", \"NT_166527\", \"NT_166528\", \"NT_166529\",\n",
        "    \"NT_166530\", \"NT_166531\", \"NT_166532\", \"NT_166533\",\n",
        "    \"NT_166537\", \"NT_166538\", \"NT_166539\"\n",
        "]\n",
        "\n",
        "# Criar dados como lista de listas\n",
        "dados = [\n",
        "    [\"ASM285v2\",     \"ASM285v2 *\",   \"GCA_000002855.2\", \"PRJNA19275\", \"CBS 513.88\",   ASM285v2_scaffolds_validos,   \"DSM, The Netherlands\",    \"Scaffold\",   34, '/content/drive/MyDrive/fasta/aspergillus/ASM285v2/ncbi_dataset/data/GCA_000002855.2/GCA_000002855.2_ASM285v2_genomic.fna'],\n",
        "    [\"ASM1928827v1\", \"ASM1928827v1\", \"GCA_019288275.1\", \"JAGRPH01\",   \"CBS 554.65\",   [], \"TU Wien\",                 \"Chromosome\", 40, '/content/drive/MyDrive/fasta/aspergillus/ASM1928827v1/ncbi_dataset/data/GCA_019288275.1/GCA_019288275.1_ASM1928827v1_genomic.fna'],\n",
        "    [\"ASM2978390v1\", \"ASM2978390v1\", \"GCA_029783905.1\", \"JAPVRD01\",   \"KJC3\",         [], \"Soongsil University\",     \"Chromosome\", 40, '/content/drive/MyDrive/fasta/aspergillus/ASM2978390v1/ncbi_dataset/data/GCA_029783905.1/GCA_029783905.1_ASM2978390v1_genomic.fna'],\n",
        "    [\"ASM2978392v1\", \"ASM2978392v1\", \"GCA_029783925.1\", \"JAPVRE01\",   \"KYF3\",         [], \"Soongsil University\",     \"Chromosome\", 37, '/content/drive/MyDrive/fasta/aspergillus/ASM2978392v1/ncbi_dataset/data/GCA_029783925.1/GCA_029783925.1_ASM2978392v1_genomic.fna'],\n",
        "    [\"ASM4765177v1\", \"ASM4765177v1\", \"GCA_047651775.1\", \"JBKZXA01\",   \"CCTCC 206047\", [], \"Zhejiang Uni.Technology\", \"Complete\",   35, '/content/drive/MyDrive/fasta/aspergillus/ASM4765177v1/ncbi_dataset/data/GCA_047651775.1/GCA_047651775.1_ASM4765177v1_genomic.fna']\n",
        "]\n",
        "\n",
        "# Criar DataFrame\n",
        "colunas = [\n",
        "    \"id\",\n",
        "    \"Nome da Montagem\",\n",
        "    \"Refer√™ncia GenBank\",\n",
        "    \"WGS Accession\",\n",
        "    \"Cepa (strain)\",\n",
        "    \"Scaffolds\",\n",
        "    \"Submissor\",\n",
        "    \"N√≠vel de montagem\",\n",
        "    \"Tamanho (Mb)\",\n",
        "    \"Dados\"\n",
        "]\n",
        "\n",
        "df_amostras = pd.DataFrame(dados, columns=colunas)\n",
        "# print(df_amostras)\n",
        "\n",
        "\n",
        "\n",
        "if len(pasta_analise) <= 0:\n",
        "  data_hora = datetime.datetime.now()\n",
        "  data_hora = data_hora.strftime('%Y-%m-%d')\n",
        "  pasta_analise = diretorio_saida + data_hora + '/'\n",
        "  # Criar a pasta se n√£o existir\n",
        "  if not os.path.exists(pasta_analise):\n",
        "      os.makedirs(pasta_analise)\n",
        "      print(f\"Pasta criada: {pasta_analise}\")\n",
        "  else:\n",
        "      print(f\"A pasta '{pasta_analise}' j√° existe!\")\n",
        "\n",
        "\n",
        "\n",
        "def gravar_arquivo_saida(nome_amostra, tamanho_str, tipo, dados):\n",
        "  url_arquivo = pasta_analise + nome_amostra + '_' + tipo+ '_' + str(tamanho_str) + '.csv'\n",
        "  if os.path.exists(url_arquivo):\n",
        "    os.remove(url_arquivo)\n",
        "  dados.to_csv(url_arquivo, sep=';', index=False)\n",
        "  # print(url_arquivo)\n",
        "  return f'{url_arquivo} ({len(dados)})'\n",
        "\n",
        "\n",
        "# buscar aquivos na pasta de analises. tipos de arquivos: STR\n",
        "def busca_arquivos(tipo, extensao=\"csv\", ordenar_por=\"nome\", reverso=False):\n",
        "  arquivos_localizados = []\n",
        "  # Percorrendo todos os diret√≥rios e subdiret√≥rios\n",
        "  for raiz, _, arquivos in os.walk(pasta_analise):\n",
        "      for nome_arquivo in arquivos:\n",
        "          # Aplicar filtros\n",
        "          if nome_arquivo.endswith(f\".{extensao}\") and tipo in nome_arquivo:\n",
        "          # if nome_arquivo.endswith(\".txt\") and tipo in nome_arquivo:\n",
        "              caminho_arquivo = os.path.join(raiz, nome_arquivo)  # Caminho completo do arquivo\n",
        "              # nome_pasta = os.path.basename(raiz)[0:5]  # Nome da pasta onde o arquivo est√°\n",
        "              arquivos_localizados.append(caminho_arquivo)\n",
        "\n",
        "  if ordenar_por == \"nome\":\n",
        "      arquivos_localizados.sort(key=lambda x: os.path.basename(x), reverse=reverso)\n",
        "  elif ordenar_por == \"data\":\n",
        "      arquivos_localizados.sort(key=lambda x: os.path.getmtime(x), reverse=reverso)\n",
        "  elif ordenar_por == \"tamanho\":\n",
        "      arquivos_localizados.sort(key=lambda x: os.path.getsize(x), reverse=reverso)\n",
        "\n",
        "  return arquivos_localizados\n",
        "# busca_arquivos('STR')\n",
        "\n",
        "\n",
        "def quebra_nome_arquivo(arquivo):\n",
        "    nome_arquivo = os.path.basename(arquivo) # Obter apenas o nome do arquivo sem o diret√≥rio\n",
        "    parte_principal = os.path.splitext(nome_arquivo)[0] # Remover a extens√£o do arquivo\n",
        "    return parte_principal.split('_')\n",
        "# quebra_nome_arquivo(busca_arquivos('STR')[0])\n",
        "\n",
        "\n",
        "\n",
        "def shannon_entropy(sequence):\n",
        "    from collections import Counter\n",
        "    # Conta as ocorr√™ncias de cada base\n",
        "    freq = Counter(sequence)\n",
        "    total = len(sequence)\n",
        "\n",
        "    # Calcula a entropia\n",
        "    entropy = 0\n",
        "    for base in freq:\n",
        "        p = freq[base] / total\n",
        "        entropy -= p * math.log2(p)\n",
        "\n",
        "    return entropy\n",
        "\n",
        "# Exemplo\n",
        "# dna_seq = \"TAATAT\"\n",
        "# entropy = shannon_entropy(dna_seq)\n",
        "# print(f\"Entropia de Shannon: {entropy:.4f}\")\n",
        "\n",
        "# testando STRs em tandem\n",
        "def find_str(sequencia_dna, tamanho_str):\n",
        "  lista = []\n",
        "  fim_da_fila = 0\n",
        "  str_localizado = False\n",
        "\n",
        "  for i in range(len(sequencia_dna)):\n",
        "    if (str_localizado and fim_da_fila > 0 and i < fim_da_fila) or (sequencia_dna[i] not in ['A', 'C', 'T', 'G']):\n",
        "      continue\n",
        "\n",
        "    amostra_str = ''\n",
        "    fim_da_fila = 0\n",
        "    if i < len(sequencia_dna)-tamanho_str+1:\n",
        "      inicio_amostra_str = i\n",
        "      fim_amostra_str = inicio_amostra_str+tamanho_str\n",
        "      amostra_str = sequencia_dna[inicio_amostra_str:fim_amostra_str]\n",
        "\n",
        "      quantidade_sequencias_repetidas = 0\n",
        "      is_testar_proxima_Sequencia = True\n",
        "      while is_testar_proxima_Sequencia:\n",
        "        inicio_proxima_sequencia = i+(tamanho_str*(quantidade_sequencias_repetidas+1))\n",
        "        fim_proxima_sequencia = inicio_proxima_sequencia+tamanho_str\n",
        "        proxima_sequencia = ''\n",
        "        if fim_proxima_sequencia <= len(sequencia_dna):\n",
        "          proxima_sequencia = sequencia_dna[inicio_proxima_sequencia:fim_proxima_sequencia]\n",
        "\n",
        "        if amostra_str == proxima_sequencia:\n",
        "          quantidade_sequencias_repetidas = quantidade_sequencias_repetidas+1\n",
        "          fim_da_fila = fim_proxima_sequencia\n",
        "        else:\n",
        "          is_testar_proxima_Sequencia = False\n",
        "      # fim while\n",
        "\n",
        "      if quantidade_sequencias_repetidas > 0:\n",
        "        str_localizado = True\n",
        "        lista.append({\n",
        "              'posicao': i+1,\n",
        "              'unidade': amostra_str,\n",
        "              'inicio_loci': inicio_amostra_str,\n",
        "              'fim_unidade': fim_amostra_str,\n",
        "              'fim_loci': fim_da_fila,\n",
        "              'copias': quantidade_sequencias_repetidas +1\n",
        "        })\n",
        "      else:\n",
        "        str_localizado = False\n",
        "  # fim for\n",
        "  df = pd.DataFrame.from_records(lista,index=['posicao'])\n",
        "  return df\n",
        "\n",
        "def busca_sequencias_fasta():\n",
        "  from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
        "\n",
        "  sequencias = []\n",
        "  for idx_amostras, arquivo in enumerate(arquivos_amostras):\n",
        "    sequencias_arquivo_sfp = ''\n",
        "    with open(arquivo) as handle:\n",
        "      for seq_id, seq in SimpleFastaParser(handle):\n",
        "        sequencias_arquivo_sfp += str(seq).upper()\n",
        "    sequencias.append(sequencias_arquivo_sfp)\n",
        "    print(f'amostra lida: {labels_amostras[idx_amostras]} - {str(len(sequencias[1]))}')\n",
        "  return sequencias\n",
        "\n",
        "def busca_sequencia_fasta(origem):\n",
        "  from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
        "\n",
        "  resultado = df_amostras.query('id == @origem')\n",
        "  if resultado.empty:\n",
        "      raise ValueError(f'Origem n√£o encontrada: \"{origem}\"')\n",
        "  arquivo = resultado['Dados'].iloc[0]\n",
        "\n",
        "  sequencias_arquivo_sfp = ''\n",
        "  with open(arquivo) as handle:\n",
        "    # print(f' > lido: {arquivo} - ({len(arquivo)})')\n",
        "    for seq_id, seq in SimpleFastaParser(handle):\n",
        "      sequencias_arquivo_sfp += str(seq).upper()\n",
        "  # print(f' > retorno [0:25]: {sequencias_arquivo_sfp[0:25]}')\n",
        "  return sequencias_arquivo_sfp\n",
        "\n",
        "\n",
        "def busca_sequencia_fasta_com_filtro(origem, target_contig=None):\n",
        "    resultado = df_amostras.query('id == @origem')\n",
        "    if resultado.empty:\n",
        "        raise ValueError(f'Origem n√£o encontrada: \"{origem}\"')\n",
        "    arquivo = resultado['Dados'].iloc[0]\n",
        "\n",
        "    sequencias_alvo = ''\n",
        "    with open(arquivo) as handle:\n",
        "        for seq_id, seq in SimpleFastaParser(handle):\n",
        "            if target_contig is None or target_contig in seq_id:\n",
        "                sequencias_alvo += str(seq).upper()\n",
        "    return sequencias_alvo\n",
        "\n",
        "\n",
        "\n",
        "def create_flank(dna_sequence, inicio, fim, tamanho_fatias=200):\n",
        "    # Garante que os √≠ndices est√£o dentro dos limites da sequ√™ncia\n",
        "    flank_length = max(tamanho_fatias, fim - inicio -1)\n",
        "\n",
        "    # Fragmento central\n",
        "    fragment = dna_sequence[inicio:fim]\n",
        "\n",
        "    # Flanqueamento anterior (forward)\n",
        "    inicio_forward = max(0, inicio - flank_length)\n",
        "    forward_flank = dna_sequence[inicio_forward:inicio]\n",
        "    # print(f'inicio_forward: {inicio_forward} forward_flank: {forward_flank} flank_length: {flank_length}')\n",
        "\n",
        "    # Flanqueamento posterior (reverse)\n",
        "    fim_reverse = min(fim + flank_length, len(dna_sequence))\n",
        "    reverse_flank = dna_sequence[fim:fim_reverse]\n",
        "    # print(f'fim_reverse: {fim_reverse} reverse_flank: {reverse_flank} flank_length: {flank_length}')\n",
        "\n",
        "    # Bordas adicionais para contexto\n",
        "    inicio_amostral = max(0, inicio_forward - tamanho_fatias)\n",
        "    fim_amostral = min(fim_reverse + tamanho_fatias, len(dna_sequence))\n",
        "    border_left = dna_sequence[inicio_amostral:inicio_forward]\n",
        "    border_right = dna_sequence[fim_reverse:fim_amostral]\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"inicio_amostral\": inicio_amostral,\n",
        "        \"fim_amostral\": fim_amostral,\n",
        "        \"fragment\": fragment,\n",
        "        \"montante\": forward_flank,\n",
        "        \"jusante\": reverse_flank,\n",
        "        \"full_flank\": forward_flank + fragment + reverse_flank,\n",
        "        \"borda_esquerda\": border_left,\n",
        "        \"borda_direita\": border_right\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def dna_complementar(seq):\n",
        "    complemento = {\n",
        "        'A': 'T',\n",
        "        'T': 'A',\n",
        "        'C': 'G',\n",
        "        'G': 'C'\n",
        "    }\n",
        "    complementar = ''\n",
        "    for base in seq.upper():\n",
        "        complementar += complemento.get(base, base)\n",
        "    return complementar\n",
        "\n",
        "def dna_complementar_inverso(seq):\n",
        "    complementar_inverso = dna_complementar(''.join(reversed(seq)))\n",
        "    return complementar_inverso\n",
        "\n",
        "\n",
        "\n",
        "# descontinuada por busca_melhor_alinhamento\n",
        "def localiza_primers(amostra, loc, inv=False):\n",
        "  # print(f'=> {loc}') dna_complementar\n",
        "  x = -1\n",
        "  y = 0\n",
        "  pos = amostra.find(loc)\n",
        "  if inv:\n",
        "    # o primer right esta √© base complementar e esta invertido para manter o 5'->3'\n",
        "    # print(f'primer: {loc}')\n",
        "    comp = dna_complementar(loc)\n",
        "    # print(f' comp: {comp}')\n",
        "    reverse = ''.join(reversed(comp))\n",
        "    # print(f' reverse: {reverse}')\n",
        "    pos = amostra.find(dna_complementar(''.join(reversed(loc))))\n",
        "    if not pd.isna(pos) and pos != -1:\n",
        "        y = 1\n",
        "        x = pos\n",
        "        fim = pos + len(loc)\n",
        "        resultado = amostra[pos:fim]\n",
        "        # print(f'Amostra REVERSA localizada: {resultado} de {pos} a {fim}')\n",
        "    if y == 0:\n",
        "      print(\"Amostra REVERSA n√£o localizada com os primers.\")\n",
        "  else:\n",
        "    if not pd.isna(pos) and pos != -1:\n",
        "        y = 1\n",
        "        x = pos\n",
        "        fim = pos + len(loc)\n",
        "        resultado = amostra[pos:fim]\n",
        "        # print(f'Amostra DIRETA localizada: {resultado} de {pos} a {fim}')\n",
        "    if y == 0:\n",
        "      print(\"Amostra DIRETA n√£o localizada com os primers.\")\n",
        "  return x\n",
        "\n",
        "\n",
        "\n",
        "def hamming_distance(string_1, string_2):\n",
        "  if len(string_1) > len(string_2):\n",
        "    for i in range(len(string_1) - len(string_2)):\n",
        "      string_2 += ' '\n",
        "  if len(string_2) > len(string_1):\n",
        "    for i in range(len(string_2) - len(string_1)):\n",
        "      string_1 += ' '\n",
        "  diferencas = sum(x != y for x, y in zip(string_1, string_2))\n",
        "  return diferencas / len(string_1)\n",
        "\n",
        "\n",
        "# nova VERSAO ajustada abaixo\n",
        "# procura melhor alinhamento em uma amostra\n",
        "#  qual a posicao se encaixa melhor na amostra\n",
        "#    distancia == 1 quer dizer nenhum alinhamento, neste caso devolve √≠ndice -1\n",
        "#    => o primer_left precisa estar alinhado √† esquerda do STR (meio para traz)\n",
        "#    => o primer_right precisa estar alinhado √† direita do STR (meio at√© o fim)\n",
        "#   ________________________\n",
        "def busca_melhor_alinhamento(amostra, localizar, reverse=False):\n",
        "  menor_dh = 1000.9\n",
        "  recorte = ''\n",
        "  idx = -1\n",
        "\n",
        "  inicio_consulta = 0\n",
        "  fim_consulta    = len(amostra)\n",
        "\n",
        "  if reverse:\n",
        "    localizar_inv_compl = dna_complementar_inverso(localizar)\n",
        "    print(f' >> localizar foi invertido e complementado: de {localizar} para {localizar_inv_compl}')\n",
        "    localizar = localizar_inv_compl\n",
        "\n",
        "  # print(f'consultar: \"{localizar}\" de: {inicio_consulta} a: {fim_consulta} amostra: {len(amostra)}')\n",
        "\n",
        "  for i in range(inicio_consulta, fim_consulta):\n",
        "    corte = amostra[i:i + len(localizar)]\n",
        "    dh = hamming_distance(localizar.upper(), corte.upper())\n",
        "    # print(f'{i}: {corte} - {dh}')\n",
        "    if dh < menor_dh:\n",
        "      menor_dh = dh\n",
        "      idx = i\n",
        "  # print(f'menor distancia: {menor_dh} idx: {idx}' )\n",
        "  if menor_dh != 1.0:\n",
        "    recorte = amostra[idx:idx + len(localizar)]\n",
        "    # print(f' > menor que 1: {menor_dh} idx: {idx} recorte: {recorte}' )\n",
        "    return {'distancia': menor_dh, 'indice': idx, 'recorte': recorte, 'localizar': localizar}\n",
        "  else:\n",
        "    return {'distancia': -1, 'indice': -1, 'recorte': '', 'localizar': ''}\n",
        "\n",
        "# def busca_melhor_alinhamento(amostra, localizar, reverse=False):\n",
        "#     menor_dh = float('inf')\n",
        "#     idx = -1\n",
        "#     recorte = ''\n",
        "\n",
        "#     if reverse:\n",
        "#         localizar = dna_complementar_inverso(localizar)\n",
        "\n",
        "#     for i in range(len(amostra) - len(localizar) + 1):\n",
        "#         corte = amostra[i:i + len(localizar)]\n",
        "#         dh = hamming_distance(localizar.upper(), corte.upper())\n",
        "#         if dh < menor_dh:\n",
        "#             menor_dh = dh\n",
        "#             idx = i\n",
        "\n",
        "#     if menor_dh < len(localizar):  # encontrou algum alinhamento\n",
        "#         recorte = amostra[idx:idx + len(localizar)]\n",
        "#         similaridade = 1 - (menor_dh / len(localizar))\n",
        "#         return {\n",
        "#             'distancia': menor_dh,\n",
        "#             'indice': idx,\n",
        "#             'recorte': recorte,\n",
        "#             'localizar': localizar,\n",
        "#             'similaridade': similaridade\n",
        "#         }\n",
        "#     else:\n",
        "#         return {'distancia': -1, 'indice': -1, 'recorte': '', 'localizar': ''}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2zpeUTSmJt7"
      },
      "outputs": [],
      "source": [
        "# PASSO 1 - Analise unidade de repeti√ß√£o com tamanhos de 1 a 9\n",
        "# leitura dos arquivos fasta bigData e gera√ß√£o dos STRs para cada amostra (arquivo)\n",
        "# !pip install biopython\n",
        "data1 = datetime.now()\n",
        "print(f'>>> inicio - {data1.strftime(\"%Y-%m-%d %H:%M:%S\").replace(\" \", \"_\").replace(\":\", \"-\")}')\n",
        "\n",
        "from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
        "\n",
        "for idx_amostras, arquivo in enumerate(arquivos_amostras):\n",
        "  # print(arquivo)\n",
        "  sequencias_arquivo_sfp = ''\n",
        "  with open(arquivo) as handle:\n",
        "    for seq_id, seq in SimpleFastaParser(handle):\n",
        "      sequencias_arquivo_sfp += str(seq).upper()\n",
        "\n",
        "  print(f'   > lido: {arquivo}')\n",
        "\n",
        "  for idx, tamanho_analise in enumerate(analises):\n",
        "    analise_str = find_str(sequencias_arquivo_sfp, tamanho_analise)\n",
        "    arq = gravar_arquivo_saida(labels_amostras[idx_amostras], tamanho_analise, 'STR', analise_str)\n",
        "    print(f'   > gravado: {arq}')\n",
        "\n",
        "data2 = datetime.now()\n",
        "print(f'>>> fim - {data2.strftime(\"%Y-%m-%d %H:%M:%S\").replace(\" \", \"_\").replace(\":\", \"-\")} - {data2 - data1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKoD8noHnFI2"
      },
      "outputs": [],
      "source": [
        "# PASSO 1.1 => TESTES de confirma√ß√£o do PASSO 1\n",
        "# !pip install biopython\n",
        "\n",
        "from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
        "\n",
        "amostra_selecionada = df_amostras.iloc[0]\n",
        "print(amostra_selecionada.id)\n",
        "\n",
        "arquivo = str(amostra_selecionada['Dados'])\n",
        "print(arquivo)\n",
        "\n",
        "sequencia_arquivo = ''\n",
        "with open(arquivo) as handle:\n",
        "  for seq_id, seq in SimpleFastaParser(handle):\n",
        "    sequencia_arquivo += str(seq).upper()\n",
        "print(f'amostra lida: {amostra_selecionada.id}: {sequencia_arquivo[:100]}')\n",
        "\n",
        "arquivo_str = f'{pasta_analise}ASM285v2_STR_4.csv'\n",
        "df_str_levantados = pd.read_csv(arquivo_str, delimiter=\";\")\n",
        "print(f'{len(df_str_levantados)}')\n",
        "\n",
        "qt_amostras_divergentes = 0\n",
        "for idx, str_levantado in df_str_levantados.iterrows():\n",
        "  fragmento = str(str_levantado.unidade * str_levantado.copias)\n",
        "  str_localizado_amostra              = sequencia_arquivo[str_levantado.inicio_loci     :str_levantado.fim_loci]\n",
        "  str_localizado_amostra_com_bordas10 = sequencia_arquivo[str_levantado.inicio_loci - 10:str_levantado.fim_loci + 10]\n",
        "\n",
        "  # print(f'{str_levantado.unidade} inicio_loci: {str_levantado.inicio_loci} fim_unidade: {str_levantado.fim_unidade} fim_loci: {str_levantado.fim_loci} copias: {str_levantado.copias}')\n",
        "  # print(f'    sequencia original....: {str_localizado_amostra}')\n",
        "  # print(f'    sequencia fragmento...: {fragmento}')\n",
        "  # print(f'    sequencia +10 posi√ß√µes: {str_localizado_amostra_com_bordas10}')\n",
        "  if str_localizado_amostra != fragmento:\n",
        "    qt_amostras_divergentes = qt_amostras_divergentes + 1\n",
        "    print(f'>>>>>> amostras diferentes original/str: {str_localizado_amostra}/{str_localizado_amostra_com_bordas10}')\n",
        "print(f'>>> fim ({qt_amostras_divergentes} fragmentos de STR divergentes da amostra original nas mesmas posi√ß√µes)')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8QUfs457ZgA"
      },
      "outputs": [],
      "source": [
        "# Passo 2 - Remover Duplicados (refer√™ncia p artigo?) 10-09-2025\n",
        "#   => gera dois arquivos:\n",
        "#       1) _TIPO_ que sao os _STR_ sem as duplicidades de locus\n",
        "#       2) _DIFF_ que sao os _STR_ duplicados em mesmo locus e removidos no _TIPO_\n",
        "#   => procura por 'STRs dentro de STR' o que indica presen√ßa em um STR mais curto sendo replicado em um STR longo (AA em A, GTA em GTAGTAGTA...)\n",
        "#      busca arquivos de STR levantados de uma amostra com tipos > 1 e analisa str\n",
        "#   => performance: tabela de CONTENCAO e entropia de shannon\n",
        "\n",
        "# antigo: ds_sel_filtro = df_final.loc[df_final.groupby('inicio')['tamanho_unidade'].idxmin()]\n",
        "# remover inicialmente os de mesmo inicio_loci ?? para melhorar performance\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "def processar_sub_str(arquivo_str):\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    CONTENCAO = {\n",
        "        9: [3, 1],\n",
        "        8: [4, 2, 1],\n",
        "        7: [1],\n",
        "        6: [3, 2, 1],\n",
        "        5: [1],\n",
        "        4: [2, 1],\n",
        "        3: [1],\n",
        "        2: [1],\n",
        "        1: []\n",
        "    }\n",
        "\n",
        "    id_arquivo = quebra_nome_arquivo(arquivo_str)\n",
        "    tipo = int(id_arquivo[2])\n",
        "    origem = id_arquivo[0]\n",
        "\n",
        "    # testando tipo de STR 1 para copiar direto o arquivo completo sem precisar ler/gravar no pandas\n",
        "    if tipo == 1: # mantem todos os STRs do tipo 1, sem filtro\n",
        "      copied_file = f'{pasta_analise}{origem}_TIPO_{tipo}.csv'\n",
        "      shutil.copy(arquivo_str, copied_file)\n",
        "      tempo = time.perf_counter() - t0\n",
        "      tempo_formatado = time.strftime(\"%H:%M:%S\", time.gmtime(tempo)) + f\".{int((tempo % 1) * 1000):03d}\"\n",
        "      print(f'   > gravado: {copied_file} ###sem leitura no pandas #tempo: {tempo_formatado}')\n",
        "      return\n",
        "\n",
        "    df_str_levantados = pd.read_csv(arquivo_str, delimiter=\";\")\n",
        "    print(f'   > lido {arquivo_str} ({len(df_str_levantados)})')\n",
        "\n",
        "    # print(f' > arquivo: {arquivo_str} - origem: {origem} - tipo: {tipo} => {len(df_str_levantados)}')\n",
        "\n",
        "    df_str_levantados['entropy'] = df_str_levantados['unidade'].apply(shannon_entropy)\n",
        "    df_str_levantados['tipo'] = tipo\n",
        "    df_str_levantados['origem'] = origem\n",
        "\n",
        "    # if tipo == 1: # mantem todos os STRs do tipo 1, sem filtro\n",
        "    #   arq = gravar_arquivo_saida(origem, tipo, 'TIPO', df_str_levantados)\n",
        "    #   tempo = time.perf_counter() - t0\n",
        "    #   tempo_formatado = time.strftime(\"%H:%M:%S\", time.gmtime(tempo)) + f\".{int((tempo % 1) * 1000):03d}\"\n",
        "    #   print(f'   > gravado: {arq} #diferen√ßa: {len(df_str_levantados) - len(df_str_levantados)} () tempo: {tempo_formatado}')\n",
        "    #   return\n",
        "\n",
        "    # df_str_levantados = df_str_levantados.reset_index(drop=True)\n",
        "    ultimo_str_processado = ''\n",
        "    ultimo_str_processado_loc = False\n",
        "    indices_mantidos = []\n",
        "    sub_str_localizado = False\n",
        "\n",
        "    # ordenar por unidade e copias e evitar chamar find_str para mesma sequencia anteiror\n",
        "    df_str_ordenado = df_str_levantados.sort_values(by=[\"unidade\"], ascending=[True])\n",
        "    # print(df_str_ordenado)\n",
        "\n",
        "    for idx_str_levantados, str_levantado in df_str_ordenado.iterrows():\n",
        "      str_levantamento = str(str_levantado['unidade'])\n",
        "      # print(f'ultimo_str_processado: {str(ultimo_str_processado)} == str_levantamento: {str_levantamento} > origem: {origem}-{idx_str_levantados}')\n",
        "\n",
        "      if ultimo_str_processado == str_levantamento:\n",
        "        sub_str_localizado = ultimo_str_processado_loc\n",
        "      else:\n",
        "        sub_str_localizado = False\n",
        "        for tipo_str_contencao in CONTENCAO.get(tipo, []):\n",
        "          # if int(tipo_str_contencao) == 1:\n",
        "          if str_levantado['entropy'] == 0: # performance\n",
        "            # print(f'str_levantamento: {str_levantamento} #tipo: {tipo} #unidade: {str_levantado['unidade']} #entropia: {str_levantado['entropy']} #tipo_str_contencao: {tipo_str_contencao}')\n",
        "            sub_str_localizado = True # se entropia √© 0 (AAAAA por exemplo) este sempre estar√° em um analise tipo/tamanho 1\n",
        "          # else:\n",
        "          #   sub_str_localizado = False # se entropia √© direfente de 0 (AAAAC por exemplo) quer dizer que ele nao ser√° um STR do tipo 1\n",
        "            break # nos dois casos acima dele deve dispensar esse registro, salta o tratamento abaixo e vai para pr√≥ximo registro\n",
        "\n",
        "          sub_strs = find_str(str_levantamento, tipo_str_contencao)\n",
        "          if not sub_strs.empty:\n",
        "            for _, sub_str in sub_strs.iterrows():\n",
        "              amostra_sub_str = sub_str['unidade'] * sub_str['copias']\n",
        "              if str_levantamento == amostra_sub_str:\n",
        "                sub_str_localizado = True\n",
        "                # print(f'str_levantamento: {str_levantamento} #tipo: {tipo} #unidade: {str_levantado['unidade']} #unidade_sub_str: {sub_str['unidade']}-{sub_str['copias']} #entropia: {str_levantado['entropy']} #tipo_str_contencao: {tipo_str_contencao}')\n",
        "                break\n",
        "          if sub_str_localizado:\n",
        "            break\n",
        "      if not sub_str_localizado:\n",
        "        indices_mantidos.append(idx_str_levantados)\n",
        "\n",
        "      ultimo_str_processado = str_levantamento\n",
        "      ultimo_str_processado_loc = sub_str_localizado\n",
        "\n",
        "    df_sub = df_str_ordenado.loc[indices_mantidos]\n",
        "    arq = gravar_arquivo_saida(origem, tipo, 'TIPO', df_sub)\n",
        "    tempo = time.perf_counter() - t0\n",
        "    tempo_formatado = time.strftime(\"%H:%M:%S\", time.gmtime(tempo)) + f\".{int((tempo % 1) * 1000):03d}\"\n",
        "    print(f'   > gravado: {arq} #diferen√ßa: {len(df_str_levantados) - len(df_sub)} ({len(df_str_levantados)}-{len(df_sub)}) tempo: {tempo_formatado}')\n",
        "\n",
        "# principal\n",
        "t_principal = time.perf_counter()\n",
        "for idx_label, label_amostra in enumerate(labels_amostras):\n",
        "  print(f'> amostra: {label_amostra} ({idx_label + 1}) de {len(labels_amostras)}')\n",
        "  nome_arq = label_amostra + '_STR'\n",
        "  arquivos_str = busca_arquivos(nome_arq, ordenar_por=\"nome\", reverso=True)\n",
        "\n",
        "  for idx_arquivo, arquivo in enumerate(arquivos_str):\n",
        "    print(f'> processar arquivo: {arquivo}')\n",
        "    processar_sub_str(arquivo)\n",
        "\n",
        "tempo_final = time.perf_counter() - t_principal\n",
        "tempo_final_formatado = time.strftime(\"%H:%M:%S\", time.gmtime(tempo_final)) + f\".{int((tempo_final % 1) * 1000):03d}\"\n",
        "print(f'>>> fim ({tempo_final_formatado})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdavbULAdWJR"
      },
      "outputs": [],
      "source": [
        "# PASSO 3 - soma as repeticoes - cria arquivos sumarizados (com repeticoes):\n",
        "#  üîπ _REP1_ => geral - tamanho do dataFrame de cada tipo (tamanho)\n",
        "#  üîπ _REP2_ => por unidade: agrupamento dos STRs (unidade) - quantidade de cada unidade str por tipo (tamanho)\n",
        "#  üîπ _REP3_ => por unidade e tamanho: agrupamento dos STRs (unidade) de mesmo tamanho (copias) em cada tipo (tamanho)\n",
        "#\n",
        "# 11-09-20025\n",
        "#\n",
        "# .\n",
        "# leitura dos arquivos com os STRs filtrados e gera√ß√£o de arquivos com group_by\n",
        "# repeti√ß√µes\n",
        "#   > soma o numero de repeticoes (unidade e copias) na mesma amostra\n",
        "\n",
        "arquivos_str = busca_arquivos('TIPO')\n",
        "\n",
        "for idx, arquivo in enumerate(arquivos_str):\n",
        "  id_arquivo = quebra_nome_arquivo(arquivo)\n",
        "  result_csv = pd.read_csv(arquivo, delimiter = \";\")\n",
        "  print(f'> lido: {arquivo} ({len(arquivo)})')\n",
        "\n",
        "  dados_geral = {\n",
        "    \"origem\": id_arquivo[0],\n",
        "    \"tipo\": id_arquivo[2],\n",
        "    \"repeticoes\": len(result_csv)\n",
        "  }\n",
        "  df_geral = pd.DataFrame([dados_geral])\n",
        "\n",
        "\n",
        "  df_rep = result_csv.groupby(by=['unidade']).size().reset_index(name='repeticoes')\n",
        "  df_rep['origem'] = id_arquivo[0]\n",
        "  df_rep['tipo'] = id_arquivo[2]\n",
        "\n",
        "  df_rep_copias = result_csv.groupby(by=['unidade', 'copias']).size().reset_index(name='repeticoes')\n",
        "  df_rep_copias['origem'] = id_arquivo[0]\n",
        "  df_rep_copias['tipo'] = id_arquivo[2]\n",
        "\n",
        "  arq1 = gravar_arquivo_saida(id_arquivo[0], id_arquivo[2], 'REP1', df_geral)\n",
        "  print(f'  > gravado: {arq1}')\n",
        "\n",
        "  arq2 = gravar_arquivo_saida(id_arquivo[0], id_arquivo[2], 'REP2', df_rep)\n",
        "  print(f'  > gravado: {arq2}')\n",
        "\n",
        "  arq3 = gravar_arquivo_saida(id_arquivo[0], id_arquivo[2], 'REP3', df_rep_copias)\n",
        "  print(f'  > gravado: {arq3}')\n",
        "\n",
        "print('>>> fim')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQktxQATz3fi"
      },
      "outputs": [],
      "source": [
        "# PASSO 3.1 - agrupamento dos arquivos gerados no PASSO 3 - gera arquivoo SOMA_1\n",
        "#   - quantidade de STRs localizados para cada tipo de STR (tamanho) de cada amostra\n",
        "\n",
        "# REP1 => total geral: quantos STRs de cada tipo\n",
        "arquivos_str = busca_arquivos('REP1')\n",
        "dados = []\n",
        "for idx, arquivo in enumerate(arquivos_str):\n",
        "  print(f'> lido: {arquivo}')\n",
        "  # print(arquivo)\n",
        "  result_csv = pd.read_csv(arquivo, delimiter = \";\")\n",
        "\n",
        "  total_repeticoes = result_csv['repeticoes'].sum()\n",
        "\n",
        "\n",
        "  id_arquivo = quebra_nome_arquivo(arquivo)\n",
        "  # print(f'{id_arquivo[0]} {id_arquivo[2]} {contagem_unidades}')\n",
        "  dados.append({\n",
        "    \"origem\": id_arquivo[0],\n",
        "    \"tipo\": id_arquivo[2],\n",
        "    \"quantidade\": total_repeticoes\n",
        "  })\n",
        "df_quantidades = pd.DataFrame(dados)\n",
        "\n",
        "df_quantidades[\"tipo\"] = df_quantidades[\"tipo\"].astype(int)\n",
        "\n",
        "df_pivot = df_quantidades.pivot_table(\n",
        "    index=[\"tipo\"],\n",
        "    columns=\"origem\",\n",
        "    values=\"quantidade\",\n",
        "    aggfunc=\"sum\",\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "df_pivot.columns.name = None\n",
        "\n",
        "df_pivot = df_pivot.sort_values(by='tipo')\n",
        "\n",
        "url_arquivo = pasta_analise + 'SOMA_1.csv'\n",
        "df_pivot.to_csv(url_arquivo, sep=';', index=False)\n",
        "print(f'  > gravado: {url_arquivo}')\n",
        "\n",
        "\n",
        "print('>>> fim')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFf8HvwLC1X7"
      },
      "outputs": [],
      "source": [
        "# PASSO 3.1.1 - visualizacao dos arquivos gerados no PASSO 3.1\n",
        "# Gera Tabela 2.a - resultados e discuss√£o\n",
        "\n",
        "arquivo_soma = pasta_analise + 'SOMA_1.csv'\n",
        "result_csv = pd.read_csv(arquivo_soma, delimiter = \";\")\n",
        "\n",
        "numero_para_nome_map = {v: k for k, v in tipo_nucleotideo_map.items()}\n",
        "result_csv.insert(\n",
        "    0,\n",
        "    \"Tipo de STR\",\n",
        "    result_csv[\"tipo\"].map(numero_para_nome_map)\n",
        ")\n",
        "\n",
        "mapa_colunas = df_amostras.set_index(\"id\")[\"Cepa (strain)\"].to_dict()\n",
        "result_csv = result_csv.rename(columns=mapa_colunas)\n",
        "\n",
        "result_csv = result_csv.drop(columns='tipo')\n",
        "\n",
        "url_arquivo = pasta_analise + 'TABELA_2A.csv'\n",
        "result_csv.to_csv(url_arquivo, sep=';', index=False)\n",
        "print(f'  > gravado: {url_arquivo}')\n",
        "\n",
        "result_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMfsQok7F17o"
      },
      "outputs": [],
      "source": [
        "# PASSO 3.2 - agrupamento dos arquivos gerados no PASSO 3 -  gera arquivoo SOMA_2\n",
        "#   - quantidade de STRs localizados para cada unidade e tipo de STR (tamanho) de cada amostra\n",
        "# Tabela 2.b - resultados e discuss√£o\n",
        "\n",
        "# REP2 => total geral de cada str, tipo e unidade\n",
        "import pandas as pd\n",
        "\n",
        "arquivos_str = busca_arquivos('REP2')\n",
        "dados = []\n",
        "for idx, arquivo in enumerate(arquivos_str):\n",
        "  result_csv = pd.read_csv(arquivo, delimiter = \";\")\n",
        "  dados.append(result_csv)\n",
        "df_concatenado = pd.concat(dados, ignore_index=True)\n",
        "\n",
        "df_concatenado[\"tipo\"] = df_concatenado[\"tipo\"].astype(int)\n",
        "df_pivot = df_concatenado.pivot_table(\n",
        "    index=[\"tipo\", \"unidade\"],\n",
        "    columns=\"origem\",\n",
        "    values=\"repeticoes\",\n",
        "    aggfunc=\"sum\",\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "df_pivot.columns.name = None\n",
        "\n",
        "# df_pivot = df_pivot.sort_values(by=['unidade', 'tipo'])\n",
        "df_pivot = df_pivot.sort_values(by='tipo')\n",
        "df_pivot = df_pivot.reset_index()\n",
        "\n",
        "url_arquivo = pasta_analise + 'SOMA_2.csv'\n",
        "df_pivot.to_csv(url_arquivo, sep=';', index=False)\n",
        "print(f'  > gravado: {url_arquivo}')\n",
        "\n",
        "df_pivot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VyL8fTPhV_b"
      },
      "outputs": [],
      "source": [
        "# PASSO 3.2.1 - visualizacao dos arquivos gerados no PASSO 3.2\n",
        "#   - quantidade de STRs localizados para cada unidade e tipo de STR (tamanho) de cada amostra\n",
        "# Gera Tabela 2.b - resultados e discuss√£o\n",
        "\n",
        "arquivo_soma = pasta_analise + 'SOMA_2.csv'\n",
        "result_csv = pd.read_csv(arquivo_soma, delimiter = \";\")\n",
        "\n",
        "numero_para_nome_map = {v: k for k, v in tipo_nucleotideo_map.items()}\n",
        "result_csv.insert(\n",
        "    0,\n",
        "    \"Tipo de STR\",\n",
        "    result_csv[\"tipo\"].map(numero_para_nome_map)\n",
        ")\n",
        "\n",
        "mapa_colunas = df_amostras.set_index(\"id\")[\"Cepa (strain)\"].to_dict()\n",
        "result_csv = result_csv.rename(columns=mapa_colunas)\n",
        "\n",
        "result_csv = result_csv.drop(columns=['index', 'tipo'])\n",
        "\n",
        "url_arquivo = pasta_analise + 'TABELA_2B.csv'\n",
        "result_csv.to_csv(url_arquivo, sep=';', index=False)\n",
        "print(f'  > gravado: {url_arquivo}')\n",
        "\n",
        "result_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwOvQeh6TsJV"
      },
      "outputs": [],
      "source": [
        "# PASSO 3.3 - agrupamento dos arquivos gerados no PASSO 3 - gera arquivo SOMA_3\n",
        "#   - quantidade de STRs localizados para cada unidade, copias e tipo de STR (tamanho) de cada amostra\n",
        "#   - quantidade de STRs localizados para cada unidade e tipo de STR (tamanho) de cada amostra\n",
        "# Tabela 2.c - resultados e discuss√£o\n",
        "\n",
        "# REP3 => total geral de cada str, tipo e unidade e copias\n",
        "import pandas as pd\n",
        "\n",
        "arquivos_str = busca_arquivos('REP3')\n",
        "dados = []\n",
        "for idx, arquivo in enumerate(arquivos_str):\n",
        "  result_csv = pd.read_csv(arquivo, delimiter = \";\")\n",
        "  dados.append(result_csv)\n",
        "df_concatenado = pd.concat(dados, ignore_index=True)\n",
        "\n",
        "df_concatenado[\"tipo\"] = df_concatenado[\"tipo\"].astype(int)\n",
        "df_pivot = df_concatenado.pivot_table(\n",
        "    index=[\"tipo\", \"unidade\", \"copias\"],\n",
        "    columns=\"origem\",\n",
        "    values=\"repeticoes\",\n",
        "    aggfunc=\"sum\",\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "df_pivot.columns.name = None\n",
        "\n",
        "# df_pivot = df_pivot.sort_values(by=['unidade', 'tipo'])\n",
        "df_pivot = df_pivot.sort_values(by=['tipo', 'copias'])\n",
        "df_pivot = df_pivot.reset_index()\n",
        "\n",
        "url_arquivo = pasta_analise + 'SOMA_3.csv'\n",
        "df_pivot.to_csv(url_arquivo, sep=';', index=False)\n",
        "print(f'  > gravado: {url_arquivo}')\n",
        "\n",
        "df_pivot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "338SypPF-o8F"
      },
      "outputs": [],
      "source": [
        "# PASSO 3.3.1 - visualizacao dos arquivos gerados no PASSO 3.3\n",
        "# aki - esse passo eh igual para 3.1.1 e 3.2.1 - criar funcao separada\n",
        "#   - quantidade de STRs localizados para cada unidade, tipo e numero de c√≥pias  de STR (tamanho) de cada amostra\n",
        "# Gera Tabela 2.c - resultados e discuss√£o\n",
        "# 14-09-2025\n",
        "\n",
        "arquivo_soma = pasta_analise + 'SOMA_3.csv'\n",
        "result_csv = pd.read_csv(arquivo_soma, delimiter = \";\")\n",
        "\n",
        "numero_para_nome_map = {v: k for k, v in tipo_nucleotideo_map.items()}\n",
        "result_csv.insert(\n",
        "    0,\n",
        "    \"Tipo de STR\",\n",
        "    result_csv[\"tipo\"].map(numero_para_nome_map)\n",
        ")\n",
        "\n",
        "mapa_colunas = df_amostras.set_index(\"id\")[\"Cepa (strain)\"].to_dict()\n",
        "result_csv = result_csv.rename(columns=mapa_colunas)\n",
        "\n",
        "result_csv = result_csv.drop(columns=['index', 'tipo'])\n",
        "\n",
        "url_arquivo = pasta_analise + 'TABELA_2C.csv'\n",
        "result_csv.to_csv(url_arquivo, sep=';', index=False)\n",
        "print(f'  > gravado: {url_arquivo}')\n",
        "\n",
        "result_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9mwlM34Pi6Q"
      },
      "outputs": [],
      "source": [
        "# aki aki - teste diagrama\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from upsetplot import UpSet, from_memberships\n",
        "\n",
        "# Caminho do arquivo\n",
        "url_arquivo = pasta_analise + 'SOMA_3.csv'\n",
        "\n",
        "# Carregar os dados com separador correto\n",
        "df = pd.read_csv(url_arquivo, sep=\";\")\n",
        "\n",
        "# Lista das amostras\n",
        "amostras = ['CBS 554.65', 'CBS 513.88', 'KJC3', 'KYF3', 'CCTCC 206047']\n",
        "\n",
        "# Fun√ß√£o para obter o conjunto de amostras onde o STR est√° presente\n",
        "def obter_conjunto(row):\n",
        "    return tuple([amostra for amostra in amostras if row[amostra] > 0])\n",
        "\n",
        "# Aplicar a fun√ß√£o para criar a coluna de conjuntos\n",
        "df['conjunto'] = df.apply(obter_conjunto, axis=1)\n",
        "\n",
        "# Gerar gr√°fico para cada tipo de STR\n",
        "tipos_str = df['Tipo de STR'].unique()\n",
        "\n",
        "for tipo in tipos_str:\n",
        "    df_tipo = df[df['Tipo de STR'] == tipo]\n",
        "\n",
        "    # Gerar Series com conjuntos\n",
        "    series = from_memberships(df_tipo['conjunto'], data=1)\n",
        "\n",
        "    # Agrupar conjuntos duplicados\n",
        "    series_agrupada = series.groupby(series.index).sum()\n",
        "\n",
        "    # Plotar UpSetPlot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    upset = UpSet(series_agrupada, show_counts=True, sort_by='degree')\n",
        "    upset.plot()\n",
        "    plt.suptitle(f\"Interse√ß√µes de STRs - Tipo {tipo}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vg8DqLXIEaSv"
      },
      "outputs": [],
      "source": [
        "# PASSO 4 - busca por STRs de mesmo tamanho (copias) que aparecem em TODAS as amostras\n",
        "# 14-09-2025\n",
        "\n",
        "arquivo_soma = pasta_analise + 'SOMA_3.csv'\n",
        "result_csv = pd.read_csv(arquivo_soma, delimiter = \";\")\n",
        "\n",
        "# montar a query dinamicamente\n",
        "# ASM285v2 > 0 & ASM1928827v1 > 0 & ASM2978390v1 > 0 & ASM2978392v1 > 0 & ASM4765177v1 > 0\n",
        "labels_repeticao = [label for label in labels_amostras] # levanta as colunas com mesmo nome das amostras q √© a coluna que possui o numero de repeticoes\n",
        "query_str = \" & \".join([f\"{col} > 0\" for col in labels_repeticao])\n",
        "print(f' > lido: {query_str}')\n",
        "\n",
        "df_result_set = pd.DataFrame(result_csv.query(query_str))\n",
        "df_result_set.reset_index(drop=True, inplace=True)\n",
        "df_result_set = df_result_set.drop(columns=['index'])\n",
        "\n",
        "url_arquivo = pasta_analise + 'INCLUSIVOS.csv'\n",
        "df_result_set.to_csv(url_arquivo, sep=';', index=False)\n",
        "\n",
        "print(f' > gravado: {url_arquivo}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx7A-oJRd1gU"
      },
      "outputs": [],
      "source": [
        "AKI - Diagrama de Venn\n",
        "\n",
        "# PASSO 4.1 Diagrama de Venn - STRs de mesmo tamanho (copias) que aparecem em\n",
        "#   TODAS as amostras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib_venn import venn3\n",
        "\n",
        "def diagrama_veen(id_analise, only_1928827, only_2978390, only_2978392, inter_1928827_2978390, inter_1928827_2978392, inter_2978390_2978392, inter_todos):\n",
        "  plt.figure(figsize=(14, 6))\n",
        "  venn = venn3(subsets=(only_1928827, only_2978390, inter_1928827_2978390,\n",
        "                        only_2978392, inter_1928827_2978392, inter_2978390_2978392, inter_todos),\n",
        "              set_labels=(labels_amostras))\n",
        "\n",
        "  legend_labels = [\n",
        "      f\"Somente ASM1928827v1: {only_1928827}\",\n",
        "      f\"Somente ASM2978390v1: {only_2978390}\",\n",
        "      f\"Somente ASM2978392v1: {only_2978392}\",\n",
        "      f\"ASM1928827v1 & ASM2978390v1: {inter_1928827_2978390}\",\n",
        "      f\"ASM1928827v1 & ASM2978392v1: {inter_1928827_2978392}\",\n",
        "      f\"ASM2978390v1 & ASM2978392v1: {inter_2978390_2978392}\",\n",
        "      f\"Todos: {inter_todos}\"\n",
        "  ]\n",
        "  plt.legend(legend_labels, loc=\"lower left\",  bbox_to_anchor=(1, 0.5), fontsize=10)\n",
        "  # plt.legend(legend_labels, loc=\"center left\", bbox_to_anchor=(0.65, 0.5), fontsize=10)\n",
        "  # 'best', 'upper right', 'upper left', 'lower left', 'lower right', 'right', 'center left', 'center right', 'lower center', 'upper center', 'center'\n",
        "\n",
        "  plt.title(\"Distribui√ß√£o dos STR-\" + id_analise + \" nas amostras\")\n",
        "\n",
        "  # salvar aquivo de imagem\n",
        "  arquivo_png = pasta_analise + \"veen_\" + id_analise + \".png\"\n",
        "  if os.path.exists(arquivo_png):\n",
        "    os.remove(arquivo_png)\n",
        "  plt.savefig(arquivo_png)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def diagrama_veen_unweighted(id_analise, only_1928827, only_2978390, only_2978392, inter_1928827_2978390, inter_1928827_2978392, inter_2978390_2978392, inter_todos):\n",
        "  plt.figure(figsize=(6, 6))\n",
        "  # Desenha o diagrama unweighted: todos os subsets recebem \"1\"\n",
        "  venn = venn3(subsets=(1, 1, 1, 1, 1, 1, 1),\n",
        "            set_labels=(labels_amostras))\n",
        "\n",
        "  # Atualiza os labels para exibir os valores reais\n",
        "  if venn.get_label_by_id('100'):\n",
        "      venn.get_label_by_id('100').set_text(str(only_1928827))\n",
        "  if venn.get_label_by_id('010'):\n",
        "      venn.get_label_by_id('010').set_text(str(only_2978390))\n",
        "  if venn.get_label_by_id('001'):\n",
        "      venn.get_label_by_id('001').set_text(str(only_2978392))\n",
        "  if venn.get_label_by_id('110'):\n",
        "      venn.get_label_by_id('110').set_text(str(inter_1928827_2978390))\n",
        "  if venn.get_label_by_id('101'):\n",
        "      venn.get_label_by_id('101').set_text(str(inter_1928827_2978392))\n",
        "  if venn.get_label_by_id('011'):\n",
        "      venn.get_label_by_id('011').set_text(str(inter_2978390_2978392))\n",
        "  if venn.get_label_by_id('111'):\n",
        "      venn.get_label_by_id('111').set_text(str(inter_todos))\n",
        "\n",
        "  # legend_labels = [\n",
        "  #     f\"Somente ASM1928827v1: {only_1928827}\",\n",
        "  #     f\"Somente ASM2978390v1: {only_2978390}\",\n",
        "  #     f\"Somente ASM2978392v1: {only_2978392}\",\n",
        "  #     f\"ASM1928827v1 & ASM2978390v1: {inter_1928827_2978390}\",\n",
        "  #     f\"ASM1928827v1 & ASM2978392v1: {inter_1928827_2978392}\",\n",
        "  #     f\"ASM2978390v1 & ASM2978392v1: {inter_2978390_2978392}\",\n",
        "  #     f\"Todos: {inter_todos}\"\n",
        "  # ]\n",
        "  # plt.legend(legend_labels, loc=\"lower left\",  bbox_to_anchor=(1, 0.5), fontsize=10)\n",
        "\n",
        "  plt.title(\"Distribui√ß√£o dos STR-\" + id_analise + \" nas amostras\")\n",
        "\n",
        "  # salvar aquivo de imagem\n",
        "  arquivo_png = pasta_analise + \"veen_unweighted_\" + id_analise + \".png\"\n",
        "  if os.path.exists(arquivo_png):\n",
        "    os.remove(arquivo_png)\n",
        "  plt.savefig(arquivo_png)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "# diagrama_veen(\"'teste diagrama com tamanhos proporcionais aas areas dos conjuntos de valores'\", 5, 5, 5, 10, 10, 10, 20)\n",
        "# diagrama_veen_unweighted(\"'teste diagrama com tamanhos iguais (nao ponderado - unweighte)'\", 5, 5, 5, 10, 10, 10, 20)\n",
        "\n",
        "arquivos_analise = busca_arquivos('XXX-DET')\n",
        "\n",
        "for idx, arquivo in enumerate(arquivos_analise):\n",
        "  result_analise = pd.read_csv(arquivo, delimiter = \";\")\n",
        "\n",
        "  # Calcular interse√ß√µes\n",
        "  df_1928827 = result_analise.query('repeticoes_ASM1928827v1  > 0 & repeticoes_ASM2978390v1 == 0 & repeticoes_ASM2978392v1 == 0')\n",
        "  df_2978390 = result_analise.query('repeticoes_ASM1928827v1 == 0 & repeticoes_ASM2978390v1  > 0 & repeticoes_ASM2978392v1 == 0')\n",
        "  df_2978392 = result_analise.query('repeticoes_ASM1928827v1 == 0 & repeticoes_ASM2978390v1 == 0 & repeticoes_ASM2978392v1  > 0')\n",
        "  only_1928827 = len(df_1928827)\n",
        "  only_2978390 = len(df_2978390)\n",
        "  only_2978392 = len(df_2978392)\n",
        "\n",
        "  df_1928827_2978390 = result_analise.query('repeticoes_ASM1928827v1  > 0 & repeticoes_ASM2978390v1  > 0 & repeticoes_ASM2978392v1 == 0')\n",
        "  df_1928827_2978392 = result_analise.query('repeticoes_ASM1928827v1  > 0 & repeticoes_ASM2978390v1 == 0 & repeticoes_ASM2978392v1  > 0')\n",
        "  df_2978390_2978392 = result_analise.query('repeticoes_ASM1928827v1 == 0 & repeticoes_ASM2978390v1  > 0 & repeticoes_ASM2978392v1  > 0')\n",
        "  inter_1928827_2978390 = len(df_1928827_2978390)\n",
        "  inter_1928827_2978392 = len(df_1928827_2978392)\n",
        "  inter_2978390_2978392 = len(df_2978390_2978392)\n",
        "\n",
        "  df_todos = result_analise.query('repeticoes_ASM1928827v1 > 0 & repeticoes_ASM2978390v1 > 0 & repeticoes_ASM2978392v1 > 0')\n",
        "  inter_todos = len(df_todos)\n",
        "\n",
        "  id_arquivo = quebra_nome_arquivo(arquivo)\n",
        "  diagrama_veen_unweighted(id_arquivo[2], only_1928827, only_2978390, only_2978392, inter_1928827_2978390, inter_1928827_2978392, inter_2978390_2978392, inter_todos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVMn2pW7HE3S"
      },
      "outputs": [],
      "source": [
        "# PASSO 5 - busca por STRs de mesmo tamanho (copias) que aparecem somente\n",
        "#   em uma das amostras - monta arquivo _EXCLUSIVOS\n",
        "# 14-09-2025 09-10-2025\n",
        "\n",
        "arquivo_soma = pasta_analise + 'SOMA_3.csv'\n",
        "result_csv = pd.read_csv(arquivo_soma, delimiter = \";\")\n",
        "\n",
        "# dados = []\n",
        "# ASM285v2 > 0 & ASM1928827v1 == 0 & ASM2978390v1 == 0 & ASM2978392v1 == 0 & ASM4765177v1 == 0\n",
        "labels_repeticao = [label for label in labels_amostras] # levanta as colunas com mesmo nome das amostras q √© a coluna que possui o numero de repeticoes\n",
        "for label_amosta in labels_repeticao:\n",
        "    # montar a query dinamicamente cada query seta a coluna com nome da amostras > 0 e as demais == 0\n",
        "    # para a coluna atual: > 0\n",
        "    condicoes = [f\"{c} > 0\" if c == label_amosta else f\"{c} == 0\" for c in labels_repeticao]\n",
        "\n",
        "    # montar a query\n",
        "    query_str = \" & \".join(condicoes)\n",
        "    print(f' > lido: {query_str}')\n",
        "    # print(f\"Arquivo: {arquivo} | Coluna positiva: {col} | Linhas encontradas: {len(dt)}\")\n",
        "\n",
        "    # aplicar filtro\n",
        "    df_result_set = result_csv.query(query_str)\n",
        "\n",
        "    df_result_set = pd.DataFrame(result_csv.query(query_str))\n",
        "    df_result_set.reset_index(drop=True, inplace=True)\n",
        "    df_result_set = df_result_set.drop(columns=['index'])\n",
        "\n",
        "    url_arquivo = f'{pasta_analise}{label_amosta}_EXCLUSIVOS.csv'\n",
        "    df_result_set.to_csv(url_arquivo, sep=';', index=False)\n",
        "    print(f' > gravado: {url_arquivo} ({len(url_arquivo)})')\n",
        "    # dados.append(df_result_set)\n",
        "\n",
        "# df_concatenado = pd.concat(dados, ignore_index=True)\n",
        "\n",
        "# url_arquivo_fim = pasta_analise + 'GERAL_EXCLUSIVOS.csv'\n",
        "# df_concatenado.to_csv(url_arquivo_fim, sep=';', index=False)\n",
        "# dados.append(df_result_set)\n",
        "# print(f'> fim > gravado: {url_arquivo_fim} (len({url_arquivo_fim})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHLyv4bQSm0y"
      },
      "outputs": [],
      "source": [
        "# PASSO 6 - Busca origem dos STRs exclusivos de cada amostra\n",
        "#  - ler arquivos EXCLUSIVOS de cada genoma e buscar as posi√ß√µes dos STRs\n",
        "#     nos arquivos de STRs levantados sem duplicidades (_TIPO_)\n",
        "#     -> gera arquivos SELECT com as informa√ß√µes de loci -> uma linha para cada\n",
        "#        repeti√ß√£o (sequencial_loci) dos STRs exclusivos de cada cepa\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def ler_arquivo_str(nome_amostra, tipo):\n",
        "  dados = pd.DataFrame()\n",
        "  arquivo = f'{pasta_analise}{nome_amostra}_TIPO_{str(tipo)}.csv'\n",
        "  dados = pd.read_csv(arquivo, delimiter = \";\")\n",
        "  print(f'   > lido str (TIPO): {arquivo} ({len(dados)})')\n",
        "  return dados\n",
        "\n",
        "tipo_atual = ''\n",
        "id_amostra_atual = ''\n",
        "dados_str = pd.DataFrame()\n",
        "\n",
        "erros = []\n",
        "\n",
        "arquivos_exclusivos = busca_arquivos('EXCLUSIVOS')\n",
        "for idx, arquivo in enumerate(arquivos_exclusivos):\n",
        "  nome_arquivo = os.path.basename(arquivo) # Obter apenas o nome do arquivo sem o diret√≥rio\n",
        "  parte_principal = os.path.splitext(nome_arquivo)[0] # Remover a extens√£o do arquivo\n",
        "  partes = parte_principal.split('_')\n",
        "  id_amostra = partes[0]\n",
        "\n",
        "  dados_exclusivos = pd.read_csv(arquivo, delimiter = \";\")\n",
        "  print(f'   > lido exclusivos: {arquivo} ({len(dados_exclusivos)})')\n",
        "  dados_exclusivos = dados_exclusivos.sort_values(['tipo', 'unidade', 'copias'], ascending=[True, True, True])\n",
        "\n",
        "  linhas_novas = []\n",
        "  for _, row in dados_exclusivos.iterrows():\n",
        "    if id_amostra != id_amostra_atual or tipo_atual != row['tipo']:\n",
        "      dados_str = ler_arquivo_str(id_amostra, row['tipo'])\n",
        "    tipo_atual = row['tipo']\n",
        "    id_amostra_atual = id_amostra\n",
        "\n",
        "    # id_amostra\n",
        "    tipo = row['tipo']\n",
        "    unidade = row['unidade']\n",
        "    copias = row['copias']\n",
        "\n",
        "    df_str_localizados = dados_str.query(\"`unidade` == @unidade & copias == @copias\")\n",
        "    df_str_localizados = pd.DataFrame()\n",
        "    try:\n",
        "        df_str_localizados = dados_str.query(\"`unidade` == @unidade & copias == @copias\")\n",
        "    except Exception as e:\n",
        "        print(f'Erro ao buscar STR: {e}')\n",
        "        erros.append(f'ERROR {id_amostra} Erro ao buscar STR: {e} -> row: {row}')\n",
        "        continue\n",
        "    if df_str_localizados.empty:\n",
        "        erros.append(f'ERROR {id_amostra} df_str_localizados.empty -> row: {row}')\n",
        "        print(f'      > Nenhum STR localizado para unidade={unidade}, c√≥pias={copias}')\n",
        "    repeticoes_na_amostra = int(row[f'{id_amostra}'])\n",
        "    if repeticoes_na_amostra != len(df_str_localizados): # quantidade de repeticoes da amostra\n",
        "        erros.append(f'ERROR {id_amostra} quantidade de repeticoes direfentes -> row: {row}')\n",
        "\n",
        "    sequencial_loci = 0\n",
        "    for ix_str_localizado, str_localizado in df_str_localizados.iterrows():\n",
        "      nova_linha = {}\n",
        "      nova_linha['origem'] = id_amostra\n",
        "      nova_linha['tipo'] = tipo\n",
        "      nova_linha['unidade'] = unidade\n",
        "      nova_linha['copias'] = copias\n",
        "      nova_linha['inicio_loci'] = str_localizado['inicio_loci']\n",
        "      nova_linha['fim_unidade'] = str_localizado['fim_unidade']\n",
        "      nova_linha['fim_loci'] = str_localizado['fim_loci']\n",
        "      nova_linha['repeticoes_na_amostra'] = repeticoes_na_amostra\n",
        "      nova_linha['sequencial_loci'] = sequencial_loci\n",
        "      sequencial_loci = sequencial_loci + 1\n",
        "      # print(f'      > NOVA_LINHA: \\n{nova_linha})')\n",
        "      linhas_novas.append(nova_linha)\n",
        "\n",
        "  df_resultado = pd.DataFrame(linhas_novas)\n",
        "  arquivo_saida = f'{pasta_analise}{id_amostra}_SELECT.csv'\n",
        "  df_resultado.to_csv(arquivo_saida, sep=';', index=False)\n",
        "  print(f'   > gravado: {arquivo_saida} ({len(df_resultado)})')\n",
        "\n",
        "print(f'>>> fim - erros: {len(erros)}')\n",
        "if len(erros) > 0:\n",
        "  print(f'!!! erros: {erros}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1IGgqFr0CNF"
      },
      "outputs": [],
      "source": [
        "# PASSO 7 - levantar as regioes flanqueadoras dos STRs exclusivos - gera arquivos _FLANK_\n",
        "#   ler arquivos SELECT e buscar regi√µes flanqueadoras\n",
        "# 14-09-2025 15-09-2025 10-10-2025\n",
        "# L√™ arquivos de STRs marcados como SELECT.\n",
        "# Identifica a origem gen√¥mica e o tipo de STR.\n",
        "# Busca a sequ√™ncia FASTA correspondente √† origem.\n",
        "# Para cada STR:\n",
        "#     Calcula os flancos (create_flank) com base nas posi√ß√µes.\n",
        "#     Adiciona informa√ß√µes como fragmento, montante, jusante, bordas etc.\n",
        "# Salva os dados enriquecidos em um novo arquivo com sufixo FLANK.\n",
        "\n",
        "!pip install biopython\n",
        "\n",
        "import pandas as pd\n",
        "from str_utils import busca_arquivos, busca_sequencia_fasta, create_flank, gravar_arquivo_saida\n",
        "\n",
        "arquivos_str = busca_arquivos('SELECT')\n",
        "origem_atual = ''\n",
        "sequencia = ''\n",
        "quantidade_arquivos_lidos = 0\n",
        "quantidade_arquivos_gravados = 0\n",
        "\n",
        "for idx, arquivo in enumerate(arquivos_str):\n",
        "  df_arq_str = pd.read_csv(arquivo, delimiter = \";\")\n",
        "  print(f' > lido: {arquivo} ({len(df_arq_str)})')\n",
        "  quantidade_arquivos_lidos = quantidade_arquivos_lidos + 1\n",
        "\n",
        "  linhas_atualizadas = []\n",
        "  for idx_unidade_str, unidade_str in df_arq_str.iterrows():\n",
        "    origem = unidade_str['origem']\n",
        "    if origem != origem_atual:\n",
        "        sequencia = busca_sequencia_fasta(origem)\n",
        "    origem_atual = origem\n",
        "\n",
        "    # print(f\"processando{idx_unidade_str + 1} de ({len(df_arq_str)})...\")\n",
        "    flank = create_flank(sequencia, unidade_str['inicio_loci'], unidade_str['fim_loci'])\n",
        "    linha_completa = unidade_str.to_dict()\n",
        "    linha_completa['inicio_amostral'] = flank['inicio_amostral']\n",
        "    linha_completa['fim_amostral'] = flank['fim_amostral']\n",
        "    linha_completa['fragment'] = flank['fragment']\n",
        "    linha_completa['montante'] = flank['montante']\n",
        "    linha_completa['jusante'] = flank['jusante']\n",
        "    linha_completa['full_flank'] = flank['full_flank']\n",
        "    linha_completa['borda_esquerda'] = flank['borda_esquerda']\n",
        "    linha_completa['borda_direita'] = flank['borda_direita']\n",
        "    linhas_atualizadas.append(linha_completa)\n",
        "\n",
        "  df_flankeado = pd.DataFrame(linhas_atualizadas)\n",
        "  arq_sel = f'{pasta_analise}{origem}_FLANK.csv'\n",
        "  df_flankeado.to_csv(arq_sel, sep=';', index=False)\n",
        "  quantidade_arquivos_gravados = quantidade_arquivos_gravados + 1\n",
        "  print(f' > gravado: {arq_sel} ({len(df_flankeado)})')\n",
        "\n",
        "print(f'>>> fim: {quantidade_arquivos_lidos} arquivos lidos e {quantidade_arquivos_gravados} gravados')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8cSJ9Red03k"
      },
      "outputs": [],
      "source": [
        "# Passo XX verificar se √°reas levantadas coincidem na amostra\n",
        "# !pip install biopython\n",
        "from str_utils import busca_sequencia_fasta\n",
        "\n",
        "url_arquivo =  f'{pasta_analise}FIM_full.csv'\n",
        "result = pd.read_csv(url_arquivo, delimiter = \";\")\n",
        "print(f\"> leitura: {url_arquivo} ({len(result)})\")\n",
        "\n",
        "origem_fasta = ''\n",
        "sequencia_fasta = ''\n",
        "quant_erros = 0\n",
        "erros = []\n",
        "for idx, str_loc in result.iterrows():\n",
        "  print(f\"...analisando {idx + 1} de {len(result)}\")\n",
        "  if origem_fasta != str_loc['origem']:\n",
        "    sequencia_fasta = busca_sequencia_fasta(str_loc['origem'])\n",
        "    print(f\"> fasta..: {sequencia_fasta[:100]}\")\n",
        "  origem_fasta = str_loc['origem']\n",
        "\n",
        "  try:\n",
        "    area_loc = ( str_loc['borda_esquerda'] +\n",
        "      str_loc['montante'] +\n",
        "      str_loc['fragment'] +\n",
        "      str_loc['jusante'] +\n",
        "      str_loc['borda_direita'] )\n",
        "  except Exception as e:\n",
        "      erros.append(f'ERRO exce√ß√£o - c√°lculo de area_loc - inesperada: {e} str_loc: {str_loc}')\n",
        "\n",
        "  # print(area_total)\n",
        "  inicio_amostra = str_loc['inicio_amostral']\n",
        "  fim_amostra    = inicio_amostra + len(area_loc)\n",
        "\n",
        "  try:\n",
        "    area_original = sequencia_fasta[inicio_amostra:fim_amostra]\n",
        "  except IndexError as e:\n",
        "      erros.append(f'ERRO exce√ß√£o de √≠ndice: {e} str_loc: {str_loc}')\n",
        "  except Exception as e:\n",
        "      erros.append(f'ERRO erro ao buscar sequencia fasta: {e} str_loc: {str_loc}')\n",
        "\n",
        "  if area_loc != area_original:\n",
        "    erros.append(f'ERRO √°rea da amostra √© diferente da √°rea localizada no genoma original - original.: {area_original} analisada: {area_loc} str_loc: {str_loc}')\n",
        "\n",
        "print(f'>>> fim - erros: {len(erros)}')\n",
        "if len(erros) > 0:\n",
        "  for erro in erros:\n",
        "    print(f'!!! erro: \\n        {erro}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_VK-kAg-JRt"
      },
      "outputs": [],
      "source": [
        "# PASSO 7.1 - visualizar STR e √°reas flankeadas\n",
        "# 15-09-2025 fragm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import pandas as pd\n",
        "# from str_utils import busca_arquivos, busca_sequencia_fasta, create_flank, gravar_arquivo_saida\n",
        "\n",
        "dados = []\n",
        "arquivos_flank = busca_arquivos('FLANK')\n",
        "for idx, arquivo in enumerate(arquivos_flank):\n",
        "  df_arq_flank = pd.read_csv(arquivo, delimiter = \";\")\n",
        "  print(f' > lido: {arquivo} ({len(df_arq_flank)})')\n",
        "  # id_arquivo = quebra_nome_arquivo(arquivo)\n",
        "  # origem = id_arquivo[0]\n",
        "  # tipo = id_arquivo[2]\n",
        "  dados.append(df_arq_flank)\n",
        "df_flankeados = pd.concat(dados, ignore_index=True)\n",
        "# url_arquivo = pasta_analise + 'FLANQUEAMENTOS.csv'\n",
        "# df_flankeados.to_csv(url_arquivo, sep=';', index=False)\n",
        "# print(f'>>> fim > gravado: {url_arquivo}')\n",
        "\n",
        "\n",
        "# # Fun√ß√£o para gerar sequ√™ncia aleat√≥ria de DNA\n",
        "# def random_dna(length):\n",
        "#     return ''.join(random.choices(['A','T','G','C'], k=length))\n",
        "# Criando regi√µes do exemplo\n",
        "# seq_before = random_dna(200)       # regi√£o livre antes da montante\n",
        "# montante = random_dna(150)         # regi√£o montante\n",
        "# str_seq = \"GA\" * 50                 # STR central (100 bp de GA)\n",
        "# jusante = random_dna(150)           # regi√£o jusante\n",
        "# seq_after = random_dna(200)         # regi√£o livre depois da jusante\n",
        "\n",
        "##################################\n",
        "# query_res = df_flankeados.query('(tipo * copias) > 30')\n",
        "# flank_sel = query_res.iloc[int(len(query_res)/ 2)]\n",
        "flank_sel = df_flankeados.iloc[0]\n",
        "##################################\n",
        "\n",
        "seq_before = flank_sel['borda_esquerda']\n",
        "montante = flank_sel['montante']\n",
        "str_seq = flank_sel['fragment']\n",
        "jusante = flank_sel['jusante']\n",
        "seq_after = flank_sel['borda_direita']\n",
        "unidade_str = flank_sel['unidade']\n",
        "tipo_str = flank_sel['tipo']\n",
        "offset = flank_sel['inicio_amostral']\n",
        "tipo = flank_sel['tipo']\n",
        "copias = flank_sel['copias']\n",
        "\n",
        "#  > fragment:\tSequ√™ncia entre inicio e fim\n",
        "#  > forward_flank:\tRegi√£o anterior ao fragmento, com tamanho m√≠nimo de 200\n",
        "#  > reverse_flank:\tRegi√£o posterior ao fragmento, com tamanho m√≠nimo de 200\n",
        "#  > full_flank:\tFlanqueadoras + fragmento central\n",
        "#  > borda_esquerda:\tRegi√£o antes da flanqueadora esquerda (200 bases)\n",
        "#  > borda_direita:\tRegi√£o ap√≥s a flanqueadora direita (200 bases)\n",
        "#  > inicio_amostral:\t√çndice inicial da sequ√™ncia total estendida\n",
        "#  > fim_amostral:\t√çndice final da sequ√™ncia total estendida\n",
        "\n",
        "# Concatenando para formar a sequ√™ncia final\n",
        "seq = seq_before + montante + str_seq + jusante + seq_after\n",
        "\n",
        "# Definir posi√ß√µes de cada regi√£o\n",
        "montante_start = len(seq_before)\n",
        "montante_end = montante_start + len(montante)\n",
        "str_start = montante_end\n",
        "str_end = str_start + len(str_seq)\n",
        "jusante_start = str_end\n",
        "jusante_end = jusante_start + len(jusante)\n",
        "borda_direita_end = jusante_end + len(seq_after)\n",
        "\n",
        "# Mapear bases para n√∫meros\n",
        "bases = {'A':1, 'T':2, 'G':3, 'C':4}\n",
        "\n",
        "# Cores para cada base\n",
        "base_colors = {'A':'red', 'T':'blue', 'G':'green', 'C':'orange'}\n",
        "\n",
        "# Criar stem plot\n",
        "fig, ax = plt.subplots(figsize=(25,5))\n",
        "\n",
        "# Fundo das regi√µes com transpar√™ncia menor\n",
        "ax.axvspan(0, montante_start, color='lightgray', alpha=0.2)\n",
        "ax.axvspan(montante_start, montante_end, color='skyblue', alpha=0.2)\n",
        "ax.axvspan(str_start, str_end, color='gold', alpha=0.2)\n",
        "ax.axvspan(jusante_start, jusante_end, color='lightgreen', alpha=0.2)\n",
        "ax.axvspan(jusante_end, len(seq), color='lightgray', alpha=0.2)\n",
        "\n",
        "# Linhas verticais pontilhadas delimitando regi√µes\n",
        "for pos in [montante_start, montante_end, str_end, jusante_end]:\n",
        "    ax.axvline(x=pos, color='black', linestyle='--', linewidth=1)\n",
        "\n",
        "# Stem plot por base (linhas verticais)\n",
        "for i, base in enumerate(seq):\n",
        "    if base in bases:\n",
        "        ax.stem([i+1], [bases[base]],\n",
        "                linefmt=base_colors[base],\n",
        "                markerfmt='',\n",
        "                basefmt=\" \")\n",
        "\n",
        "# Labels das regi√µes no topo\n",
        "ax.text(montante_start + len(montante)/2, 4.5, f\"Montante ({len(montante)})\", ha='center', fontsize=12, fontweight='bold')\n",
        "ax.text(str_start + len(str_seq)/2, 4.5, f'STR-{tipo_str}: {unidade_str} ({len(str_seq)})', ha='center', fontsize=12, fontweight='bold')\n",
        "ax.text(jusante_start + len(jusante)/2, 4.5, f\"Jusante ({len(jusante)})\", ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Ajustes visuais\n",
        "ax.set_ylim(0,5)\n",
        "ax.set_yticks([1,2,3,4])\n",
        "ax.set_yticklabels(['A','T','G','C'])\n",
        "\n",
        "ax.set_xlabel(\"Posi√ß√£o na sequ√™ncia\")\n",
        "# ax.set_title(\"Sequ√™ncia de DNA com STR e regi√µes flanqueadoras\")\n",
        "\n",
        "ax.set_xticks([0, montante_start, montante_end, str_start, str_end, jusante_start, jusante_end, borda_direita_end])\n",
        "# Mant√©m o gr√°fico como est√°, mas altera apenas os labels do eixo x\n",
        "\n",
        "xticks = ax.get_xticks()           # pega os ticks atuais\n",
        "# ax.set_xticklabels([int(tick + offset) for tick in xticks])\n",
        "ax.set_xticklabels([f\"{int(tick + offset):,}\".replace(\",\", \".\") for tick in xticks])\n",
        "\n",
        "ax.set_xlabel(f\"Posi√ß√£o das por√ß√µes destacadas na sequ√™ncia FASTA original da amostra {origem}\", labelpad=5, fontsize=12, fontweight='bold')\n",
        "ax.tick_params(axis='x', labelsize=14, width=2, length=8)\n",
        "ax.set_ylabel(f\"Nucleot√≠deos\", labelpad=5, fontsize=12, fontweight='bold')\n",
        "ax.tick_params(axis='y', labelsize=14, width=2, length=8)\n",
        "\n",
        "\n",
        "# Legenda manual\n",
        "from matplotlib.patches import Patch\n",
        "# legend_areas = [Patch(facecolor='skyblue', label='Montante', alpha=0.2),\n",
        "#                    Patch(facecolor='gold', label='STR', alpha=0.2),\n",
        "#                    Patch(facecolor='lightgreen', label='Jusante', alpha=0.2),\n",
        "#                    Patch(facecolor='lightgray', label='Regi√£o livre', alpha=0.2)]\n",
        "# ax.legend(handles=legend_areas, title=\"√Åreas\", loc='upper right')\n",
        "\n",
        "base_colors = {'A': 'red', 'C': 'orange', 'T': 'blue', 'G': 'green'}\n",
        "legend_bases = [Patch(color=base_colors[base], label=base) for base in ['A', 'T', 'G', 'C']]\n",
        "# legend_total = legend_bases + legend_areas\n",
        "ax.legend(handles=legend_bases , title=\"Bases\", loc='upper left')\n",
        "\n",
        "# ax.set_title(f'{str_seq} tipo: {tipo} copias: {copias}')\n",
        "\n",
        "# fig.text(0.5, -0.05, f'Montante: {montante}', fontsize=12)\n",
        "# fig.text(0.5, -0.05, f'Jusante: {jusante}', ha='center', fontsize=12)\n",
        "\n",
        "# salvar aquivo de imagem\n",
        "arquivo_png = \"/content/drive/MyDrive/pesquisa/regioes_flanqueadoras.png\"\n",
        "plt.savefig(arquivo_png, bbox_inches='tight', pad_inches=0, dpi=300)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NLX1GUYLp52"
      },
      "outputs": [],
      "source": [
        "# PASSO 8 - Montar os primers com primer3 - gera arquivos PRIMERS\n",
        "# 15-09-2025 10-10-2025\n",
        "!pip install primer3-py\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "# from str_utils import busca_arquivos, busca_sequencia_fasta, create_flank, gravar_arquivo_saida\n",
        "import primer3\n",
        "\n",
        "\n",
        "def safe_len(val):\n",
        "    if pd.isna(val):   # se for NaN\n",
        "        return 0\n",
        "    return len(str(val))\n",
        "\n",
        "\n",
        "# Par√¢metros de desenho dos primers\n",
        "parametros_primers = {\n",
        "    'PRIMER_MIN_SIZE': 18,\n",
        "    'PRIMER_OPT_SIZE': 20,\n",
        "    'PRIMER_MAX_SIZE': 23,\n",
        "    'PRIMER_MIN_TM': 57.0,\n",
        "    'PRIMER_OPT_TM': 59.0,\n",
        "    'PRIMER_MAX_TM': 62.0,\n",
        "    'PRIMER_MAX_POLY_X': 5,\n",
        "    'PRIMER_PAIR_MAX_DIFF_TM': 5.0,\n",
        "}\n",
        "\n",
        "def sequencia_valida(seq):\n",
        "    return bool(re.fullmatch(r'[ACGTNS]+', seq))\n",
        "\n",
        "def limpar_sequencia(seq):\n",
        "    return re.sub(r'[^ACGTN]', 'N', seq)\n",
        "\n",
        "# Fun√ß√£o para desenhar primers para uma linha\n",
        "\n",
        "def desenhar_primers(row):\n",
        "    id = f\"{row['origem']}-{row['unidade']}-{row['copias']}\"\n",
        "    seq = str(row['full_flank']).upper()\n",
        "    target_start = safe_len(row['montante'])\n",
        "    target_end   = safe_len(row['fragment'])\n",
        "    if not sequencia_valida(seq):\n",
        "        print(f\">>>>>>>>>>>> PRIMER_ERROR: Sequ√™ncia inv√°lida: {seq}\")\n",
        "        return pd.Series({'PRIMER_ERROR': 'Sequ√™ncia inv√°lida'})\n",
        "    entrada = {\n",
        "      'SEQUENCE_ID': id,\n",
        "      'SEQUENCE_TEMPLATE': seq,\n",
        "      'SEQUENCE_TARGET': [target_start, target_end]\n",
        "    }\n",
        "    resultado = primer3.bindings.design_primers(entrada, parametros_primers)\n",
        "    return pd.Series(resultado)\n",
        "\n",
        "\n",
        "quantidade_arquivos_lidos = 0\n",
        "quantidade_arquivos_gravados = 0\n",
        "\n",
        "arquivos_flank = busca_arquivos('FLANK')\n",
        "for idx, arquivo in enumerate(arquivos_flank):\n",
        "  df_arq_flank = pd.read_csv(arquivo, delimiter = \";\")\n",
        "  print(f' > lido: {arquivo} ({len(df_arq_flank)})')\n",
        "  quantidade_arquivos_lidos = quantidade_arquivos_lidos + 1\n",
        "\n",
        "  df_arq_flank['full_flank'] = df_arq_flank['full_flank'].apply(limpar_sequencia)\n",
        "\n",
        "  nome_arquivo = os.path.basename(arquivo) # Obter apenas o nome do arquivo sem o diret√≥rio\n",
        "  parte_principal = os.path.splitext(nome_arquivo)[0] # Remover a extens√£o do arquivo\n",
        "  partes = parte_principal.split('_')\n",
        "  id_amostra = partes[0]\n",
        "\n",
        "  # Aplicar a fun√ß√£o para gerar os primers\n",
        "  df_resultado_primers = df_arq_flank.apply(desenhar_primers, axis=1)\n",
        "  # juntar os resultados ao DataFrame original\n",
        "  df_primer = df_arq_flank.join(df_resultado_primers)\n",
        "\n",
        "  arq_primer = f'{pasta_analise}{id_amostra}_PRIMER.csv'\n",
        "  df_primer.to_csv(arq_primer, sep=';', index=False)\n",
        "  print(f' > gravado: {arq_primer} ({len(df_primer)})')\n",
        "  quantidade_arquivos_gravados = quantidade_arquivos_gravados + 1\n",
        "\n",
        "print(f'>>> fim: {quantidade_arquivos_lidos} arquivos lidos e {quantidade_arquivos_gravados} gravados')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULDhkgZZOXxM"
      },
      "outputs": [],
      "source": [
        "arquivos_primer = busca_arquivos('FLANK')\n",
        "\n",
        "for idx, arquivo in enumerate(arquivos_primer):\n",
        "  # df_arq_primer = pd.read_csv(arquivo, delimiter = \";\")\n",
        "  # print(f' > lido: {arquivo} ({len(df_arq_primer)})')\n",
        "  # df_arq_primer['entropy'] = df_arq_primer['unidade'].apply(shannon_entropy)\n",
        "  # print(df_arq_primer['entropy'])\n",
        "  print(arquivo)\n",
        "print('fim')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WePZ4qrSYN2n"
      },
      "outputs": [],
      "source": [
        "# PASSO 9 - Montar os Amplicons - gera arquivos _AMP_\n",
        "#   avalia os 5 primers retornados para cada full_flank (flank_L _ STR + flank_R)\n",
        "#     procura melhor alinhamento par ao primer\n",
        "# 15-09-2025 16-09-2025 17-09-2025 amplicon\n",
        "\n",
        "# aKI Aki duvida sobre a montagem (localizacao do primer reverse)\n",
        "# busca_melhor_alinhamento\n",
        "# a distrancia de hamer esta ficando alta\n",
        "\n",
        "from str_utils import localiza_primers, busca_melhor_alinhamento\n",
        "\n",
        "dados = []\n",
        "arquivos_primer = busca_arquivos('PRIMER')\n",
        "\n",
        "for idx, arquivo in enumerate(arquivos_primer):\n",
        "  df_arq_primer = pd.read_csv(arquivo, delimiter = \";\")\n",
        "  print(f' > lido: {arquivo} ({len(df_arq_primer)})')\n",
        "  id_arquivo = quebra_nome_arquivo(arquivo)\n",
        "  origem = id_arquivo[0]\n",
        "\n",
        "  registros = []\n",
        "  for i_primer, primer3_result in df_arq_primer.iterrows():\n",
        "    # print(f'{arq_primer}')\n",
        "    # filtra os primers\n",
        "    primers = []\n",
        "    vistos = set()\n",
        "\n",
        "    for i in range(5):\n",
        "        primer_left = primer3_result.get(f'PRIMER_LEFT_{i}_SEQUENCE', \"\")\n",
        "        primer_right = primer3_result.get(f'PRIMER_RIGHT_{i}_SEQUENCE', \"\")\n",
        "\n",
        "        val_left = primer3_result.get(f\"PRIMER_LEFT_{i}\", \"\")\n",
        "        val_right = primer3_result.get(f\"PRIMER_RIGHT_{i}\", \"\")\n",
        "        left_pos = -1\n",
        "        left_len = 0\n",
        "        right_pos = -1\n",
        "        right_len = 0\n",
        "        if pd.notna(val_left) and val_left != \"\":\n",
        "            # remove colchetes e espa√ßos\n",
        "            val_left_str = str(val_left).strip(\"[] \")\n",
        "            left_pos, left_len = map(int, val_left_str.split(\",\"))\n",
        "        else:\n",
        "            left_pos, left_len = -1, 0\n",
        "\n",
        "        if pd.notna(val_right) and val_right != \"\":\n",
        "            val_right_str = str(val_right).strip(\"[] \")\n",
        "            right_pos, right_len = map(int, val_right_str.split(\",\"))\n",
        "        else:\n",
        "            right_pos, right_len = -1, 0\n",
        "\n",
        "\n",
        "        primer_pair_penalty = primer3_result.get(f'PRIMER_PAIR{i}_PENALTY', \"\")\n",
        "        primer_left_penalty = primer3_result.get(f'PRIMER_LEFT{i}_PENALTY', \"\")\n",
        "        primer_right_penalty = primer3_result.get(f'PRIMER_RIGHT{i}_PENALTY', \"\")\n",
        "        par = (primer_left, primer_right)\n",
        "\n",
        "        montante = primer3_result.get('montante')\n",
        "        montante = primer3_result.get('montante')\n",
        "        jusante = primer3_result.get('jusante')\n",
        "        full_flank = primer3_result.get('full_flank')\n",
        "        fragment = primer3_result.get('fragment')\n",
        "\n",
        "        # if par not in vistos and primer_left and primer_right:\n",
        "        if pd.notna(primer_left) and pd.notna(primer_right) and par not in vistos and primer_left and primer_right and pd.notna(montante) and pd.notna(jusante):\n",
        "          # procurar melhor alinhamento pois nem sempre ser√° id√™ntico\n",
        "          # print(f\"primer: {i} montante: {primer3_result.get('montante')} \"\n",
        "          #       f\"jusante: {primer3_result.get('jusante')} \"\n",
        "          #       f\"i_primer: {i_primer} primer3_result: {primer3_result}\")\n",
        "\n",
        "          inicio_montante = 0\n",
        "          fim_montante = len(montante)\n",
        "          # inicio_jusante = fim_montante + len(fragment) - 1 - aki foi ajustado retirando 0 -1\n",
        "          inicio_jusante = fim_montante + len(fragment)\n",
        "          fim_jusante = len(full_flank)\n",
        "          primer_left_start = busca_melhor_alinhamento(full_flank[inicio_montante:fim_montante], primer_left)\n",
        "\n",
        "          # print(primer_left_start)\n",
        "          # print(f'jusante: {primer3_result['jusante']}')\n",
        "\n",
        "          # primer_right_start = busca_melhor_alinhamento(full_flank[inicio_jusante:fim_jusante], primer_right, True)\n",
        "          primer_right_start = busca_melhor_alinhamento(full_flank[inicio_jusante:fim_jusante], primer_right) # aki testando em inverter e complementar\n",
        "\n",
        "          print(f'full_flank ({len(full_flank)}): {full_flank}')\n",
        "          print(f'fragment ({len(fragment)}): {fragment}')\n",
        "          print(f' > inicio_montante: {inicio_montante}')\n",
        "          print(f' > fim_montante: {fim_montante}')\n",
        "          print(f' > inicio_jusante: {inicio_jusante}')\n",
        "          print(f' > fim_jusante: {fim_jusante}')\n",
        "          print(f' > primer_left_start: {primer_left_start}')\n",
        "          print(f' > primer_right_start: {primer_right_start}')\n",
        "          print(f'   > inicio_jusante: {inicio_jusante}')\n",
        "          print(f' > primer_left ({len(primer_left)}): {primer_left}')\n",
        "          print(f' > primer_right ({len(primer_right)}): {primer_right}')\n",
        "          print(f\" > primer_right_recorte ({len(primer_right_start['recorte'])}): {primer_right_start['recorte']}\")\n",
        "          # print(primer_left_start)\n",
        "          # {'distancia': -1, 'indice': -1, 'recorte': '', 'localizar': ''}\n",
        "\n",
        "          # amplicon = ''\n",
        "          # if primer_left_start['distancia'] != -1 and primer_right_start['distancia'] != -1:\n",
        "          #   primer_esquerdo_extendido = primer3_result['montante'][\n",
        "          #       primer_left_start['indice']:len(primer3_result['montante'])\n",
        "          #   ]\n",
        "          #   primer_direito_extendido = primer3_result['jusante'][\n",
        "          #       0:primer_right_start['indice'] + len(primer_right)\n",
        "          #   ]\n",
        "          #   amplicon = primer_esquerdo_extendido + primer3_result['fragment'] + primer_direito_extendido\n",
        "\n",
        "          amplicon = ''\n",
        "          # if primer_left_start['distancia'] != -1 and primer_right_start['distancia'] != -1: # aki - revisar essa captura\n",
        "              # # Extens√£o do primer esquerdo\n",
        "              # amplicon_inicio = primer_left_start['indice']\n",
        "              # # ampicon_esquerdo = primer3_result['full_flank'][amplicon_inicio:]\n",
        "\n",
        "              # # Extens√£o do primer direito\n",
        "              # incio_primer_right = inicio_jusante + primer_right_start['indice']\n",
        "              # amplicon_fim = incio_primer_right + len(primer_right)\n",
        "              # # amplicon_direito = primer3_result['full_flank'][:amplicon_fim]\n",
        "\n",
        "              # # Constru√ß√£o do amplicon\n",
        "              # # amplicon = ampicon_esquerdo + primer3_result['fragment'] + amplicon_direito\n",
        "              # amplicon = primer3_result['full_flank'][amplicon_inicio:amplicon_fim]\n",
        "\n",
        "          # aki - ultima alteracao, utilizar os indices e tamanho de primer retornado pelo primer3 ao inv√©s de calcular\n",
        "          if left_pos != -1 and left_len > 0 and right_pos != -1 and right_len > 0:\n",
        "              # Extens√£o do primer esquerdo\n",
        "              amplicon_inicio = left_pos\n",
        "              # ampicon_esquerdo = primer3_result['full_flank'][amplicon_inicio:]\n",
        "\n",
        "              # Extens√£o do primer direito\n",
        "              incio_primer_right = right_pos\n",
        "              amplicon_fim = incio_primer_right + right_len\n",
        "              # amplicon_direito = primer3_result['full_flank'][:amplicon_fim]\n",
        "\n",
        "              # Constru√ß√£o do amplicon\n",
        "              # amplicon = ampicon_esquerdo + primer3_result['fragment'] + amplicon_direito\n",
        "              amplicon = primer3_result['full_flank'][amplicon_inicio:amplicon_fim]\n",
        "\n",
        "\n",
        "              print(f'    > amplicon_inicio/primer_left_inicio: {amplicon_inicio}')\n",
        "              print(f'    > incio_primer_right: {incio_primer_right}')\n",
        "              print(f'    > amplicon_fim: {amplicon_fim}')\n",
        "              print(f'    > amplicon: {amplicon}')\n",
        "\n",
        "              primers.append({\n",
        "                  'primer_left': primer_left,\n",
        "                  'primer_left_loc': primer_left_start['recorte'],\n",
        "                  'primer_right': primer_right,\n",
        "                  'primer_right_loc': primer_right_start['recorte'],\n",
        "                  'primer_right_loc_direta': primer_right_start['localizar'],\n",
        "                  'primer_left_distancia': primer_left_start['distancia'],\n",
        "                  'primer_right_distancia': primer_right_start['distancia'],\n",
        "                  'primer_pair_penalty': primer_pair_penalty,\n",
        "                  'primer_left_penalty': primer_left_penalty,\n",
        "                  'primer_left_index': left_pos,\n",
        "                  'primer_left_len': left_len,\n",
        "                  'primer_right_index': right_pos,\n",
        "                  'primer_right_len': right_len,\n",
        "                  'primer_right_penalty': primer_right_penalty,\n",
        "                  'primer_left_start': amplicon_inicio,\n",
        "                  'primer_right_start': incio_primer_right,\n",
        "                  'amplicon': amplicon,\n",
        "                  'full_flank': primer3_result['full_flank'],\n",
        "                  'montante': primer3_result['montante'],\n",
        "                  'jusante': primer3_result['jusante']\n",
        "                  })\n",
        "              vistos.add(par)\n",
        "\n",
        "    # print(f'quantidade primers montados: {len(primers)}')\n",
        "\n",
        "    qt = 0\n",
        "    for i_p in primers:\n",
        "      qt = qt + 1\n",
        "      # print(f'{i_p} - {qt} de {len(primers)}')\n",
        "\n",
        "      registro_primer = {\n",
        "          'sequencial': qt,\n",
        "          'unidade': primer3_result['unidade'],\n",
        "          'inicio_loci': primer3_result['inicio_loci'],\n",
        "          'fim_unidade': primer3_result['fim_unidade'],\n",
        "          'fim_loci': primer3_result['fim_loci'],\n",
        "          'copias': primer3_result['copias'],\n",
        "          'tipo': primer3_result['tipo'],\n",
        "          'origem': primer3_result['origem'],\n",
        "          'inicio_amostral': primer3_result['inicio_amostral'],\n",
        "          'fim_amostral': primer3_result['fim_amostral'],\n",
        "          'fragment': primer3_result['fragment'],\n",
        "          'montante': primer3_result['montante'],\n",
        "          'jusante': primer3_result['jusante'],\n",
        "          'full_flank': primer3_result['full_flank'],\n",
        "          'borda_esquerda': primer3_result['borda_esquerda'],\n",
        "          'borda_direita': primer3_result['borda_direita'],\n",
        "\n",
        "          'primer_left': i_p['primer_left'],\n",
        "          'primer_left_loc': i_p['primer_left_loc'],\n",
        "          'primer_left_start': i_p['primer_left_start'],\n",
        "          'primer_left_distancia': i_p['primer_left_distancia'],\n",
        "          'primer_right': i_p['primer_right'],\n",
        "          'primer_right_loc': i_p['primer_right_loc'],\n",
        "          'primer_right_loc_direta': i_p['primer_right_loc_direta'],\n",
        "          'primer_right_start': i_p['primer_right_start'],\n",
        "          'primer_right_distancia': i_p['primer_right_distancia'],\n",
        "          'primer_pair_penalty': i_p['primer_pair_penalty'],\n",
        "          'primer_left_penalty': i_p['primer_left_penalty'],\n",
        "          'primer_right_penalty': i_p['primer_right_penalty'],\n",
        "          'primer_left_index': i_p['primer_left_index'],\n",
        "          'primer_left_len': i_p['primer_left_len'],\n",
        "          'primer_right_index': i_p['primer_right_index'],\n",
        "          'primer_right_len': i_p['primer_right_len'],\n",
        "          'amplicon': i_p['amplicon'],\n",
        "          'tamanho_str': primer3_result['tipo'] * primer3_result['copias'],\n",
        "          'tamanho_amplicon': len(i_p['amplicon'])\n",
        "      }\n",
        "      registros.append(registro_primer)\n",
        "\n",
        "  # df_registros = pd.concat(registros, ignore_index=True)\n",
        "  # df_registros = pd.DataFrame([registros])\n",
        "  # df_registros = pd.DataFrame(registros, columns=colunas_manter)\n",
        "  df_registros = pd.DataFrame(registros)\n",
        "\n",
        "  url_arquivo =  f'{pasta_analise}{origem}_AMP.csv'\n",
        "  df_registros.to_csv(url_arquivo, sep=';', index=False)\n",
        "  print(f'   > gravado: {url_arquivo} ({len(df_registros)})')\n",
        "\n",
        "print(f'>>> fim')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZRK0ynwcfkK"
      },
      "outputs": [],
      "source": [
        "# testes de busca_melhor_alinhamento - antigo mantido\n",
        "#  ajustar e apagar nao utilizados no utils\n",
        "\n",
        "from str_utils import busca_melhor_alinhamento\n",
        "#                       1         2         3         4         5\n",
        "#              012345678901234567890123456789012345678901234567890\n",
        "sequencia = \"ATGCGTACGTAGCTAGCTAGGATCCGATCGTACGTAGCTAGCTAGCTA\"\n",
        "primer_left  = \"ATGCGTACG\"   # esperado alinhar no in√≠cio\n",
        "primer_right = \"TAGCTC\"      # esperado alinhar no final\n",
        "                # TAGCTA GAGCTA GAGCTA\n",
        "teste = busca_melhor_alinhamento(sequencia, primer_left)\n",
        "print(teste)\n",
        "\n",
        "teste2 = busca_melhor_alinhamento(sequencia, primer_right, True)\n",
        "print(teste2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZihv3VPlCKJ"
      },
      "outputs": [],
      "source": [
        "# PASSO 10 - Filtrar amplicons para conferir com BLASTn\n",
        "#  -> melhores\n",
        "#  -> gera arquivo FIM (index_amp = sequencial para recuperar resultado do blastN)\n",
        "#\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from str_utils import busca_arquivos\n",
        "\n",
        "# def filtrar_amostras(df):\n",
        "#   print(f'  inicio filtro: {len(df)}')\n",
        "#   df = df[~df['amplicon'].duplicated(keep=False)]\n",
        "#   print(f'  filtro amplicons duplicados: {len(df)}')\n",
        "#   df = df.query('tamanho_amplicon > 300')\n",
        "#   print(f'  filtro tamanho do amplicon: {len(df)}')\n",
        "\n",
        "#   total = len(df)\n",
        "#   if total == 0:\n",
        "#     return df\n",
        "\n",
        "#   minimo = 100\n",
        "#   maximo = int(total * 0.1)\n",
        "\n",
        "#   if maximo < minimo:\n",
        "#     maximo = minimo\n",
        "\n",
        "#   if maximo > total:\n",
        "#     maximo = total\n",
        "\n",
        "#   # Seleciona aleatoriamente entre m√≠nimo e m√°ximo de amostras\n",
        "#   # n_amostras = max(minimo, min(maximo, total))\n",
        "#   # return df.sample(n=n_amostras, random_state=42)\n",
        "\n",
        "#   # seleciona na ordem recebida\n",
        "#   return df[0:maximo]\n",
        "\n",
        "arquivos_amplicon = busca_arquivos('AMP')\n",
        "\n",
        "dados = []\n",
        "totais = 0\n",
        "for idx, arquivo in enumerate(arquivos_amplicon):\n",
        "  df_amplicons = pd.read_csv(arquivo, delimiter = \";\")\n",
        "  id_arquivo = quebra_nome_arquivo(arquivo)\n",
        "  origem = id_arquivo[0]\n",
        "  print(f'> lido: {arquivo}')\n",
        "\n",
        "  df_amplicons['entropy_amplicon'] = df_amplicons['amplicon'].apply(shannon_entropy)\n",
        "  df_amplicons['entropy_fragment'] = df_amplicons['fragment'].apply(shannon_entropy)\n",
        "\n",
        "  print(f\"df_amplicons: {len(df_amplicons)}\")\n",
        "\n",
        "  # # filtra apenas os amplicons que cont√™m o fragmento - entre o inicio e o fim do str\n",
        "  # df_amplicons = df_amplicons[\n",
        "  #   df_amplicons.apply(lambda row: row['fragment'] in row['amplicon'], axis=1)\n",
        "  # ]\n",
        "  # print(f\"df_amplicons - filtra apenas os amplicons que cont√™m o fragment: {len(df_amplicons)}\")\n",
        "\n",
        "  # filtra posicoes\n",
        "  df_amplicons = df_amplicons[\n",
        "    (df_amplicons['primer_left_index'] <= 200) &\n",
        "    (df_amplicons['primer_right_index'] >= (df_amplicons['montante'].str.len() + df_amplicons['fragment'].str.len() - 2))\n",
        "  ]\n",
        "  print(f\"df_amplicons - filtra posicoes: {len(df_amplicons)}\")\n",
        "\n",
        "\n",
        "\n",
        "  # exclui unidades STR duplicados\n",
        "  df_amplicons = df_amplicons.sort_values(by=['unidade', 'copias', 'primer_right_distancia'], ascending=[True, False, True])\n",
        "  # df_amplicons = df_amplicons.sort_values(by=['unidade', 'copias'], ascending=[True, False])\n",
        "  df_amplicons = df_amplicons.drop_duplicates(subset=['unidade', 'copias'], keep='first').reset_index(drop=True)\n",
        "  print(f\"df_amplicons - exclui STR duplicados: {len(df_amplicons)}\")\n",
        "\n",
        "  # exclui amplicons duplicados\n",
        "  df_amplicons = df_amplicons.sort_values(by=['amplicon', 'tamanho_str', 'primer_right_distancia', 'tamanho_amplicon'], ascending=[True, False, True, False])\n",
        "  df_amplicons = df_amplicons.drop_duplicates(subset=['amplicon'], keep='first').reset_index(drop=True)\n",
        "  print(f\"df_amplicons - exclui amplicons duplicados: {len(df_amplicons)}\")\n",
        "\n",
        "  # ordem final\n",
        "  # df_amplicons = df_amplicons.sort_values(by=['tamanho_str', 'tamanho_amplicon'], ascending=[False, False])\n",
        "  df_amplicons = df_amplicons.sort_values(by=['tamanho_str'], ascending=[False])\n",
        "  df_amplicons = df_amplicons.sort_values(by=['tamanho_str', 'score'], ascending=[False, False])\n",
        "\n",
        "  # limita quantidade final de registros\n",
        "  # df_filtrado = filtrar_amostras(df_amplicons) # COM filtro\n",
        "  maximo = 100\n",
        "  if len(df_amplicons) < 100:\n",
        "    maximo = len(df_amplicons)\n",
        "  df_filtrado = df_amplicons[0:maximo]\n",
        "  # df_filtrado = df_amplicons # SEM filtro\n",
        "\n",
        "  url_arquivo =  f'{pasta_analise}{origem}_FIM.csv'\n",
        "  df_filtrado = df_filtrado.reset_index(drop=True)\n",
        "  df_filtrado.insert(0, 'index_amp', range(1, len(df_filtrado) + 1))\n",
        "  df_filtrado.to_csv(url_arquivo, sep=';', index=False)\n",
        "  print(f'   > gravado/filtrados: {url_arquivo} ({len(df_filtrado)} de {len(df_amplicons)}) - {datetime.now()}')\n",
        "\n",
        "  dados.append(df_filtrado)\n",
        "  totais = totais + len(df_amplicons)\n",
        "\n",
        "# df_geral = pd.concat(dados, ignore_index=False)\n",
        "# url_arquivo_geral =  f'{pasta_analise}FIM_full.csv'\n",
        "# df_geral = df_geral.reset_index()\n",
        "# df_geral = df_geral.rename(columns={'index': 'fk_'})\n",
        "# df_geral.to_csv(url_arquivo_geral, sep=';', index=False)\n",
        "# print(f'>>> fim - full - gravado: {url_arquivo_geral} ({len(df_geral)} de {totais}) - {datetime.now()}')\n",
        "print(f'>>> fim')\n",
        "\n",
        "# > lido: /content/drive/MyDrive/pesquisa/2025-09-04/ASM1928827v1_AMP.csv\n",
        "# df_amplicons: 5698\n",
        "# df_amplicons - STR duplicados: 1066\n",
        "# df_amplicons - exclui amplicons duplicados: 1047\n",
        "#   inicio filtro: 1047\n",
        "#   filtro amplicons duplicados: 1047\n",
        "#   filtro tamanho do amplicon: 184\n",
        "#    > gravado/filtrados: /content/drive/MyDrive/pesquisa/2025-09-04/ASM1928827v1_FIM.csv (100 de 1047) - 2025-11-07 16:52:23.302261\n",
        "# > lido: /content/drive/MyDrive/pesquisa/2025-09-04/ASM285v2_AMP.csv\n",
        "# df_amplicons: 5328\n",
        "# df_amplicons - STR duplicados: 1029\n",
        "# df_amplicons - exclui amplicons duplicados: 1021\n",
        "#   inicio filtro: 1021\n",
        "#   filtro amplicons duplicados: 1021\n",
        "#   filtro tamanho do amplicon: 187\n",
        "#    > gravado/filtrados: /content/drive/MyDrive/pesquisa/2025-09-04/ASM285v2_FIM.csv (100 de 1021) - 2025-11-07 16:52:23.591699\n",
        "# > lido: /content/drive/MyDrive/pesquisa/2025-09-04/ASM2978390v1_AMP.csv\n",
        "# df_amplicons: 4402\n",
        "# df_amplicons - STR duplicados: 822\n",
        "# df_amplicons - exclui amplicons duplicados: 815\n",
        "#   inicio filtro: 815\n",
        "#   filtro amplicons duplicados: 815\n",
        "#   filtro tamanho do amplicon: 156\n",
        "#    > gravado/filtrados: /content/drive/MyDrive/pesquisa/2025-09-04/ASM2978390v1_FIM.csv (100 de 815) - 2025-11-07 16:52:24.502141\n",
        "# > lido: /content/drive/MyDrive/pesquisa/2025-09-04/ASM2978392v1_AMP.csv\n",
        "# df_amplicons: 2259\n",
        "# df_amplicons - STR duplicados: 410\n",
        "# df_amplicons - exclui amplicons duplicados: 408\n",
        "#   inicio filtro: 408\n",
        "#   filtro amplicons duplicados: 408\n",
        "#   filtro tamanho do amplicon: 84\n",
        "#    > gravado/filtrados: /content/drive/MyDrive/pesquisa/2025-09-04/ASM2978392v1_FIM.csv (84 de 408) - 2025-11-07 16:52:24.637469\n",
        "# > lido: /content/drive/MyDrive/pesquisa/2025-09-04/ASM4765177v1_AMP.csv\n",
        "# df_amplicons: 3781\n",
        "# df_amplicons - STR duplicados: 666\n",
        "# df_amplicons - exclui amplicons duplicados: 655\n",
        "#   inicio filtro: 655\n",
        "#   filtro amplicons duplicados: 655\n",
        "#   filtro tamanho do amplicon: 128\n",
        "#    > gravado/filtrados: /content/drive/MyDrive/pesquisa/2025-09-04/ASM4765177v1_FIM.csv (100 de 655) - 2025-11-07 16:52:24.837377\n",
        "# >>> fim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coDthn8Z66q1"
      },
      "outputs": [],
      "source": [
        "# PASSO 11 - Gerar arquivo FASTA para conferir com BlastN NCBI\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# excluir todos os arquivos FASTA (*_FIM.fasta) da pasta de trabalho\n",
        "for fasta_file in glob.glob(os.path.join(pasta_analise, \"*_FIM.fasta\")):\n",
        "  os.remove(fasta_file)\n",
        "  print(f\"  > exclu√≠do {fasta_file}\")\n",
        "\n",
        "arquivos_amplicons = busca_arquivos('_FIM', ordenar_por=\"nome\", reverso=True)\n",
        "for idx_arquivo, arquivo in enumerate(arquivos_amplicons):\n",
        "  df_amplicons = pd.read_csv(arquivo, delimiter = \";\")\n",
        "  print(f'  > lido: {arquivo} ({len(df_amplicons)})')\n",
        "\n",
        "  for _, row in df_amplicons.iterrows():\n",
        "    origem = row['origem']\n",
        "    id_seq = f\"{row['index_amp']}-{row['unidade']}-{row['copias']}\"\n",
        "    seq = row[\"amplicon\"]\n",
        "    url_fasta = os.path.join(pasta_analise, f\"{origem}_FIM.fasta\")\n",
        "    with open(url_fasta, \"a\") as f:\n",
        "          f.write(f\">{id_seq}\\n{seq}\\n\")\n",
        "  print(f\"  > gravado {url_fasta}\")\n",
        "print(f\">>> fim\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAdrv_3LAJEK"
      },
      "outputs": [],
      "source": [
        "# busca rapida de arquivos\n",
        "# *_FIM.fasta Alignment\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import datetime\n",
        "\n",
        "print(f\"data atual: {datetime.datetime.now()}\")\n",
        "\n",
        "# tipo_arquivo = \"*_FIM.fasta\"\n",
        "tipo_arquivo = \"*Alignment.xml\"\n",
        "# tipo_arquivo = \"*_BLASTN*.csv\"-\n",
        "# tipo_arquivo = \"*_PRIMER.csv*\"\n",
        "# tipo_arquivo = \"*BLASTN.csv\"\n",
        "\n",
        "arquivos = glob.glob(os.path.join(pasta_analise, tipo_arquivo))\n",
        "print(f\"arquivos localizados: {len(arquivos)}\")\n",
        "for arquivo in arquivos:\n",
        "  timestamp_criacao = os.path.getctime(arquivo)\n",
        "  data_hora_criacao = datetime.datetime.fromtimestamp(timestamp_criacao)\n",
        "  print(f'{arquivo} {data_hora_criacao}')\n",
        "  # os.remove(arquivo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8LCbCAIH7hh"
      },
      "outputs": [],
      "source": [
        "# mover arquivos\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "from time import sleep\n",
        "import datetime\n",
        "\n",
        "# Caminho base onde os arquivos foram movidos\n",
        "pasta_arquivos = os.path.join(pasta_analise, 'fim_blast')\n",
        "# pasta_arquivos = os.path.join(pasta_analise, 'fim_blast')\n",
        "# pasta_arquivos = os.path.join(pasta_analise, 'blast_result')\n",
        "if not os.path.exists(pasta_arquivos):\n",
        "    os.makedirs(pasta_arquivos)\n",
        "    sleep(2)\n",
        "\n",
        "# tipo_arquivo = \"*_FIM.fasta\"\n",
        "# tipo_arquivo = \"*Alignment.xml\"\n",
        "tipo_arquivo = \"*BLASTN*.csv\"\n",
        "# tipo_arquivo = \"*_PRIMER.csv*\"\n",
        "\n",
        "arquivos_localizados = glob(os.path.join(pasta_analise, tipo_arquivo))\n",
        "print(f\"arquivos_localizados: {len(arquivos_localizados)}\")\n",
        "for arquivo in arquivos_localizados:\n",
        "  timestamp_criacao = os.path.getctime(arquivo)\n",
        "  data_hora_criacao = datetime.datetime.fromtimestamp(timestamp_criacao)\n",
        "  destino = os.path.join(pasta_arquivos, os.path.basename(arquivo))\n",
        "  # shutil.move(arquivo, destino)\n",
        "  # os.remove(arquivo)\n",
        "  print(f'   Movido para: {arquivo} ‚Üí {destino}')\n",
        "  print(f'     Cria√ß√£o: {data_hora_criacao}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwYWggpxYMxG",
        "outputId": "b994d72f-9ff6-4a09-d906-6105418d7df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: biopython in /usr/local/lib/python3.12/dist-packages (1.86)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython) (2.0.2)\n",
            "5 arquivos blastn localizados.\n",
            "  > lido: /content/drive/MyDrive/pesquisa/2025-09-04/ASM4765177v1_HBMEFTE6014-Alignment.xml)\n",
            "NCBIXML: Ignored: '\\nCREATE_VIEW\\n\\n\\n'\n",
            "NCBIXML: Ignored: '\\nCREATE_VIEW\\n\\n\\n'\n",
            "NCBIXML: Ignored: '\\nCREATE_VIEW\\n\\n\\n'\n",
            "NCBIXML: Ignored: '\\nCREATE_VIEW\\n\\n\\n'\n",
            "   > ASM4765177v1 Total de sequ√™ncias consultadas (queries): 100\n",
            "arquivos amplicons localizado: ['/content/drive/MyDrive/pesquisa/2025-09-04/ASM4765177v1_HBMEFTE6014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM2978392v1_HBM6CA75014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM2978390v1_HBM3H5EF014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM285v2_refseqgenomes-M402K4SV014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM1928827v1_HBKJYJJ9014-Alignment.xml']\n",
            "      ASM4765177v1 ‚ö†Ô∏è Nenhum alinhamento encontrado. origem: ASM4765177v1, unidade: AAACCCTA, copias: 12\n",
            "      ASM4765177v1 ‚ö†Ô∏è Nenhum alinhamento encontrado. origem: ASM4765177v1, unidade: TTTTAGGG, copias: 10\n",
            "      ASM4765177v1 ‚ö†Ô∏è Nenhum alinhamento encontrado. origem: ASM4765177v1, unidade: TTTTAGGG, copias: 6\n",
            "      ASM4765177v1 ‚ö†Ô∏è Nenhum alinhamento encontrado. origem: ASM4765177v1, unidade: CCTAAAAC, copias: 5\n",
            "      ASM4765177v1 ‚ö†Ô∏è Nenhum alinhamento encontrado. origem: ASM4765177v1, unidade: AAACCCTA, copias: 5\n",
            "      ASM4765177v1 ‚ö†Ô∏è Nenhum alinhamento encontrado. origem: ASM4765177v1, unidade: AAAACCT, copias: 5\n",
            "      ASM4765177v1 ‚ö†Ô∏è Nenhum alinhamento encontrado. origem: ASM4765177v1, unidade: CAGGCCT, copias: 4\n",
            "      ASM4765177v1 ‚ö†Ô∏è Nenhum alinhamento encontrado. origem: ASM4765177v1, unidade: TTTTAGGGT, copias: 3\n",
            "  > gravado: /content/drive/MyDrive/pesquisa/2025-09-04/ASM4765177v1_BLASTN.csv (8651)\n",
            "  > lido: /content/drive/MyDrive/pesquisa/2025-09-04/ASM2978392v1_HBM6CA75014-Alignment.xml)\n",
            "NCBIXML: Ignored: '\\nCREATE_VIEW\\n\\n\\n'\n",
            "NCBIXML: Ignored: '\\nCREATE_VIEW\\n\\n\\n'\n",
            "NCBIXML: Ignored: '\\nCREATE_VIEW\\n\\n\\n'\n",
            "NCBIXML: Ignored: '\\nCREATE_VIEW\\n\\n\\n'\n",
            "   > ASM2978392v1 Total de sequ√™ncias consultadas (queries): 100\n",
            "arquivos amplicons localizado: ['/content/drive/MyDrive/pesquisa/2025-09-04/ASM4765177v1_HBMEFTE6014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM2978392v1_HBM6CA75014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM2978390v1_HBM3H5EF014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM285v2_refseqgenomes-M402K4SV014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM1928827v1_HBKJYJJ9014-Alignment.xml']\n",
            "      ASM2978392v1 ‚ö†Ô∏è Nenhum alinhamento encontrado. origem: ASM2978392v1, unidade: TTTAGGGT, copias: 12\n",
            "      ASM2978392v1 ‚ö†Ô∏è Nenhum alinhamento encontrado. origem: ASM2978392v1, unidade: TAAAACCC, copias: 7\n",
            "      ASM2978392v1 ‚ö†Ô∏è Nenhum alinhamento encontrado. origem: ASM2978392v1, unidade: GAGCCT, copias: 6\n",
            "  > gravado: /content/drive/MyDrive/pesquisa/2025-09-04/ASM2978392v1_BLASTN.csv (10123)\n",
            "  > lido: /content/drive/MyDrive/pesquisa/2025-09-04/ASM2978390v1_HBM3H5EF014-Alignment.xml)\n",
            "NCBIXML: Ignored: '\\nCREATE_VIEW\\n\\n\\n'\n",
            "NCBIXML: Ignored: '\\nCREATE_VIEW\\n\\n\\n'\n",
            "NCBIXML: Ignored: '\\nCREATE_VIEW\\n\\n\\n'\n",
            "NCBIXML: Ignored: '\\nCREATE_VIEW\\n\\n\\n'\n",
            "   > ASM2978390v1 Total de sequ√™ncias consultadas (queries): 100\n",
            "arquivos amplicons localizado: ['/content/drive/MyDrive/pesquisa/2025-09-04/ASM4765177v1_HBMEFTE6014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM2978392v1_HBM6CA75014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM2978390v1_HBM3H5EF014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM285v2_refseqgenomes-M402K4SV014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM1928827v1_HBKJYJJ9014-Alignment.xml']\n",
            "  > gravado: /content/drive/MyDrive/pesquisa/2025-09-04/ASM2978390v1_BLASTN.csv (9259)\n",
            "  > lido: /content/drive/MyDrive/pesquisa/2025-09-04/ASM285v2_refseqgenomes-M402K4SV014-Alignment.xml)\n",
            "   > ASM285v2 Total de sequ√™ncias consultadas (queries): 100\n",
            "arquivos amplicons localizado: ['/content/drive/MyDrive/pesquisa/2025-09-04/ASM4765177v1_HBMEFTE6014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM2978392v1_HBM6CA75014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM2978390v1_HBM3H5EF014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM285v2_refseqgenomes-M402K4SV014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM1928827v1_HBKJYJJ9014-Alignment.xml']\n",
            "ASM285v2 - NT_166524\n",
            "ASM285v2 - NW_020290889\n",
            "ASM285v2 - NW_020290889\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290886\n",
            "ASM285v2 - NT_166524\n",
            "ASM285v2 - NW_020290889\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NT_166533\n",
            "ASM285v2 - NW_020290929\n",
            "ASM285v2 - NW_020290929\n",
            "ASM285v2 - NT_166524\n",
            "ASM285v2 - NW_020290903\n",
            "ASM285v2 - NT_166527\n",
            "ASM285v2 - NW_020290908\n",
            "ASM285v2 - NW_020290908\n",
            "ASM285v2 - NT_166520\n",
            "ASM285v2 - NT_166523\n",
            "ASM285v2 - NW_020290892\n",
            "ASM285v2 - NW_020290892\n",
            "ASM285v2 - NT_166539\n",
            "ASM285v2 - NW_020290888\n",
            "ASM285v2 - NT_166518\n",
            "ASM285v2 - NW_020290920\n",
            "ASM285v2 - NT_166526\n",
            "ASM285v2 - NW_020290894\n",
            "ASM285v2 - NT_166524\n",
            "ASM285v2 - NW_020290889\n",
            "ASM285v2 - NW_020290889\n",
            "ASM285v2 - NT_166520\n",
            "ASM285v2 - NT_166520\n",
            "ASM285v2 - NW_020290890\n",
            "ASM285v2 - NW_020290890\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290884\n",
            "ASM285v2 - NT_166530\n",
            "ASM285v2 - NW_020290894\n",
            "ASM285v2 - NW_020290894\n",
            "ASM285v2 - NT_166520\n",
            "ASM285v2 - NW_020290890\n",
            "ASM285v2 - NT_166539\n",
            "ASM285v2 - NW_020290932\n",
            "ASM285v2 - NW_020290932\n",
            "ASM285v2 - NT_166528\n",
            "ASM285v2 - NW_020290887\n",
            "ASM285v2 - NT_166524\n",
            "ASM285v2 - NW_020290942\n",
            "ASM285v2 - NW_020290942\n",
            "ASM285v2 - NT_166524\n",
            "ASM285v2 - NT_166518\n",
            "ASM285v2 - NW_020290920\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290884\n",
            "ASM285v2 - NT_166524\n",
            "ASM285v2 - NT_166524\n",
            "ASM285v2 - NW_020290889\n",
            "ASM285v2 - NT_166520\n",
            "ASM285v2 - NW_020290901\n",
            "ASM285v2 - NW_020290901\n",
            "ASM285v2 - NW_020290901\n",
            "ASM285v2 - NW_020290901\n",
            "ASM285v2 - NT_166520\n",
            "ASM285v2 - NW_020290890\n",
            "ASM285v2 - NW_020290890\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NW_020290933\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290884\n",
            "ASM285v2 - NT_166530\n",
            "ASM285v2 - NW_020290905\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290884\n",
            "ASM285v2 - NT_166530\n",
            "ASM285v2 - NW_020290905\n",
            "ASM285v2 - NT_166533\n",
            "ASM285v2 - NW_020290898\n",
            "ASM285v2 - NW_020290898\n",
            "ASM285v2 - NT_166518\n",
            "ASM285v2 - NW_020290893\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290928\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NW_020290934\n",
            "ASM285v2 - NT_166518\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NT_166518\n",
            "ASM285v2 - NW_020290926\n",
            "ASM285v2 - NT_166518\n",
            "ASM285v2 - NW_020290891\n",
            "ASM285v2 - NT_166530\n",
            "ASM285v2 - NW_020290900\n",
            "ASM285v2 - NW_020290900\n",
            "ASM285v2 - NT_166522\n",
            "ASM285v2 - NW_020290944\n",
            "ASM285v2 - NW_020290944\n",
            "ASM285v2 - NW_020290884\n",
            "ASM285v2 - NW_020290933\n",
            "ASM285v2 - NW_020290886\n",
            "ASM285v2 - NW_020290886\n",
            "ASM285v2 - NW_020290930\n",
            "ASM285v2 - NW_020290893\n",
            "ASM285v2 - NW_020290887\n",
            "ASM285v2 - NW_020290887\n",
            "ASM285v2 - NW_020290901\n",
            "ASM285v2 - NT_166520\n",
            "ASM285v2 - NT_166520\n",
            "ASM285v2 - NT_166520\n",
            "ASM285v2 - NW_020290890\n",
            "ASM285v2 - NW_020290986\n",
            "ASM285v2 - NT_166532\n",
            "ASM285v2 - NW_020290902\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290886\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290884\n",
            "ASM285v2 - NT_166524\n",
            "ASM285v2 - NW_020290889\n",
            "ASM285v2 - NT_166539\n",
            "ASM285v2 - NW_020290940\n",
            "ASM285v2 - NW_020290919\n",
            "ASM285v2 - NT_166518\n",
            "ASM285v2 - NW_020290908\n",
            "ASM285v2 - NT_166527\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290886\n",
            "ASM285v2 - NW_020290886\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290886\n",
            "ASM285v2 - NT_166518\n",
            "ASM285v2 - NW_020290917\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290886\n",
            "ASM285v2 - NW_020290893\n",
            "ASM285v2 - NT_166518\n",
            "ASM285v2 - NT_166518\n",
            "ASM285v2 - NW_020290920\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290935\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NT_166533\n",
            "ASM285v2 - NW_020290896\n",
            "ASM285v2 - NT_166524\n",
            "ASM285v2 - NT_166524\n",
            "ASM285v2 - NT_166524\n",
            "ASM285v2 - NW_020290954\n",
            "ASM285v2 - NW_020290942\n",
            "ASM285v2 - NW_020290942\n",
            "ASM285v2 - NT_166530\n",
            "ASM285v2 - NW_020290905\n",
            "ASM285v2 - NW_020290884\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290886\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NW_020290884\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NT_166524\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290884\n",
            "ASM285v2 - NW_020290886\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NW_020290884\n",
            "ASM285v2 - NW_020290884\n",
            "ASM285v2 - NW_020290884\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NW_020290887\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NT_166530\n",
            "ASM285v2 - NW_020290905\n",
            "ASM285v2 - NW_020290905\n",
            "ASM285v2 - NW_020290884\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290935\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NW_020290895\n",
            "ASM285v2 - NW_020290895\n",
            "ASM285v2 - NT_166526\n",
            "ASM285v2 - NT_166526\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NW_020290904\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290928\n",
            "ASM285v2 - NW_020290887\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290884\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290884\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NT_166520\n",
            "ASM285v2 - NW_020290901\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290884\n",
            "ASM285v2 - NT_166518\n",
            "ASM285v2 - NT_166520\n",
            "ASM285v2 - NT_166520\n",
            "ASM285v2 - NT_166518\n",
            "ASM285v2 - NW_020290919\n",
            "ASM285v2 - NW_020290919\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NW_020290885\n",
            "ASM285v2 - NT_166531\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NW_020290884\n",
            "ASM285v2 - NW_020290886\n",
            "ASM285v2 - NT_166519\n",
            "ASM285v2 - NT_166533\n",
            "ASM285v2 - NW_020290898\n",
            "  > gravado: /content/drive/MyDrive/pesquisa/2025-09-04/ASM285v2_BLASTN.csv (246)\n",
            "  > lido: /content/drive/MyDrive/pesquisa/2025-09-04/ASM1928827v1_HBKJYJJ9014-Alignment.xml)\n",
            "NCBIXML: Ignored: '\\nCREATE_VIEW\\n\\n\\n'\n",
            "NCBIXML: Ignored: '\\nCREATE_VIEW\\n\\n\\n'\n",
            "NCBIXML: Ignored: '\\nCREATE_VIEW\\n\\n\\n'\n",
            "NCBIXML: Ignored: '\\nCREATE_VIEW\\n\\n\\n'\n",
            "   > ASM1928827v1 Total de sequ√™ncias consultadas (queries): 100\n",
            "arquivos amplicons localizado: ['/content/drive/MyDrive/pesquisa/2025-09-04/ASM4765177v1_HBMEFTE6014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM2978392v1_HBM6CA75014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM2978390v1_HBM3H5EF014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM285v2_refseqgenomes-M402K4SV014-Alignment.xml', '/content/drive/MyDrive/pesquisa/2025-09-04/ASM1928827v1_HBKJYJJ9014-Alignment.xml']\n",
            "      ASM1928827v1 ‚ö†Ô∏è Nenhum alinhamento encontrado. origem: ASM1928827v1, unidade: TC, copias: 64\n",
            "      ASM1928827v1 ‚ö†Ô∏è Nenhum alinhamento encontrado. origem: ASM1928827v1, unidade: CCAGAG, copias: 15\n",
            "      ASM1928827v1 ‚ö†Ô∏è Nenhum alinhamento encontrado. origem: ASM1928827v1, unidade: ACTTCCCAG, copias: 9\n",
            "  > gravado: /content/drive/MyDrive/pesquisa/2025-09-04/ASM1928827v1_BLASTN.csv (9468)\n",
            ">>> fim\n"
          ]
        }
      ],
      "source": [
        "# PASSO 12 - Ler arquivos (XML) retornados do BlastN NCBI e filtrar os alinhamentos\n",
        "!pip install biopython\n",
        "import os\n",
        "from Bio.Blast import NCBIXML\n",
        "\n",
        "# arquivos_blastn = busca_arquivos('ASM285v2_HRCSWB4N016-Alignment', extensao='xml', ordenar_por=\"nome\", reverso=True)\n",
        "arquivos_blastn = busca_arquivos('Alignment', extensao='xml', ordenar_por=\"nome\", reverso=True)\n",
        "print(f\"{len(arquivos_blastn)} arquivos blastn localizados.\")\n",
        "\n",
        "for idx_arquivo, arquivo in enumerate(arquivos_blastn):\n",
        "  # df_amplicons = pd.read_csv(arquivo, delimiter = \";\")\n",
        "  print(f'  > lido: {arquivo})')\n",
        "  nome_arquivo = os.path.basename(arquivo) # Obter append o nome do arquivo sem o diret√≥rio\n",
        "  origem = nome_arquivo.split('_')[0]\n",
        "  dicionario = df_amostras.set_index(\"id\")[\"Cepa (strain)\"].to_dict()\n",
        "  cepa = dicionario.get(origem)\n",
        "  dicionario_loc = df_amostras.set_index(\"id\")[\"Cepa (strain)\"].to_dict()\n",
        "  cepa_origem = dicionario_loc.get(origem)\n",
        "  # print(f'  > lido: {arquivo} ({len(df_amplicons)})')\n",
        "\n",
        "  # abre e faz o parsing\n",
        "  with open(arquivo) as handle:\n",
        "      blast_records = list(NCBIXML.parse(handle))  # lista com um registro por query\n",
        "\n",
        "  print(f\"   > {origem} Total de sequ√™ncias consultadas (queries): {len(blast_records)}\")\n",
        "\n",
        "  # busca arquivo amplicons que gerou os arquivo .fasta\n",
        "  arquivo_amplicons = f'{pasta_analise}{origem}_FIM.csv'\n",
        "  df_amplicons = pd.read_csv(arquivo_amplicons, delimiter = \";\")\n",
        "  print(f\"arquivos amplicons localizado: {arquivos_blastn}\")\n",
        "\n",
        "  # percorrer cada sequ√™ncia da query\n",
        "  resultados = []\n",
        "  for index_record, record in enumerate(blast_records, start=1):\n",
        "      query_id = record.query.split()[0]\n",
        "      sequencial_id = int(query_id.split('-')[0])\n",
        "      unidade = query_id.split('-')[1]\n",
        "      copias = query_id.split('-')[2]\n",
        "      amplicon = getattr(record, \"query_sequence\", \"\")\n",
        "\n",
        "      df_query = df_amplicons.query(\"index_amp == @sequencial_id\")\n",
        "      if not df_query.empty:\n",
        "        _df_amplicom = df_query.iloc[0]\n",
        "      else:\n",
        "        print(f\"   {origem} ‚ö†Ô∏è Nenhuma linha encontrada com ID == {sequencial_id} no arquivo {arquivo_amplicons}\")\n",
        "        continue  # pula para a pr√≥xima sequ√™ncia\n",
        "      # mostrar o melhor alinhamento (se existir)\n",
        "      if record.alignments:\n",
        "          best_alignment = record.alignments[0]\n",
        "          hsp = best_alignment.hsps[0]\n",
        "          # print(f\"      > Acesso (accession): {best_alignment.accession}\")\n",
        "          # print(f\"      > T√≠tulo: {best_alignment.title}\")\n",
        "          # print(f\"      > Identidade: {hsp.identities}/{hsp.align_length} ({hsp.identities / hsp.align_length:.1%})\")\n",
        "          # print(f\"      > E-value: {hsp.expect}\")\n",
        "\n",
        "          sequencial_alinhamento = 0\n",
        "          quantidade_cepas_localizadas = 0\n",
        "          ultima_cepa_localizada = {}\n",
        "          for alignment in record.alignments:\n",
        "            for hsp in alignment.hsps:\n",
        "              sequencial_alinhamento = sequencial_alinhamento + 1\n",
        "\n",
        "              cepa_loc = \"\"\n",
        "              is_cepa_localizada = False\n",
        "              if cepa_origem in alignment.title:\n",
        "                is_cepa_localizada = True\n",
        "                cepa_loc = cepa_origem\n",
        "              else:\n",
        "                if origem == 'ASM285v2':\n",
        "                  print(f'{origem} - {alignment.accession}')\n",
        "                if alignment.accession in df_amostras.query('id == @origem')[\"Scaffolds\"]:\n",
        "                  is_cepa_localizada = True\n",
        "                  cepa_loc = alignment.accession\n",
        "\n",
        "              # tamanho_amplicon = len(_df_amplicom['amplicon'])\n",
        "              tamanho_amplicon = _df_amplicom['tamanho_amplicon']\n",
        "              is_tamanhos_exatos = False\n",
        "              # if hsp.align_length == hsp.identities:\n",
        "              if hsp.align_length == tamanho_amplicon:\n",
        "                is_tamanhos_exatos = True\n",
        "\n",
        "              percent_identity = (hsp.identities / hsp.align_length) * 100\n",
        "\n",
        "              # Um amplicon pode alinhar em mais de uma organismo/cepa do banco de dados\n",
        "              #     - nome completo da cepa (alinhamento somente com a cepa verdadeira)\n",
        "              #         is_cepa_localizada\n",
        "              #     - tamanhos exatos entre o resultado de alinhamento x identities\n",
        "              #         hsp.align_length == hsp.identities ou hsp.align_length == tamanho_amplicon\n",
        "              #     - identidade\n",
        "              #         percent_identity == 100\n",
        "              is_identico = False\n",
        "              if is_cepa_localizada and is_tamanhos_exatos and percent_identity == 100:\n",
        "                is_identico = True\n",
        "\n",
        "              novaLinha = _df_amplicom.to_dict()\n",
        "\n",
        "              novaLinha['origem_'] = origem;\n",
        "              novaLinha['cepa'] = cepa;\n",
        "              novaLinha['texto_cepa_localizada'] = cepa_loc;\n",
        "              novaLinha['unidade_'] = unidade;\n",
        "              novaLinha['copias_s'] = copias;\n",
        "              novaLinha['query_id'] = query_id;\n",
        "              novaLinha['query_record'] = record.query;\n",
        "              novaLinha['numero_alinhamentos'] = len(record.alignments);\n",
        "              novaLinha['sequencial_alinhamento'] = sequencial_alinhamento;\n",
        "              novaLinha['accession_number'] = alignment.accession;\n",
        "              novaLinha['alignment_title'] = alignment.title;\n",
        "              novaLinha['alignment_hit_id'] = alignment.hit_id;\n",
        "              novaLinha['alignment_hit_def'] = alignment.hit_def;\n",
        "              novaLinha['alignment_length'] = alignment.length;\n",
        "              novaLinha['score'] = hsp.score;\n",
        "              novaLinha['bits'] = hsp.bits;\n",
        "              novaLinha['e-value'] = hsp.expect;\n",
        "              novaLinha['query'] = hsp.query;\n",
        "              novaLinha['match'] = hsp.match;\n",
        "              novaLinha['sbjct'] = hsp.sbjct;\n",
        "              novaLinha['query_start'] = hsp.query_start;\n",
        "              novaLinha['query_end'] = hsp.query_end;\n",
        "              novaLinha['sbjct_start'] = hsp.sbjct_start;\n",
        "              novaLinha['sbjct_end'] = hsp.sbjct_end;\n",
        "              novaLinha['identities'] = hsp.identities;\n",
        "              novaLinha['positives'] = hsp.positives;\n",
        "              novaLinha['gaps'] = hsp.gaps;\n",
        "              novaLinha['align_length'] = hsp.align_length;\n",
        "              novaLinha['percent_identity'] = percent_identity;\n",
        "              novaLinha['is_cepa_localizada'] = is_cepa_localizada;\n",
        "              novaLinha['is_tamanhos_exatos'] = is_tamanhos_exatos;\n",
        "              novaLinha['is_identico'] = is_identico;\n",
        "\n",
        "              resultados.append(novaLinha)\n",
        "      else:\n",
        "          print(f\"      {origem} ‚ö†Ô∏è Nenhum alinhamento encontrado. origem: {origem}, unidade: {unidade}, copias: {copias}\")\n",
        "\n",
        "  df_alinhamentos = pd.DataFrame(resultados)\n",
        "  url_arquivo = f'{pasta_analise}{origem}_BLASTN.csv'\n",
        "  df_alinhamentos.to_csv(url_arquivo, sep=';', index=False)\n",
        "  print(f'  > gravado: {url_arquivo} ({len(df_alinhamentos)})')\n",
        "\n",
        "print(f'>>> fim')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ASM285v2_scaffolds_validos = [\n",
        "#     \"NT_166518\", \"NT_166519\", \"NT_166520\", \"NT_166521\",\n",
        "#     \"NT_166522\", \"NT_166523\", \"NT_166524\", \"NT_166525\",\n",
        "#     \"NT_166526\", \"NT_166527\", \"NT_166528\", \"NT_166529\",\n",
        "#     \"NT_166530\", \"NT_166531\", \"NT_166532\", \"NT_166533\",\n",
        "#     \"NT_166537\", \"NT_166538\", \"NT_166539\"\n",
        "# ]\n",
        "df_amostras\n",
        "# df_amostras.query(\"id == 'ASM285v2'\")[\"Scaffolds\"]\n",
        "# if alignment.accession in df_amostras.query('id == ASM285v2')[\"Scaffolds\"]:\n",
        "#   is_cepa_localizada = True\n",
        "#   cepa_loc = alignment.hit_id\n",
        "\n",
        "#   accession_number"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "DniVGnhUZAxC",
        "outputId": "5259860b-ba40-4917-cf17-967cad49fea5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             id Nome da Montagem Refer√™ncia GenBank WGS Accession  \\\n",
              "0      ASM285v2       ASM285v2 *    GCA_000002855.2    PRJNA19275   \n",
              "1  ASM1928827v1     ASM1928827v1    GCA_019288275.1      JAGRPH01   \n",
              "2  ASM2978390v1     ASM2978390v1    GCA_029783905.1      JAPVRD01   \n",
              "3  ASM2978392v1     ASM2978392v1    GCA_029783925.1      JAPVRE01   \n",
              "4  ASM4765177v1     ASM4765177v1    GCA_047651775.1      JBKZXA01   \n",
              "\n",
              "  Cepa (strain)  \\\n",
              "0    CBS 513.88   \n",
              "1    CBS 554.65   \n",
              "2          KJC3   \n",
              "3          KYF3   \n",
              "4  CCTCC 206047   \n",
              "\n",
              "                                                                                                                                                                                                           Scaffolds  \\\n",
              "0  [NT_166518, NT_166519, NT_166520, NT_166521, NT_166522, NT_166523, NT_166524, NT_166525, NT_166526, NT_166527, NT_166528, NT_166529, NT_166530, NT_166531, NT_166532, NT_166533, NT_166537, NT_166538, NT_166539]   \n",
              "1                                                                                                                                                                                                                 []   \n",
              "2                                                                                                                                                                                                                 []   \n",
              "3                                                                                                                                                                                                                 []   \n",
              "4                                                                                                                                                                                                                 []   \n",
              "\n",
              "                 Submissor N√≠vel de montagem  Tamanho (Mb)  \\\n",
              "0     DSM, The Netherlands          Scaffold            34   \n",
              "1                  TU Wien        Chromosome            40   \n",
              "2      Soongsil University        Chromosome            40   \n",
              "3      Soongsil University        Chromosome            37   \n",
              "4  Zhejiang Uni.Technology          Complete            35   \n",
              "\n",
              "                                                                                                                              Dados  \n",
              "0          /content/drive/MyDrive/fasta/aspergillus/ASM285v2/ncbi_dataset/data/GCA_000002855.2/GCA_000002855.2_ASM285v2_genomic.fna  \n",
              "1  /content/drive/MyDrive/fasta/aspergillus/ASM1928827v1/ncbi_dataset/data/GCA_019288275.1/GCA_019288275.1_ASM1928827v1_genomic.fna  \n",
              "2  /content/drive/MyDrive/fasta/aspergillus/ASM2978390v1/ncbi_dataset/data/GCA_029783905.1/GCA_029783905.1_ASM2978390v1_genomic.fna  \n",
              "3  /content/drive/MyDrive/fasta/aspergillus/ASM2978392v1/ncbi_dataset/data/GCA_029783925.1/GCA_029783925.1_ASM2978392v1_genomic.fna  \n",
              "4  /content/drive/MyDrive/fasta/aspergillus/ASM4765177v1/ncbi_dataset/data/GCA_047651775.1/GCA_047651775.1_ASM4765177v1_genomic.fna  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5ab19ad9-492c-455a-895a-f22c54cf022a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Nome da Montagem</th>\n",
              "      <th>Refer√™ncia GenBank</th>\n",
              "      <th>WGS Accession</th>\n",
              "      <th>Cepa (strain)</th>\n",
              "      <th>Scaffolds</th>\n",
              "      <th>Submissor</th>\n",
              "      <th>N√≠vel de montagem</th>\n",
              "      <th>Tamanho (Mb)</th>\n",
              "      <th>Dados</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ASM285v2</td>\n",
              "      <td>ASM285v2 *</td>\n",
              "      <td>GCA_000002855.2</td>\n",
              "      <td>PRJNA19275</td>\n",
              "      <td>CBS 513.88</td>\n",
              "      <td>[NT_166518, NT_166519, NT_166520, NT_166521, NT_166522, NT_166523, NT_166524, NT_166525, NT_166526, NT_166527, NT_166528, NT_166529, NT_166530, NT_166531, NT_166532, NT_166533, NT_166537, NT_166538, NT_166539]</td>\n",
              "      <td>DSM, The Netherlands</td>\n",
              "      <td>Scaffold</td>\n",
              "      <td>34</td>\n",
              "      <td>/content/drive/MyDrive/fasta/aspergillus/ASM285v2/ncbi_dataset/data/GCA_000002855.2/GCA_000002855.2_ASM285v2_genomic.fna</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ASM1928827v1</td>\n",
              "      <td>ASM1928827v1</td>\n",
              "      <td>GCA_019288275.1</td>\n",
              "      <td>JAGRPH01</td>\n",
              "      <td>CBS 554.65</td>\n",
              "      <td>[]</td>\n",
              "      <td>TU Wien</td>\n",
              "      <td>Chromosome</td>\n",
              "      <td>40</td>\n",
              "      <td>/content/drive/MyDrive/fasta/aspergillus/ASM1928827v1/ncbi_dataset/data/GCA_019288275.1/GCA_019288275.1_ASM1928827v1_genomic.fna</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ASM2978390v1</td>\n",
              "      <td>ASM2978390v1</td>\n",
              "      <td>GCA_029783905.1</td>\n",
              "      <td>JAPVRD01</td>\n",
              "      <td>KJC3</td>\n",
              "      <td>[]</td>\n",
              "      <td>Soongsil University</td>\n",
              "      <td>Chromosome</td>\n",
              "      <td>40</td>\n",
              "      <td>/content/drive/MyDrive/fasta/aspergillus/ASM2978390v1/ncbi_dataset/data/GCA_029783905.1/GCA_029783905.1_ASM2978390v1_genomic.fna</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ASM2978392v1</td>\n",
              "      <td>ASM2978392v1</td>\n",
              "      <td>GCA_029783925.1</td>\n",
              "      <td>JAPVRE01</td>\n",
              "      <td>KYF3</td>\n",
              "      <td>[]</td>\n",
              "      <td>Soongsil University</td>\n",
              "      <td>Chromosome</td>\n",
              "      <td>37</td>\n",
              "      <td>/content/drive/MyDrive/fasta/aspergillus/ASM2978392v1/ncbi_dataset/data/GCA_029783925.1/GCA_029783925.1_ASM2978392v1_genomic.fna</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ASM4765177v1</td>\n",
              "      <td>ASM4765177v1</td>\n",
              "      <td>GCA_047651775.1</td>\n",
              "      <td>JBKZXA01</td>\n",
              "      <td>CCTCC 206047</td>\n",
              "      <td>[]</td>\n",
              "      <td>Zhejiang Uni.Technology</td>\n",
              "      <td>Complete</td>\n",
              "      <td>35</td>\n",
              "      <td>/content/drive/MyDrive/fasta/aspergillus/ASM4765177v1/ncbi_dataset/data/GCA_047651775.1/GCA_047651775.1_ASM4765177v1_genomic.fna</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ab19ad9-492c-455a-895a-f22c54cf022a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5ab19ad9-492c-455a-895a-f22c54cf022a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5ab19ad9-492c-455a-895a-f22c54cf022a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e3d9c74c-12a9-4dd7-9c5e-8cc3ff9fcc9c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e3d9c74c-12a9-4dd7-9c5e-8cc3ff9fcc9c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e3d9c74c-12a9-4dd7-9c5e-8cc3ff9fcc9c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_a1a863c6-b12f-4c7c-b6b7-c9e8434a1f8a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_amostras')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a1a863c6-b12f-4c7c-b6b7-c9e8434a1f8a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_amostras');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_amostras",
              "summary": "{\n  \"name\": \"df_amostras\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"ASM1928827v1\",\n          \"ASM4765177v1\",\n          \"ASM2978390v1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Nome da Montagem\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"ASM1928827v1\",\n          \"ASM4765177v1\",\n          \"ASM2978390v1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Refer\\u00eancia GenBank\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"GCA_019288275.1\",\n          \"GCA_047651775.1\",\n          \"GCA_029783905.1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"WGS Accession\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"JAGRPH01\",\n          \"JBKZXA01\",\n          \"JAPVRD01\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cepa (strain)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"CBS 554.65\",\n          \"CCTCC 206047\",\n          \"KJC3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Scaffolds\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Submissor\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"TU Wien\",\n          \"Zhejiang Uni.Technology\",\n          \"DSM, The Netherlands\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"N\\u00edvel de montagem\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Scaffold\",\n          \"Chromosome\",\n          \"Complete\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tamanho (Mb)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 34,\n        \"max\": 40,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          40,\n          35,\n          34\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dados\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"/content/drive/MyDrive/fasta/aspergillus/ASM1928827v1/ncbi_dataset/data/GCA_019288275.1/GCA_019288275.1_ASM1928827v1_genomic.fna\",\n          \"/content/drive/MyDrive/fasta/aspergillus/ASM4765177v1/ncbi_dataset/data/GCA_047651775.1/GCA_047651775.1_ASM4765177v1_genomic.fna\",\n          \"/content/drive/MyDrive/fasta/aspergillus/ASM2978390v1/ncbi_dataset/data/GCA_029783905.1/GCA_029783905.1_ASM2978390v1_genomic.fna\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brCXIyJ44u--",
        "outputId": "2a8dfc0d-7f87-47c8-93fb-ac28401f69eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arquivos localizados: 5\n",
            "/content/drive/MyDrive/pesquisa/2025-09-04/ASM4765177v1_BLASTN.csv 2025-12-18 16:35:50\n",
            "  > gravado: /content/drive/MyDrive/pesquisa/2025-09-04/ASM4765177v1_BLASTN-FILTRO.csv (18)\n",
            "/content/drive/MyDrive/pesquisa/2025-09-04/ASM2978392v1_BLASTN.csv 2025-12-18 16:44:24\n",
            "  > gravado: /content/drive/MyDrive/pesquisa/2025-09-04/ASM2978392v1_BLASTN-FILTRO.csv (30)\n",
            "/content/drive/MyDrive/pesquisa/2025-09-04/ASM2978390v1_BLASTN.csv 2025-12-18 16:44:53\n",
            "  > gravado: /content/drive/MyDrive/pesquisa/2025-09-04/ASM2978390v1_BLASTN-FILTRO.csv (56)\n",
            "/content/drive/MyDrive/pesquisa/2025-09-04/ASM285v2_BLASTN.csv 2025-12-18 17:04:31\n",
            "  > gravado: /content/drive/MyDrive/pesquisa/2025-09-04/ASM285v2_BLASTN-FILTRO.csv (0)\n",
            "/content/drive/MyDrive/pesquisa/2025-09-04/ASM1928827v1_BLASTN.csv 2025-12-18 17:04:31\n",
            "  > gravado: /content/drive/MyDrive/pesquisa/2025-09-04/ASM1928827v1_BLASTN-FILTRO.csv (43)\n",
            ">> fim\n"
          ]
        }
      ],
      "source": [
        "# PASSO 14.1 filtro sobre os resultados\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import datetime\n",
        "\n",
        "tipo_arquivo = \"*BLASTN.csv\"\n",
        "\n",
        "arquivos = glob.glob(os.path.join(pasta_analise, tipo_arquivo))\n",
        "print(f\"arquivos localizados: {len(arquivos)}\")\n",
        "\n",
        "for arquivo in arquivos:\n",
        "  timestamp_criacao = os.path.getctime(arquivo)\n",
        "  data_hora_criacao = datetime.datetime.fromtimestamp(timestamp_criacao)\n",
        "  print(f'{arquivo} {data_hora_criacao}')\n",
        "\n",
        "  nome_arquivo = os.path.basename(arquivo)\n",
        "  origem = nome_arquivo.split('_')[0]\n",
        "\n",
        "  df = pd.read_csv(arquivo, delimiter = \";\")\n",
        "  # 1 - Filtrar pelos crit√©rios\n",
        "  df_filtrado = df[\n",
        "      (df[\"percent_identity\"] == 100)\n",
        "      # & (df[\"align_length\"] == df[\"tamanho_amplicon\"])\n",
        "      & (df[\"is_tamanhos_exatos\"] == True)\n",
        "  ]\n",
        "\n",
        "  # url_arquivo_filtro_1 = f'{pasta_analise}{origem}_BLASTN-FILTRO-1.csv'\n",
        "  # df_filtrado.to_csv(url_arquivo_filtro_1, sep=';', index=False)\n",
        "  # print(f'  > gravado: {url_arquivo_filtro_1} ({len(df_filtrado)})')\n",
        "\n",
        "  # 2 - remover duplicidade de cepa localizada\n",
        "  # -------------------------------------------------------\n",
        "  # 2) APLICAR A REGRA PRINCIPAL:\n",
        "  #    \"Se houver QUALQUER outra cepa, descarta o index_amp\"\n",
        "  # -------------------------------------------------------\n",
        "\n",
        "  # index_amp v√°lidos s√£o aqueles onde TODO MUNDO √© localizado\n",
        "  index_validos = (\n",
        "      df_filtrado\n",
        "      .groupby(\"index_amp\")[\"is_cepa_localizada\"]\n",
        "      .apply(lambda s: s.all())     # True s√≥ se TODAS as linhas forem True\n",
        "      .loc[lambda s: s]             # mant√©m apenas True\n",
        "      .index\n",
        "  )\n",
        "  # -------------------------------------------------------\n",
        "  # 3) FILTRAR APENAS OS index_amp V√ÅLIDOS\n",
        "  # -------------------------------------------------------\n",
        "  df_apenas_localizada = df_filtrado[df_filtrado[\"index_amp\"].isin(index_validos)]\n",
        "\n",
        "  # -------------------------------------------------------\n",
        "  # 4) REMOVER DUPLICATAS DENTRO DA MESMA CEPA LOCALIZADA (opcional)\n",
        "  # -------------------------------------------------------\n",
        "  df_resultado = df_apenas_localizada.drop_duplicates(\n",
        "      subset=[\"index_amp\"],  # s√≥ para garantir um registro por index_amp\n",
        "      keep=\"first\"\n",
        "  )\n",
        "\n",
        "  url_arquivo_filtro_2 = f'{pasta_analise}{origem}_BLASTN-FILTRO.csv'\n",
        "  df_resultado.to_csv(url_arquivo_filtro_2, sep=';', index=False)\n",
        "  print(f'  > gravado: {url_arquivo_filtro_2} ({len(df_resultado)})')\n",
        "print(f'>> fim')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# passo 13 - validar cepa principal pois o ban\n",
        "\n",
        "scaffolds_validos = {\n",
        "    \"NT_166518.1\", \"NT_166519.1\", \"NT_166520.1\", \"NT_166521.1\",\n",
        "    \"NT_166522.1\", \"NT_166523.1\", \"NT_166524.1\", \"NT_166525.1\",\n",
        "    \"NT_166526.1\", \"NT_166527.1\", \"NT_166528.1\", \"NT_166529.1\",\n",
        "    \"NT_166530.1\", \"NT_166531.1\", \"NT_166532.1\", \"NT_166533.1\",\n",
        "    \"NT_166537.1\", \"NT_166538.4\", \"NT_166539.1\"\n",
        "}\n",
        "\n",
        "def pertence_asm285(hit_id, hit_def):\n",
        "    texto = f\"{hit_id} {hit_def}\"\n",
        "    return any(acc in texto for acc in scaffolds_validos)"
      ],
      "metadata": {
        "id": "l7g56jGNQ13b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PASSO 14 tabelas de resultado - tabela 6\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import datetime\n",
        "\n",
        "tipo_arquivo = \"*_BLASTN-FILTRO.csv\"\n",
        "arquivos = glob.glob(os.path.join(pasta_analise, tipo_arquivo))\n",
        "print(f\"arquivos localizados: {len(arquivos)}\")\n",
        "\n",
        "for arquivo in arquivos:\n",
        "  df = pd.read_csv(arquivo, sep=\";\")\n",
        "  print(f'  > lido: {arquivo})')\n",
        "  nome_arquivo = os.path.basename(arquivo)\n",
        "  origem = nome_arquivo.split('_')[0]\n",
        "\n",
        "  # criar IDs para os LOCUS (STR01, STR02, ...)\n",
        "  df = df.reset_index(drop=True)\n",
        "  df[\"Locus\"] = [f\"STR{i+1:02d}\" for i in range(len(df))]\n",
        "  # print(df)\n",
        "\n",
        "  colunas_resumo = [\n",
        "      \"Locus\",\n",
        "      \"unidade\",\n",
        "      \"inicio_loci\",\n",
        "      \"fim_loci\",\n",
        "      \"copias\",\n",
        "      \"tamanho_amplicon\",\n",
        "      \"score\",\n",
        "      \"e-value\"\n",
        "  ]\n",
        "\n",
        "  tabela_principal = df[colunas_resumo].copy()\n",
        "\n",
        "  # Renomear colunas para apresenta√ß√£o\n",
        "  tabela_principal = tabela_principal.rename(columns={\n",
        "      \"unidade\": \"Unidade STR\",\n",
        "      \"inicio_loci\": \"In√≠cio\",\n",
        "      \"fim_loci\": \"Fim\",\n",
        "      \"copias\": \"N¬∫ de c√≥pias\",\n",
        "      \"tamanho_amplicon\": \"Tamanho (bp)\",\n",
        "      \"score\": \"Score\",\n",
        "      \"e-value\": \"E-value\"\n",
        "  })\n",
        "\n",
        "  colunas_suplementar = [\n",
        "      \"Locus\",\n",
        "      \"primer_left\",\n",
        "      \"primer_right\",\n",
        "      \"amplicon\"\n",
        "  ]\n",
        "\n",
        "  tabela_suplementar = df[colunas_suplementar].copy()\n",
        "\n",
        "  tabela_suplementar = tabela_suplementar.rename(columns={\n",
        "      \"primer_left\": \"Primer esquerdo (5'‚Äì3')\",\n",
        "      \"primer_right\": \"Primer direito (5'‚Äì3')\",\n",
        "      \"amplicon\": \"Amplicon (5'‚Äì3')\"\n",
        "  })\n",
        "\n",
        "  url_arquivo_resumo = f'{pasta_analise}{origem}_TABELA-RESUMO.csv'\n",
        "  tabela_principal.to_csv(url_arquivo_resumo, sep=';', index=False)\n",
        "  print(f'  > gravado: {url_arquivo_resumo} ({len(tabela_principal)})')\n",
        "\n",
        "  url_arquivo_suplementar = f'{pasta_analise}{origem}_TABELA-SUPLEMENTAR.csv'\n",
        "  tabela_suplementar.to_csv(url_arquivo_suplementar, sep=';', index=False)\n",
        "  print(f'  > gravado: {url_arquivo_suplementar} ({len(tabela_suplementar)})')\n",
        "\n",
        "print(\">>> fim\")"
      ],
      "metadata": {
        "id": "iDExsTJD3uM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "id": "4gyR6nV19JA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJyhnOCSklPq"
      },
      "outputs": [],
      "source": [
        "# analise sobre a sa√≠da\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Total de alinhamentos positivos\")\n",
        "arquivos_blastn = busca_arquivos('_BLASTN_FILTRO', extensao='csv', ordenar_por=\"nome\", reverso=True)\n",
        "for idx_arquivo, arquivo in enumerate(arquivos_blastn):\n",
        "  if os.path.getsize(arquivo) == 0:\n",
        "    print(f\"   > ignorado (vazio): {nome_arquivo}\")\n",
        "    continue\n",
        "\n",
        "  nome_arquivo = os.path.basename(arquivo) # Obter apenas o nome do arquivo sem o diret√≥rio\n",
        "  origem = nome_arquivo.split('_')[0]\n",
        "\n",
        "  try:\n",
        "    df_alinhamentos = pd.read_csv(arquivo, delimiter = \";\")\n",
        "    print(f\"   > lido: {nome_arquivo}\")\n",
        "    if 'index_amp' in df_alinhamentos.columns:\n",
        "        total_alinhamentos = df_alinhamentos['index_amp'].nunique()\n",
        "        print(f\"   > {origem}: {total_alinhamentos} de {len(df_alinhamentos)}\")\n",
        "    else:\n",
        "        print(f\"   > {origem}: coluna 'index_amp' n√£o encontrada\")\n",
        "  except pd.errors.EmptyDataError:\n",
        "    print(f\"   > erro ao ler (sem dados): {nome_arquivo}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEi3qb9cViE-"
      },
      "outputs": [],
      "source": [
        "# PASSO 13 - gerar imagens -> visualizar STR, flankeadas primers e amplicon\n",
        "# 05-11-2025\n",
        "\n",
        "def safe_str(val):\n",
        "    return \"\" if pd.isna(val) else str(val)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "# from str_utils import busca_arquivos, busca_sequencia_fasta, create_flank, gravar_arquivo_saida\n",
        "\n",
        "arquivos_blastn = busca_arquivos('_FIM', ordenar_por=\"nome\", reverso=True)\n",
        "for index_arquivo, arquivo in enumerate(arquivos_blastn):\n",
        "  # df_amplicons = pd.read_csv(arquivo, delimiter = \";\")\n",
        "  print(f'  > lido: {arquivo}')\n",
        "  nome_arquivo = os.path.basename(arquivo) # Obter apenas o nome do arquivo sem o diret√≥rio\n",
        "  origem = nome_arquivo.split('_')[0]\n",
        "  df_blast = pd.read_csv(arquivo, delimiter = \";\")\n",
        "\n",
        "  for index_blast, alinhamento in df_blast.iterrows():\n",
        "    borda_esquerda = safe_str(alinhamento['borda_esquerda'])\n",
        "    montante       = safe_str(alinhamento['montante'])\n",
        "    str_seq        = safe_str(alinhamento['fragment'])\n",
        "    jusante        = safe_str(alinhamento['jusante'])\n",
        "    borda_right    = safe_str(alinhamento['borda_direita'])\n",
        "\n",
        "# ajuste acima\n",
        "    # borda_esquerda = alinhamento['borda_esquerda']\n",
        "    # montante = alinhamento['montante']\n",
        "    # str_seq = alinhamento['fragment']\n",
        "    # jusante = alinhamento['jusante']\n",
        "    # # borda_right = alinhamento['borda_direita'] aki\n",
        "    # borda_right = \"\" if pd.isna(borda_right) else str(borda_right)\n",
        "\n",
        "    unidade_str = alinhamento['unidade']\n",
        "    primer_left = alinhamento['primer_left']\n",
        "    primer_right = alinhamento['primer_right']\n",
        "    primer_forward = alinhamento['primer_right']\n",
        "    tipo_str = alinhamento['tipo']\n",
        "    offset = alinhamento['inicio_amostral']\n",
        "    amplicon = alinhamento['amplicon']\n",
        "    # origem = alinhamento['origem']\n",
        "    index_amp = alinhamento['index_amp']\n",
        "\n",
        "    pasta_imagens = os.path.join(pasta_analise, 'imagens', origem)\n",
        "    print(f'pasta_imagens: {pasta_imagens}')\n",
        "    arquivo_png = os.path.join(pasta_imagens, f\"{origem}_schema_{index_amp}.png\")\n",
        "    if os.path.exists(arquivo_png):\n",
        "      print(f\"Arquivo j√° existe: {arquivo_png}\")\n",
        "      pasta_imagens\n",
        "      continue\n",
        "    if not os.path.exists(pasta_imagens):\n",
        "        os.makedirs(pasta_imagens)\n",
        "        sleep(10)\n",
        "\n",
        "    #  > fragment:\tSequ√™ncia entre inicio e fim\n",
        "    #  > montante:\tRegi√£o anterior ao fragmento, com tamanho m√≠nimo de 200\n",
        "    #  > jusante:\tRegi√£o posterior ao fragmento, com tamanho m√≠nimo de 200\n",
        "    #  > full_flank:\tFlanqueadoras + fragmento central\n",
        "    #  > borda_esquerda:\tRegi√£o antes da flanqueadora esquerda (200 bases)\n",
        "    #  > borda_direita:\tRegi√£o ap√≥s a flanqueadora direita (200 bases)\n",
        "    #  > inicio_amostral:\t√çndice inicial da sequ√™ncia total estendida\n",
        "    #  > fim_amostral:\t√çndice final da sequ√™ncia total estendida\n",
        "\n",
        "    try:\n",
        "      seq = str(borda_esquerda) + str(montante) + str(str_seq) + str(jusante) + str(borda_right)\n",
        "      seq = seq.upper()\n",
        "    except Exception as e:\n",
        "      print(\"Erro ao concatenar seq:\", e)\n",
        "      print(f\"borda_esquerda: {borda_esquerda} | montante: {montante} | str_seq: {str_seq} | jusante: {jusante} | borda_right: {borda_right}\")\n",
        "      seq = \"\"  # fallback vazio\n",
        "\n",
        "\n",
        "    # print(f'seq: {len(seq)} borda_esquerda: {len(borda_esquerda)} montante: {len(montante)} str_seq: {len(str_seq)} jusante: {len(jusante)} borda_right: {len(borda_right)}')\n",
        "\n",
        "    # Posi√ß√µes das regi√µes\n",
        "    montante_start = len(borda_esquerda)\n",
        "    montante_end = montante_start + len(montante)\n",
        "    str_start = montante_end\n",
        "    str_end = str_start + len(str_seq)\n",
        "    jusante_start = str_end\n",
        "    jusante_end = jusante_start + len(jusante)\n",
        "\n",
        "    # print(f'montante_start: {montante_start} montante_end: {montante_end} str_start: {str_start} str_end: {str_end} jusante_start: {jusante_start} jusante_end: {jusante_end}')\n",
        "    # print(f'amplicon ({len(amplicon)}): {amplicon}')\n",
        "    # print(f'primer_left ({len(primer_left)}): {primer_left}')\n",
        "    # print(f'primer_right ({len(primer_right)}): {primer_right}')\n",
        "    # print(f\"primer_right_loc ({len(alinhamento['primer_right_loc'])}): {alinhamento['primer_right_loc']}\")\n",
        "\n",
        "    # Posi√ß√µes dos primers dentro das regi√µes flanqueadoras\n",
        "    primer_left_start = len(borda_esquerda) + alinhamento['primer_left_start']\n",
        "    amplicon_start = primer_left_start\n",
        "    amplicon_end = amplicon_start + len(amplicon)\n",
        "    primer_left_end = primer_left_start + len(alinhamento['primer_left'])\n",
        "\n",
        "    primer_forward_start = amplicon_end - len(primer_right)\n",
        "    primer_forward_end = amplicon_end\n",
        "\n",
        "    # Mapear bases para n√∫meros\n",
        "    bases = {'A':1, 'T':2, 'G':3, 'C':4, 'N':5}\n",
        "    base_colors = {'A':'red', 'T':'blue', 'G':'green', 'C':'orange', 'N':'white'}\n",
        "\n",
        "    # Criar stem plot\n",
        "    fig, ax = plt.subplots(figsize=(25,4.5))\n",
        "\n",
        "    # Fundo das regi√µes\n",
        "    ax.axvspan(0, montante_start, color='lightgray', alpha=0.2)\n",
        "    ax.axvspan(montante_start, montante_end, color='skyblue', alpha=0.2)\n",
        "    ax.axvspan(str_start, str_end, color='gold', alpha=0.2)\n",
        "    ax.axvspan(jusante_start, jusante_end, color='lightgreen', alpha=0.2)\n",
        "    ax.axvspan(jusante_end, len(seq), color='lightgray', alpha=0.2)\n",
        "\n",
        "    # linhas verticais pontilhadas\n",
        "    # for pos in [montante_start, montante_end, str_end, jusante_end, primer_left_start, primer_left_end, primer_forward_start, primer_forward_end]:\n",
        "    for pos in [montante_start, montante_end, jusante_start, jusante_end]:\n",
        "        ax.axvline(x=pos, color='black', linestyle='--', linewidth=1)\n",
        "\n",
        "    # Stem plot por base\n",
        "    # for i, base in enumerate(seq):\n",
        "    #     ax.stem([i+1], [bases[base]], linefmt=base_colors[base], markerfmt='', basefmt=\" \")\n",
        "\n",
        "    for i, base in enumerate(seq):\n",
        "      b = base.upper()\n",
        "      ax.stem([i+1],\n",
        "              [bases.get(b, 0)],  # se n√£o existir, usa 0\n",
        "              linefmt=base_colors.get(b, 'gray'),  # se n√£o existir, usa cor cinza\n",
        "              markerfmt='',\n",
        "              basefmt=\" \")\n",
        "\n",
        "    # Labels das regi√µes no topo\n",
        "    ax.text(montante_start + len(montante)/2, 5.6, f\"Montante ({len(montante)})\", ha='center', fontsize=12, fontweight='bold')\n",
        "    ax.text(str_start + len(str_seq)/2, 5.6, f'STR-{tipo_str}: {unidade_str}  ({len(str_seq)})', ha='center', fontsize=12, fontweight='bold')\n",
        "    ax.text(jusante_start + len(jusante)/2, 5.6, f\"Jusante ({len(jusante)})\", ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Labels dos primers acima dos ret√¢ngulos dos primers\n",
        "    ax.text(primer_left_start + (primer_left_end-primer_left_start)/2, 5.1, f\"Primer Left ({len(primer_left)})\", ha='center', fontsize=12, fontweight='bold', color='purple')\n",
        "    ax.text(primer_forward_start + (primer_forward_end-primer_forward_start)/2, 5.1, f\"Primer Right ({len(primer_forward)})\", ha='center', fontsize=12, fontweight='bold', color='brown')\n",
        "\n",
        "    # Primer Left e Forward abaixo de regi√µes flankeadoras\n",
        "    primer_y_bottom = 4.7\n",
        "    primer_y_top = 5\n",
        "    ax.fill_betweenx([primer_y_bottom, primer_y_top], primer_left_start, primer_left_end, color='purple', alpha=0.6)\n",
        "    ax.fill_betweenx([primer_y_bottom, primer_y_top], primer_forward_start, primer_forward_end, color='brown', alpha=0.6)\n",
        "\n",
        "    # Legenda do primer_left\n",
        "    hd_l = hamming_distance(alinhamento['primer_left'], alinhamento['primer_left_loc'])\n",
        "    ax.text(primer_left_start - 3, primer_y_top -0.2, f\"{alinhamento['primer_left']}\",\n",
        "            ha='right', va='center', fontname='monospace', fontsize=12, color='purple')\n",
        "    label_primer_left_loc = f\"{alinhamento['primer_left_loc']})\"\n",
        "    if hd_l == 0:\n",
        "      cor = 'purple'\n",
        "    else:\n",
        "      cor = 'red'\n",
        "      label_primer_left_loc += f\" ({hd_l})\"\n",
        "\n",
        "    ax.text(primer_left_start - 3, primer_y_top - 0.5, label_primer_left_loc,\n",
        "            ha='right', va='center', fontname='monospace', fontsize=12, color=f'{cor}')\n",
        "\n",
        "    # Legenda do primer_right\n",
        "    hd_r = hamming_distance(alinhamento['primer_right'], alinhamento['primer_right_loc'])\n",
        "    ax.text(primer_forward_end + 3, primer_y_top -0.2, f\"{alinhamento['primer_right']}\",\n",
        "            ha='left', va='center', fontname='monospace', fontsize=12, color='brown')\n",
        "    label_primer_right_loc = f\"{alinhamento['primer_right_loc']}\"\n",
        "    if hd_r == 0:\n",
        "      cor = 'brown'\n",
        "    else:\n",
        "      cor = 'red'\n",
        "      label_primer_right_loc += f\" ({hd_r})\"\n",
        "    ax.text(primer_forward_end + 3, primer_y_top -0.5, label_primer_right_loc, # primer_right_loc\n",
        "            ha='left', va='center', fontname='monospace', fontsize=12, color=f'{cor}')\n",
        "\n",
        "\n",
        "    # Amplicon (produto amplificado) abaixo dos primers\n",
        "    amplicon_y_bottom = 4.1\n",
        "    amplicon_y_top = 4.4 # aki linha abaixo\n",
        "    ax.fill_betweenx([amplicon_y_bottom, amplicon_y_top], primer_left_start, amplicon_end, color='black', alpha=0.5)\n",
        "    ax.text(primer_left_start + (primer_forward_end-primer_left_start)/2, 4.55, f\"Amplicon ({len(amplicon)})\", ha='center', fontsize=12, fontweight='bold', color='black')\n",
        "\n",
        "    # Ajustes visuais\n",
        "    ax.set_ylim(0,6)\n",
        "    ax.set_yticks([1,2,3,4])\n",
        "    ax.set_yticklabels(['A','T','G','C'])\n",
        "\n",
        "    # ticks horizontais - x_ticks\n",
        "    xticks_personalizados = [0,\n",
        "                            montante_start,\n",
        "                            primer_left_start,\n",
        "                            primer_left_end,\n",
        "                            montante_end,\n",
        "                            str_start,\n",
        "                            str_end,\n",
        "                            primer_forward_start,\n",
        "                            primer_forward_end,\n",
        "                            jusante_end,\n",
        "                            jusante_end + len(borda_right)]\n",
        "\n",
        "    ax.set_xticks(xticks_personalizados)\n",
        "    ax.set_xticklabels([''] * len(xticks_personalizados))  # oculta os r√≥tulos padr√£o\n",
        "\n",
        "    # Ajusta o offset na da posicao na sequencia original (x_tics)\n",
        "    for i, tick in enumerate(xticks_personalizados):\n",
        "        label = f\"{int(tick + offset):,}\".replace(\",\", \".\")\n",
        "        y_offset = -0.04\n",
        "        if i in [2, 7]:\n",
        "          y_offset = -0.08\n",
        "        if i in [3, 8]:\n",
        "          y_offset = -0.12\n",
        "        ax.text(tick, y_offset, label,\n",
        "                ha='center', va='top',\n",
        "                transform=ax.get_xaxis_transform(),\n",
        "                fontsize=10, color='black')\n",
        "\n",
        "    ax.set_xlabel(f\"Posi√ß√£o das por√ß√µes destacadas na sequ√™ncia FASTA original da amostra {origem}\", labelpad=33, fontsize=12, fontweight='bold')\n",
        "    ax.tick_params(axis='x', labelsize=14, width=2, length=8)\n",
        "\n",
        "    ax.set_ylabel(f\"Nucleot√≠deos\", labelpad=5, fontsize=12, fontweight='bold')\n",
        "    ax.tick_params(axis='y', labelsize=14, width=2, length=8)\n",
        "\n",
        "    base_colors = {'A': 'red', 'C': 'orange', 'T': 'blue', 'G': 'green'}\n",
        "    legend_bases = [Patch(color=base_colors[base], label=base) for base in ['A', 'T', 'G', 'C']]\n",
        "    # legend_total = legend_bases + legend_areas\n",
        "    ax.legend(handles=legend_bases , title=\"Bases\", loc='upper left')\n",
        "\n",
        "    plt.savefig(arquivo_png, bbox_inches='tight', pad_inches=0, dpi=300)\n",
        "    print(f\"   gravado: {arquivo_png}\")\n",
        "  # plt.show()\n",
        "print(f\">>> fim\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nv_kC_zGEth"
      },
      "outputs": [],
      "source": [
        "# PASSO XX - Analise dos primers\n",
        "# Montar aqruivo FASTA para testar primers contra uma sequ√™ncia alvo para verificar especificidade e efici√™ncia.\n",
        "\n",
        "# https://www.ncbi.nlm.nih.gov/tools/primer-blast/\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9phBKMKkVVX"
      },
      "outputs": [],
      "source": [
        "past# PASSO 10.1 - visualizar STR, flankeadas primers e amplicon\n",
        "# 15-09-2025 21-09-2025 10-10-2025 NameError: name 'primer_right' is not defined\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import matplotlib.ticker as ticker\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "import pandas as pd\n",
        "# from str_utils import busca_arquivos, busca_sequencia_fasta, create_flank, gravar_arquivo_saida\n",
        "\n",
        "#####################################################\n",
        "url_arquivo = pasta_analise + 'FIM_full.csv'\n",
        "df_primer = pd.read_csv(url_arquivo, delimiter = \";\")\n",
        "df_primer = df_primer.sort_values(by='tamanho_amplicon', ascending=False)\n",
        "primer_sel = df_primer.iloc[0]\n",
        "#####################################################\n",
        "\n",
        "borda_esquerda = primer_sel['borda_esquerda']\n",
        "montante = primer_sel['montante']\n",
        "str_seq = primer_sel['fragment']\n",
        "jusante = primer_sel['jusante']\n",
        "borda_right = primer_sel['borda_direita']\n",
        "unidade_str = primer_sel['unidade']\n",
        "primer_left = primer_sel['primer_left']\n",
        "primer_right = primer_sel['primer_right']\n",
        "primer_forward = primer_sel['primer_right']\n",
        "tipo_str = primer_sel['tipo']\n",
        "offset = primer_sel['inicio_amostral']\n",
        "amplicon = primer_sel['amplicon']\n",
        "origem = primer_sel['origem']\n",
        "\n",
        "#  > fragment:\tSequ√™ncia entre inicio e fim\n",
        "#  > montante:\tRegi√£o anterior ao fragmento, com tamanho m√≠nimo de 200\n",
        "#  > jusante:\tRegi√£o posterior ao fragmento, com tamanho m√≠nimo de 200\n",
        "#  > full_flank:\tFlanqueadoras + fragmento central\n",
        "#  > borda_esquerda:\tRegi√£o antes da flanqueadora esquerda (200 bases)\n",
        "#  > borda_direita:\tRegi√£o ap√≥s a flanqueadora direita (200 bases)\n",
        "#  > inicio_amostral:\t√çndice inicial da sequ√™ncia total estendida\n",
        "#  > fim_amostral:\t√çndice final da sequ√™ncia total estendida\n",
        "\n",
        "seq = borda_esquerda + montante + str_seq + jusante + borda_right\n",
        "print(f'seq: {len(seq)} borda_esquerda: {len(borda_esquerda)} montante: {len(montante)} str_seq: {len(str_seq)} jusante: {len(jusante)} borda_right: {len(borda_right)}')\n",
        "\n",
        "# Posi√ß√µes das regi√µes\n",
        "montante_start = len(borda_esquerda)\n",
        "montante_end = montante_start + len(montante)\n",
        "str_start = montante_end\n",
        "str_end = str_start + len(str_seq)\n",
        "jusante_start = str_end\n",
        "jusante_end = jusante_start + len(jusante)\n",
        "\n",
        "print(f'montante_start: {montante_start} montante_end: {montante_end} str_start: {str_start} str_end: {str_end} jusante_start: {jusante_start} jusante_end: {jusante_end}')\n",
        "print(f'amplicon ({len(amplicon)}): {amplicon}')\n",
        "print(f'primer_left ({len(primer_left)}): {primer_left}')\n",
        "print(f'primer_right ({len(primer_right)}): {primer_right}')\n",
        "print(f\"primer_right_loc ({len(primer_sel['primer_right_loc'])}): {primer_sel['primer_right_loc']}\")\n",
        "\n",
        "# Posi√ß√µes dos primers dentro das regi√µes flanqueadoras\n",
        "primer_left_start = len(borda_esquerda) + primer_sel['primer_left_start']\n",
        "amplicon_start = primer_left_start\n",
        "amplicon_end = amplicon_start + len(amplicon)\n",
        "primer_left_end = primer_left_start + len(primer_sel['primer_left'])\n",
        "\n",
        "primer_forward_start = amplicon_end - len(primer_right)\n",
        "primer_forward_end = amplicon_end\n",
        "\n",
        "\n",
        "# Mapear bases para n√∫meros\n",
        "bases = {'A':1, 'T':2, 'G':3, 'C':4}\n",
        "base_colors = {'A':'red', 'T':'blue', 'G':'green', 'C':'orange'}\n",
        "\n",
        "# Criar stem plot\n",
        "fig, ax = plt.subplots(figsize=(25,4.5))\n",
        "\n",
        "# Fundo das regi√µes\n",
        "ax.axvspan(0, montante_start, color='lightgray', alpha=0.2)\n",
        "ax.axvspan(montante_start, montante_end, color='skyblue', alpha=0.2)\n",
        "ax.axvspan(str_start, str_end, color='gold', alpha=0.2)\n",
        "ax.axvspan(jusante_start, jusante_end, color='lightgreen', alpha=0.2)\n",
        "ax.axvspan(jusante_end, len(seq), color='lightgray', alpha=0.2)\n",
        "\n",
        "# linhas verticais pontilhadas\n",
        "# for pos in [montante_start, montante_end, str_end, jusante_end, primer_left_start, primer_left_end, primer_forward_start, primer_forward_end]:\n",
        "for pos in [montante_start, montante_end, jusante_start, jusante_end]:\n",
        "    ax.axvline(x=pos, color='black', linestyle='--', linewidth=1)\n",
        "\n",
        "# Stem plot por base\n",
        "for i, base in enumerate(seq):\n",
        "    ax.stem([i+1], [bases[base]], linefmt=base_colors[base], markerfmt='', basefmt=\" \")\n",
        "\n",
        "# Labels das regi√µes no topo\n",
        "ax.text(montante_start + len(montante)/2, 5.6, f\"Montante ({len(montante)})\", ha='center', fontsize=12, fontweight='bold')\n",
        "ax.text(str_start + len(str_seq)/2, 5.6, f'STR-{tipo_str}: {unidade_str}  ({len(str_seq)})', ha='center', fontsize=12, fontweight='bold')\n",
        "ax.text(jusante_start + len(jusante)/2, 5.6, f\"Jusante ({len(jusante)})\", ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Labels dos primers acima dos ret√¢ngulos dos primers\n",
        "ax.text(primer_left_start + (primer_left_end-primer_left_start)/2, 5.1, f\"Primer Left ({len(primer_left)})\", ha='center', fontsize=12, fontweight='bold', color='purple')\n",
        "ax.text(primer_forward_start + (primer_forward_end-primer_forward_start)/2, 5.1, f\"Primer Right ({len(primer_forward)})\", ha='center', fontsize=12, fontweight='bold', color='brown')\n",
        "\n",
        "# Primer Left e Forward abaixo de regi√µes flankeadoras\n",
        "primer_y_bottom = 4.7\n",
        "primer_y_top = 5\n",
        "ax.fill_betweenx([primer_y_bottom, primer_y_top], primer_left_start, primer_left_end, color='purple', alpha=0.6)\n",
        "ax.fill_betweenx([primer_y_bottom, primer_y_top], primer_forward_start, primer_forward_end, color='brown', alpha=0.6)\n",
        "\n",
        "# Legenda do primer_left\n",
        "ax.text(primer_left_start - 3, primer_y_top -0.2, f\"{primer_sel['primer_left']}\",\n",
        "        ha='right', va='center', fontname='monospace', fontsize=12, color='purple')\n",
        "if primer_sel['primer_left'] == primer_sel['primer_left_loc']:\n",
        "  cor = 'purple'\n",
        "else:\n",
        "  cor = 'red'\n",
        "ax.text(primer_left_start - 3, primer_y_top - 0.5, f\"{primer_sel['primer_left_loc']}\",\n",
        "        ha='right', va='center', fontname='monospace', fontsize=12, color=f'{cor}')\n",
        "\n",
        "# Legenda do primer_right\n",
        "ax.text(primer_forward_end + 3, primer_y_top -0.2, f\"{primer_sel['primer_right']}\",\n",
        "        ha='left', va='center', fontname='monospace', fontsize=12, color='brown')\n",
        "if primer_sel['primer_right'] == primer_sel['primer_right_loc']:\n",
        "  cor = 'brown'\n",
        "else:\n",
        "  cor = 'red'\n",
        "ax.text(primer_forward_end + 3, primer_y_top -0.5, f\"{primer_sel['primer_right_loc']}\", # primer_right_loc\n",
        "        ha='left', va='center', fontname='monospace', fontsize=12, color=f'{cor}')\n",
        "\n",
        "\n",
        "# Amplicon (produto amplificado) abaixo dos primers\n",
        "amplicon_y_bottom = 4.1\n",
        "amplicon_y_top = 4.4 # aki linha abaixo\n",
        "ax.fill_betweenx([amplicon_y_bottom, amplicon_y_top], primer_left_start, amplicon_end, color='black', alpha=0.5)\n",
        "ax.text(primer_left_start + (primer_forward_end-primer_left_start)/2, 4.55, f\"Amplicon ({len(amplicon)})\", ha='center', fontsize=12, fontweight='bold', color='black')\n",
        "\n",
        "# Ajustes visuais\n",
        "ax.set_ylim(0,6)\n",
        "ax.set_yticks([1,2,3,4])\n",
        "ax.set_yticklabels(['A','T','G','C'])\n",
        "\n",
        "# ticks horizontais - x_ticks\n",
        "xticks_personalizados = [0,\n",
        "                         montante_start,\n",
        "                         primer_left_start,\n",
        "                         primer_left_end,\n",
        "                         montante_end,\n",
        "                         str_start,\n",
        "                         str_end,\n",
        "                         primer_forward_start,\n",
        "                         primer_forward_end,\n",
        "                         jusante_end,\n",
        "                         jusante_end + len(borda_right)]\n",
        "\n",
        "ax.set_xticks(xticks_personalizados)\n",
        "ax.set_xticklabels([''] * len(xticks_personalizados))  # oculta os r√≥tulos padr√£o\n",
        "\n",
        "# Ajusta o offset na da posicao na sequencia original (x_tics)\n",
        "for i, tick in enumerate(xticks_personalizados):\n",
        "    label = f\"{int(tick + offset):,}\".replace(\",\", \".\")\n",
        "    y_offset = -0.04\n",
        "    if i in [2, 7]:\n",
        "      y_offset = -0.08\n",
        "    if i in [3, 8]:\n",
        "      y_offset = -0.12\n",
        "    ax.text(tick, y_offset, label,\n",
        "            ha='center', va='top',\n",
        "            transform=ax.get_xaxis_transform(),\n",
        "            fontsize=10, color='black')\n",
        "\n",
        "ax.set_xlabel(f\"Posi√ß√£o das por√ß√µes destacadas na sequ√™ncia FASTA original da amostra {origem}\", labelpad=33, fontsize=12, fontweight='bold')\n",
        "ax.tick_params(axis='x', labelsize=14, width=2, length=8)\n",
        "\n",
        "ax.set_ylabel(f\"Nucleot√≠deos\", labelpad=5, fontsize=12, fontweight='bold')\n",
        "ax.tick_params(axis='y', labelsize=14, width=2, length=8)\n",
        "\n",
        "base_colors = {'A': 'red', 'C': 'orange', 'T': 'blue', 'G': 'green'}\n",
        "legend_bases = [Patch(color=base_colors[base], label=base) for base in ['A', 'T', 'G', 'C']]\n",
        "# legend_total = legend_bases + legend_areas\n",
        "ax.legend(handles=legend_bases , title=\"Bases\", loc='upper left')\n",
        "\n",
        "arquivo_png = \"/content/drive/MyDrive/pesquisa/desenho_primers_2.png\"\n",
        "plt.savefig(arquivo_png, bbox_inches='tight', pad_inches=0, dpi=300)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peztYfsehVUx"
      },
      "outputs": [],
      "source": [
        "# PASSO 10.1.a - visualizar primers - primer left em rela√ßao montante\n",
        "# 15-09-2025 21-09-2025 25-09-2025\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#####################################################\n",
        "url_arquivo = pasta_analise + 'FIM.csv'\n",
        "df_primer = pd.read_csv(url_arquivo, delimiter = \";\")\n",
        "primer_sel = df_primer.iloc[0]\n",
        "#####################################################\n",
        "montante = primer_sel['montante']\n",
        "offset = primer_sel['inicio_amostral'] + len(primer_sel['borda_esquerda'] )\n",
        "\n",
        "#  > fragment:\tSequ√™ncia entre inicio e fim\n",
        "#  > montante:\tRegi√£o anterior ao fragmento, com tamanho m√≠nimo de 200\n",
        "#  > jusante:\tRegi√£o posterior ao fragmento, com tamanho m√≠nimo de 200\n",
        "#  > full_flank:\tFlanqueadoras + fragmento central\n",
        "#  > border_left:\tRegi√£o antes da flanqueadora esquerda (200 bases)\n",
        "#  > border_right:\tRegi√£o ap√≥s a flanqueadora direita (200 bases)\n",
        "#  > inicio_amostral:\t√çndice inicial da sequ√™ncia total estendida\n",
        "#  > fim_amostral:\t√çndice final da sequ√™ncia total estendida\n",
        "\n",
        "seq = montante\n",
        "\n",
        "# Posi√ß√µes das regi√µes\n",
        "montante_start = 0\n",
        "montante_end = montante_start + len(montante)\n",
        "\n",
        "primer_left_start = primer_sel['primer_left_start']\n",
        "primer_left_end = primer_left_start + len(primer_sel['primer_left'])\n",
        "\n",
        "# Mapear bases para n√∫meros\n",
        "bases = {'A':1, 'T':2, 'G':3, 'C':4}\n",
        "base_colors = {'A':'red', 'T':'blue', 'G':'green', 'C':'orange'}\n",
        "\n",
        "# Criar stem plot\n",
        "fig, ax = plt.subplots(figsize=(25,4.5))\n",
        "\n",
        "# Fundo das regi√µes\n",
        "ax.axvspan(primer_left_start + 0.5, primer_left_end + 0.5, color='purple', alpha=0.2)\n",
        "\n",
        "# linhas verticais pontilhadas\n",
        "for pos in [primer_left_start + 0.5, primer_left_end + 0.5]:\n",
        "    ax.axvline(x=pos, color='black', linestyle='--', linewidth=1)\n",
        "\n",
        "# Stem plot por base\n",
        "for i, base in enumerate(seq):\n",
        "    # ax.stem([i+1], [bases[base]], linefmt=base_colors[base], markerfmt='', basefmt=\" \"\n",
        "    markerline, stemlines, baseline = ax.stem([i+1], [bases[base]], linefmt=base_colors[base], markerfmt='', basefmt=\" \")\n",
        "    plt.setp(stemlines, linewidth=5)\n",
        "\n",
        "# Labels das regi√µes no topo\n",
        "ax.text(montante_start + len(montante)/2, 5.6, f\"Montante ({len(montante)})\", ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Primer Left\n",
        "primer_y_bottom = 4.7\n",
        "primer_y_top = 5\n",
        "ax.fill_betweenx([primer_y_bottom, primer_y_top], primer_left_start + 0.5, primer_left_end + 0.5, color='purple', alpha=0.6)\n",
        "\n",
        "# Labels dos primers acima dos ret√¢ngulos dos primers\n",
        "ax.text(primer_left_start + (primer_left_end-primer_left_start)/2, 5.1, f\"Primer Left {len(primer_sel['primer_left'])}\", ha='center', fontsize=12, fontweight='bold', color='purple')\n",
        "\n",
        "ax.text(primer_left_start - 3, primer_y_top -0.2, f\"{primer_sel['primer_left']}\",\n",
        "        ha='right', va='center', fontname='monospace', fontsize=12, color='purple')\n",
        "if primer_sel['primer_left'] == primer_sel['primer_left_loc']:\n",
        "  cor = 'purple'\n",
        "else:\n",
        "  cor = 'red'\n",
        "ax.text(primer_left_start - 3, primer_y_top - 0.5, f\"{primer_sel['primer_left_loc']}\",\n",
        "        ha='right', va='center', fontname='monospace', fontsize=12, color=f'{cor}')\n",
        "\n",
        "# Ajustes visuais\n",
        "ax.set_ylim(0,6)\n",
        "ax.set_yticks([1,2,3,4])\n",
        "ax.set_yticklabels(['A','T','G','C'])\n",
        "\n",
        "\n",
        "xticks_personalizados = [0,\n",
        "                         montante_start,\n",
        "                         primer_left_start,\n",
        "                         primer_left_end,\n",
        "                         montante_end]\n",
        "\n",
        "ax.set_xticks(xticks_personalizados)\n",
        "ax.set_xticklabels([''] * len(xticks_personalizados))  # oculta os r√≥tulos padr√£o\n",
        "\n",
        "# Ajusta o offset na da posicao na sequencia original (x_tics)\n",
        "for i, tick in enumerate(xticks_personalizados):\n",
        "    label = f\"{int(tick + offset):,}\".replace(\",\", \".\")\n",
        "    y_offset = -0.04\n",
        "    ax.text(tick, y_offset, label,\n",
        "            ha='center', va='top',\n",
        "            transform=ax.get_xaxis_transform(),\n",
        "            fontsize=10, color='black')\n",
        "\n",
        "ax.set_xlabel(f\"Posi√ß√£o das por√ß√µes destacadas na sequ√™ncia FASTA original da amostra {origem}\", labelpad=33, fontsize=12, fontweight='bold')\n",
        "ax.tick_params(axis='x', labelsize=14, width=2, length=8)\n",
        "\n",
        "ax.set_ylabel(f\"Nucleot√≠deos\", labelpad=5, fontsize=12, fontweight='bold')\n",
        "ax.tick_params(axis='y', labelsize=14, width=2, length=8)\n",
        "\n",
        "base_colors = {'A': 'red', 'C': 'orange', 'T': 'blue', 'G': 'green'}\n",
        "legend_bases = [Patch(color=base_colors[base], label=base) for base in ['A', 'T', 'G', 'C']]\n",
        "# legend_total = legend_bases + legend_areas\n",
        "ax.legend(handles=legend_bases , title=\"Bases\", loc='upper left')\n",
        "\n",
        "\n",
        "arquivo_png = \"/content/drive/MyDrive/pesquisa/desenho_primer_left_montante.png\"\n",
        "plt.savefig(arquivo_png, bbox_inches='tight', pad_inches=0, dpi=300)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dTvqFLV3EHr"
      },
      "outputs": [],
      "source": [
        "# PASSO 10.1.b - visualizar primers - primer right em rela√ßao ao jusante\n",
        "# 15-09-2025 21-09-2025 25-09-2025  ax.stem([i+1], [bases[base]], linefmt=base_colors[base], marke\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#####################################################\n",
        "url_arquivo = pasta_analise + 'FIM.csv'\n",
        "df_primer = pd.read_csv(url_arquivo, delimiter = \";\")\n",
        "primer_sel = df_primer.iloc[0]\n",
        "#####################################################\n",
        "montante = primer_sel['montante']\n",
        "jusante = primer_sel['jusante']\n",
        "str_seq = primer_sel['fragment']\n",
        "offset = primer_sel['inicio_amostral'] + len(primer_sel['borda_esquerda']) + len(montante) + len(str_seq)\n",
        "\n",
        "seq = jusante\n",
        "\n",
        "# Posi√ß√µes das regi√µes\n",
        "jusante_start = 0\n",
        "jusante_end = jusante_start + len(jusante)\n",
        "\n",
        "primer_right_start = primer_sel['primer_right_start'] - len(montante) - len(str_seq)\n",
        "primer_right_end = primer_right_start + len(primer_sel['primer_right'])\n",
        "\n",
        "# Mapear bases para n√∫meros\n",
        "bases = {'A':1, 'T':2, 'G':3, 'C':4}\n",
        "base_colors = {'A':'red', 'T':'blue', 'G':'green', 'C':'orange'}\n",
        "\n",
        "# Criar stem plot\n",
        "fig, ax = plt.subplots(figsize=(25,4.5))\n",
        "\n",
        "# Fundo das regi√µes\n",
        "ax.axvspan(primer_right_start + 0.5, primer_right_end + 0.5, color='brown', alpha=0.2)\n",
        "\n",
        "# linhas verticais pontilhadas\n",
        "for pos in [primer_right_start + 0.5, primer_right_end + 0.5]:\n",
        "    ax.axvline(x=pos, color='black', linestyle='--', linewidth=1)\n",
        "\n",
        "# Stem plot por base/\n",
        "for i, base in enumerate(seq):\n",
        "    # ax.stem([i+1], [bases[base]], linefmt=base_colors[base], markerfmt='', basefmt=\" \")\n",
        "    markerline, stemlines, baseline = ax.stem([i+1], [bases[base]], linefmt=base_colors[base], markerfmt='', basefmt=\" \")\n",
        "    plt.setp(stemlines, linewidth=5)\n",
        "\n",
        "\n",
        "# Labels das regi√µes no topo\n",
        "ax.text(jusante_start + len(jusante)/2, 5.6, f\"Jusante ({len(jusante)})\", ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Labels dos primers acima dos ret√¢ngulos dos primers\n",
        "ax.text(primer_right_start + (primer_right_end-primer_right_start)/2, 5.1, f\"Primer Right ({len(primer_forward)})\", ha='center', fontsize=12, fontweight='bold', color='brown')\n",
        "\n",
        "# Primer Right\n",
        "primer_y_bottom = 4.7\n",
        "primer_y_top = 5\n",
        "ax.fill_betweenx([primer_y_bottom, primer_y_top], primer_right_start + 0.5, primer_right_end + 0.5, color='brown', alpha=0.6)\n",
        "\n",
        "# Legenda do primer_forward (ap√≥s o bloco)\n",
        "ax.text(primer_right_end + 3, primer_y_top -0.2, f\"{primer_sel['primer_right']}\",\n",
        "        ha='left', va='center', fontname='monospace', fontsize=12, color='brown')\n",
        "if primer_sel['primer_right'] == primer_sel['primer_right_loc']:\n",
        "  cor = 'brown'\n",
        "else:\n",
        "  cor = 'red'\n",
        "ax.text(primer_right_end + 3, primer_y_top -0.5, f\"{primer_sel['primer_right_loc']}\", # primer_right_loc_direta\n",
        "        ha='left', va='center', fontname='monospace', fontsize=12, color=f'{cor}')\n",
        "\n",
        "# Ajustes visuais\n",
        "ax.set_ylim(0,6)\n",
        "ax.set_yticks([1,2,3,4])\n",
        "ax.set_yticklabels(['A','T','G','C'])\n",
        "\n",
        "xticks_personalizados = [0,\n",
        "                         jusante_start,\n",
        "                         primer_right_start,\n",
        "                         primer_right_end,\n",
        "                         jusante_end]\n",
        "\n",
        "ax.set_xticks(xticks_personalizados)\n",
        "ax.set_xticklabels([''] * len(xticks_personalizados))  # oculta os r√≥tulos padr√£o\n",
        "\n",
        "# Ajusta o offset na da posicao na sequencia original (x_tics)\n",
        "for i, tick in enumerate(xticks_personalizados):\n",
        "    label = f\"{int(tick + offset):,}\".replace(\",\", \".\")\n",
        "    y_offset = -0.04\n",
        "    ax.text(tick, y_offset, label,\n",
        "            ha='center', va='top',\n",
        "            transform=ax.get_xaxis_transform(),\n",
        "            fontsize=10, color='black')\n",
        "\n",
        "ax.set_xlabel(f\"Posi√ß√£o das por√ß√µes destacadas na sequ√™ncia FASTA original da amostra {origem}\", labelpad=33, fontsize=12, fontweight='bold')\n",
        "ax.tick_params(axis='x', labelsize=14, width=2, length=8)\n",
        "\n",
        "ax.set_ylabel(f\"Nucleot√≠deos\", labelpad=5, fontsize=12, fontweight='bold')\n",
        "ax.tick_params(axis='y', labelsize=14, width=2, length=8)\n",
        "\n",
        "base_colors = {'A': 'red', 'C': 'orange', 'T': 'blue', 'G': 'green'}\n",
        "legend_bases = [Patch(color=base_colors[base], label=base) for base in ['A', 'T', 'G', 'C']]\n",
        "# legend_total = legend_bases + legend_areas\n",
        "ax.legend(handles=legend_bases , title=\"Bases\", loc='upper left')\n",
        "\n",
        "\n",
        "arquivo_png = \"/content/drive/MyDrive/pesquisa/desenho_primer_right_jusante.png\"\n",
        "plt.savefig(arquivo_png, bbox_inches='tight', pad_inches=0, dpi=300)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybV_jr0JrfBZ"
      },
      "outputs": [],
      "source": [
        "# PASSO 11 - Conferencia dos amplicons - BLASTn - gera arquivos BLASTN\n",
        "# BLASTn -> Basic Local Alignment Search Tool for nucleotides.\n",
        "# BLASTN com sleep para evitar sobrecarga no servidor do blastn\n",
        "#    => com op√ß√µes de restart e/ou parada for√ßada\n",
        "# 16-09-2025 17-09-2025 18-09-2025 index: indice do arquivo FIM lido\n",
        "!pip install biopython\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "from Bio.Blast import NCBIWWW, NCBIXML\n",
        "# Bio.Blast.email = \"lucio.martins@gmail.com\"\n",
        "\n",
        "restart_pos = -1 # -1\n",
        "parada_forcada = -1\n",
        "\n",
        "total_alinhamentos = 0\n",
        "\n",
        "arquivo_selecionados = pasta_analise + 'FIM_full.csv'\n",
        "df_selecionados = pd.read_csv(arquivo_selecionados, delimiter = \";\")\n",
        "print(f\"> lido: {arquivo_selecionados} ({len(df_selecionados)})\")\n",
        "\n",
        "for index, row in df_selecionados.iterrows():\n",
        "  if parada_forcada != -1 and index > parada_forcada:\n",
        "    print(f\"  > parada for√ßada: {parada_forcada}\")\n",
        "    break\n",
        "# ============================\n",
        "  amplicon = row['amplicon']\n",
        "# ============================\n",
        "  origem = row['origem']\n",
        "  tipo = row['tipo']\n",
        "  sequencial_primer = row['sequencial']\n",
        "\n",
        "  if restart_pos != -1:\n",
        "    if index == restart_pos:\n",
        "      print(f' >> FIM RESTART {restart_pos} index: {index}')\n",
        "      restart_pos = -1\n",
        "    else:\n",
        "      print(f' >> index: {index} - n√£o processa - RESTART em {restart_pos}')\n",
        "      continue\n",
        "\n",
        "  data1 = datetime.now()\n",
        "  print(f\"  > index processamento: {index} - {index + 1} de {len(df_selecionados)} origem: {origem} tipo: {tipo} sequencial_primer: {sequencial_primer} tamanho: {len(amplicon)} - in√≠cio: {data1}\")\n",
        "  print(f\"    > BlastN - processar amplicon (index: {index}): {amplicon}\")\n",
        "\n",
        "  try:\n",
        "    # execu√ß√£o b√°sica do blast\n",
        "    # result_handle = NCBIWWW.qblast(\"blastn\", \"nt\", amplicon)\n",
        "\n",
        "    # execu√ß√£o avan√ßada do blast\n",
        "    result_handle = NCBIWWW.qblast(\n",
        "        program=\"blastn\",\n",
        "        database=\"nt\",\n",
        "        sequence=amplicon,\n",
        "        expect=10.0,\n",
        "        hitlist_size=50,\n",
        "        gapcosts=\"5 2\",\n",
        "        word_size=11,\n",
        "        filter=\"L\",\n",
        "        alignments=50,\n",
        "        format_type=\"XML\",\n",
        "        # aki testando as configuracoes\n",
        "        # https://www.ncbi.nlm.nih.gov/datasets/taxonomy/5061/\n",
        "        # entrez_query = \"txid5061[Organism]\"\n",
        "        # entrez_query = \"Aspergillus niger (taxid:5061)[Organism] Whole-genome shotgun contigs (wgs)[database]\"\n",
        "        # entrez_query = \"wgs[Filter]\"\n",
        "        entrez_query = \"wgs[filter]\"\n",
        "        # entrez_query = \"txid5061[Organism] AND wgs[Filter]\"\n",
        "        # entrez_query=\"Aspergillus[Organism] AND wgs[Filter]\"\n",
        "        # entrez_query=\"Aspergillus niger [Organism] AND wgs [Filter]\"\n",
        "        # entrez_query = \"Aspergillus niger [Organism] AND Ascomycota [Organism]\"\n",
        "    )\n",
        "\n",
        "    # resultado do blast\n",
        "    blast_records = NCBIXML.read(result_handle)\n",
        "\n",
        "    data2 = datetime.now()\n",
        "    print(f\"    > BlastN Processado - {data2} ({data2 - data1})\")\n",
        "\n",
        "    sequencial_alinhamento = 0\n",
        "    alinhamentos = []\n",
        "    for alignment in blast_records.alignments:\n",
        "      for hsp in alignment.hsps:\n",
        "        linha_completa = row.to_dict()\n",
        "        linha_completa['sequencial_alinhamento'] = sequencial_alinhamento\n",
        "        linha_completa['accession_number'] = alignment.accession\n",
        "        linha_completa['alignment_title'] = alignment.title\n",
        "        linha_completa['alignment_hit_id'] = alignment.hit_id\n",
        "        linha_completa['alignment_hit_def'] = alignment.hit_def\n",
        "        linha_completa['alignment_length'] = alignment.length\n",
        "        linha_completa['score'] = hsp.score\n",
        "        linha_completa['bits'] = hsp.bits\n",
        "        linha_completa['expect'] = hsp.expect\n",
        "        linha_completa['query'] = hsp.query\n",
        "        linha_completa['match'] = hsp.match\n",
        "        linha_completa['sbjct'] = hsp.sbjct\n",
        "        linha_completa['query_start'] = hsp.query_start\n",
        "        linha_completa['query_end'] = hsp.query_end\n",
        "        linha_completa['sbjct_start'] = hsp.sbjct_start\n",
        "        linha_completa['sbjct_end'] = hsp.sbjct_end\n",
        "        linha_completa['identities'] = hsp.identities\n",
        "        linha_completa['positives'] = hsp.positives\n",
        "        linha_completa['gaps'] = hsp.gaps\n",
        "        linha_completa['align_length'] = hsp.align_length\n",
        "        linha_completa['percent_identity'] = (hsp.identities / hsp.align_length) * 100\n",
        "\n",
        "        sequencial_alinhamento = sequencial_alinhamento + 1\n",
        "        alinhamentos.append(linha_completa)\n",
        "\n",
        "\n",
        "    texto = f'      > total de alinhamentos: {len(alinhamentos)}'\n",
        "    if len(alinhamentos) == 0:\n",
        "      texto += \" -> sem alinhamentos para este amplicon\"\n",
        "    print(texto)\n",
        "    if len(alinhamentos) > 0:\n",
        "      df_alinhamentos = pd.DataFrame(alinhamentos)\n",
        "      url_arquivo = pasta_analise + f'{origem}_BLASTN_{tipo}-{index}.csv'\n",
        "      df_alinhamentos.to_csv(url_arquivo, sep=';', index=False)\n",
        "      print(f\"        ‚ö°‚ö°‚ö°\");\n",
        "      print(f'      > gravado: {url_arquivo} ({len(df_alinhamentos)})')\n",
        "    else:\n",
        "      print(f'99999 > sem alinhamentos para este amplicon')\n",
        "\n",
        "    time.sleep(10)  # espera 10 segundos entre requisi√ß√µes\n",
        "  except Exception as e:\n",
        "    print(f\"   >>> Erro na sequ√™ncia {index}: {e}\")\n",
        "    continue\n",
        "\n",
        "print(f'>>> fim')\n",
        "\n",
        "# Um HSP representa um alinhamento local entre duas sequ√™ncias ‚Äî a query (consulta) e a subject (alvo no banco de dados) ‚Äî que obteve uma pontua√ß√£o\n",
        "# elevada com base nos crit√©rios do BLAST. Ele √© o n√∫cleo do resultado do BLAST, mostrando onde h√° similaridade significativa entre as sequ√™ncias.\n",
        "\n",
        "# Caracter√≠sticas de um HSP:\n",
        "#     Pode conter mismatches (bases diferentes) e gaps (lacunas), mas ainda assim ter uma pontua√ß√£o alta.\n",
        "#     √â gerado a partir da extens√£o de uma correspond√™ncia inicial (seed) que o BLAST encontra.\n",
        "#     Cada alinhamento entre query e subject pode ter v√°rios HSPs, se houver m√∫ltiplas regi√µes semelhantes.\n",
        "\n",
        "# Percent Identity:  Representa a porcentagem de bases id√™nticas entre a sequ√™ncia de consulta (query) e a sequ√™ncia alvo (subject) dentro da regi√£o alinhada.\n",
        "# Outros campos importantes que complementam o alinhamento:\n",
        "#     Alignment Length: n√∫mero total de bases alinhadas (inclui matches, mismatches e gaps)\n",
        "#     E-value: probabilidade de o alinhamento ocorrer por acaso (quanto menor, melhor)\n",
        "#     Bit Score: qualidade do alinhamento (quanto maior, melhor)\n",
        "# Atributo\tDescri√ß√£o\n",
        "# score\tPontua√ß√£o bruta do alinhamento\n",
        "# bits\tBit score (pontua√ß√£o normalizada)\n",
        "# expect\tE-value (probabilidade de alinhamento por acaso)\n",
        "# query\tSequ√™ncia da consulta no alinhamento\n",
        "# match\tString de correspond√™ncia (`\t`, espa√ßo, etc.) entre query e subject\n",
        "# sbjct\tSequ√™ncia da base de dados (subject) no alinhamento\n",
        "# query_start\tPosi√ß√£o inicial da sequ√™ncia de consulta\n",
        "# query_end\tPosi√ß√£o final da sequ√™ncia de consulta\n",
        "# sbjct_start\tPosi√ß√£o inicial da sequ√™ncia alvo\n",
        "# sbjct_end\tPosi√ß√£o final da sequ√™ncia alvo\n",
        "# identities\tN√∫mero de bases id√™nticas no alinhamento\n",
        "# positives\tN√∫mero de posi√ß√µes com substitui√ß√µes conservadas (em BLASTP)\n",
        "# gaps\tN√∫mero de lacunas no alinhamento\n",
        "# align_length\tComprimento total do alinhamento\n",
        "\n",
        "# Caracter√≠stica\tmegablast\te blastn\n",
        "# = Foco principal: Sequ√™ncias muito semelhantes (intrasp√©cie) ->\tSequ√™ncias divergentes (interesp√©cie)\n",
        "# = Velocidade:\tMuito mais r√°pido ->Mais lento\n",
        "# = Sensibilidade:\tMenor (pode perder alinhamentos curtos ou com erros) ->Maior (detecta alinhamentos mais fracos)\n",
        "# = Tamanho da palavra: Grande (ex: 28 bp) ->Menor (ex: 11 bp)\n",
        "# = Ideal para: Genomas da mesma esp√©cie, alta identidade -> Genes conservados entre esp√©cies diferentes\n",
        "# = Aplica√ß√µes comuns:\tWGS, genomas de refer√™ncia -> Metagen√¥mica, amplicons curtos, variantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM8EPMQtXdB4"
      },
      "outputs": [],
      "source": [
        "# PASSO 11.1 - junta arquivos BLASTN\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "arquivos_primer = busca_arquivos('BLASTN')\n",
        "\n",
        "dados = []\n",
        "print(f'quantidade_arquivos: {len(arquivos_primer)}')\n",
        "for idx, arquivo in enumerate(arquivos_primer):\n",
        "  df_arq_primer = pd.read_csv(arquivo, delimiter = \";\")\n",
        "  print(f'  > lido: {arquivo} {len(df_arq_primer)}')\n",
        "  dados.append(df_arq_primer)\n",
        "  print(f'  > lido: {arquivo} {len(df_arq_primer)}')\n",
        "df = pd.concat(dados, ignore_index=True)\n",
        "url_arquivo = pasta_analise + 'RESULT.csv'\n",
        "df.to_csv(url_arquivo, sep=';', index=False)\n",
        "print(f'>>> FIM > gravado: {url_arquivo} ({len(df)})')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5dpJee-KGJc"
      },
      "outputs": [],
      "source": [
        "# PASSO 11.2 - visualizar blastN - ler o arquivo RESULT\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "url_arquivo_result = pasta_analise + 'RESULT.csv'\n",
        "df_arquivo_result = pd.read_csv(url_arquivo_result, delimiter = \";\")\n",
        "\n",
        "cols_result = [ 'alinhamento',\n",
        "                'alignment_title',\n",
        "                'alignment_hit_id',\n",
        "                # 'alignment_hit_def',\n",
        "                # 'alignment_length',\n",
        "                # 'score',\n",
        "                # 'bits',\n",
        "                # 'expect',\n",
        "                # 'query',\n",
        "                # 'match',\n",
        "                # 'sbjct',\n",
        "                # 'query_start',\n",
        "                # 'query_end',\n",
        "                # 'sbjct_start',\n",
        "                # 'sbjct_end',\n",
        "                # 'identities',\n",
        "                # 'positives',\n",
        "                # 'gaps',\n",
        "                # 'align_length',\n",
        "                'percent_identity']\n",
        "\n",
        "arquivo_result_fim = pd.DataFrame(df_arquivo_result, columns=cols_result)\n",
        "\n",
        "print(f\"{arquivo_result_fim}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85gr7BupoGba"
      },
      "outputs": [],
      "source": [
        "# Passo 11.3 - filtro arquivo RESULT -> RESULT_FILTRO\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "url_arquivo = pasta_analise + 'RESULT.csv'\n",
        "df = pd.read_csv(url_arquivo, delimiter = \";\")\n",
        "print(f'>>> lido: {url_arquivo} ({len(df)})')\n",
        "\n",
        "df_q = df.query('percent_identity > 89')\n",
        "\n",
        "arquivo_saida = pasta_analise + 'RESULT_FILTRO.csv'\n",
        "df_q.to_csv(arquivo_saida, sep=';', index=False)\n",
        "print(f'>>> FIM > gravado: {arquivo_saida} ({len(df_q)})')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIwVSiPtF64a"
      },
      "outputs": [],
      "source": [
        "# Executar chamada ao primer-blast web\n",
        "\n",
        "!pip install selenium\n",
        "!pip install selenium webdriver-manager\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "\n",
        "# Configurar o driver (certifique-se de ter o ChromeDriver instalado)\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# Abrir o site do Primer-BLAST\n",
        "driver.get(\"https://www.ncbi.nlm.nih.gov/tools/primer-blast/\")\n",
        "\n",
        "# Preencher os campos dos primers\n",
        "primer1 = \"ATCGTACGATCGTACG\"\n",
        "primer2 = \"CGTACGTAGCTAGCTA\"\n",
        "\n",
        "driver.find_element(By.ID, \"PRIMER1\").send_keys(primer1)\n",
        "driver.find_element(By.ID, \"PRIMER2\").send_keys(primer2)\n",
        "\n",
        "# Submeter o formul√°rio\n",
        "driver.find_element(By.ID, \"submit_button\").click()\n",
        "\n",
        "# Aguardar o carregamento dos resultados (at√© 2 minutos)\n",
        "try:\n",
        "    print(\"‚è≥ Aguardando resultados do Primer-BLAST...\")\n",
        "    WebDriverWait(driver, 120).until(\n",
        "        EC.presence_of_element_located((By.ID, \"content\"))\n",
        "    )\n",
        "    print(\"‚úÖ Resultados carregados com sucesso!\")\n",
        "\n",
        "    # Capturar o conte√∫do da p√°gina de resultados\n",
        "    html_resultado = driver.page_source\n",
        "\n",
        "    # Salvar o HTML em um arquivo local\n",
        "    with open(\"resultado_primerblast.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_resultado)\n",
        "    print(\"üìÑ Resultado salvo em 'resultado_primerblast.html'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Erro ao carregar os resultados:\", e)\n",
        "\n",
        "# Fechar o navegador\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE64s-rmb2rJ"
      },
      "outputs": [],
      "source": [
        "# inclui previsao de temperatura de melting e amostra\n",
        "# amostra sera usada em seguida para gerar o flankeamento\n",
        "\n",
        "import primer3\n",
        "\n",
        "arquivo = url_arquivo = pasta_analise + 'EXCLUSIVOS.csv'\n",
        "ds = pd.read_csv(arquivo, delimiter = \";\")\n",
        "ds['amostra'] = ds['unidade'] * ds['copias']\n",
        "ds['tm'] = ds.apply(lambda row: primer3.calc_tm(row['amostra']), axis=1)\n",
        "\n",
        "url_arquivo = pasta_analise + 'EXCLUSIVOS_DET.csv'\n",
        "ds.to_csv(url_arquivo, sep=';', index=False)\n",
        "print(url_arquivo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibe0FYYcZnvK"
      },
      "outputs": [],
      "source": [
        "# ler todos os arquivos\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "arquivos_primer = busca_arquivos('SELECT')\n",
        "\n",
        "dados = []\n",
        "print(f'quantidade_arquivos: {len(arquivos_primer)}')\n",
        "for idx, arquivo in enumerate(arquivos_primer):\n",
        "  df_arq_primer = pd.read_csv(arquivo, delimiter = \";\")\n",
        "  print(f'  > lido: {arquivo} {len(df_arq_primer)}')\n",
        "#   dados.append(df_arq_primer)\n",
        "#   print(f'  > lido: {arquivo} {len(df_arq_primer)}')\n",
        "# df = pd.concat(dados, ignore_index=True)\n",
        "# print(f'>>> fim')\n",
        "# df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMrQiuKmf9DA"
      },
      "outputs": [],
      "source": [
        "# apaga arquivos - exclui arquivos - deleta arquivos\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# arquivos_str = busca_arquivos('Alignment', extensao='xml', ordenar_por=\"nome\", reverso=True)\n",
        "arquivos_str = busca_arquivos('BLASTN', extensao='csv', ordenar_por=\"nome\", reverso=True)\n",
        "\n",
        "print(f'quantidade_arquivos: {len(arquivos_str)}')\n",
        "\n",
        "for idx_arquivo, arquivo in enumerate(arquivos_str):\n",
        "  if os.path.exists(arquivo):\n",
        "    timestamp_criacao = os.path.getctime(arquivo)\n",
        "    data_hora_criacao = datetime.datetime.fromtimestamp(timestamp_criacao)\n",
        "    print(f'{arquivo} {data_hora_criacao}')\n",
        "    # os.remove(arquivo)\n",
        "    # df = pd.read_csv(arquivo, delimiter = \";\")\n",
        "    # print(f'{arquivo} ({len(df)})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtv4CRE7cp2A"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dados simulados de alinhamentos BLASTn\n",
        "alinhamentos = [\n",
        "    {\n",
        "        \"sample\": \"Ladder\",\n",
        "        \"match\": \"|\" * 50,  # bandas padr√£o\n",
        "        \"bands\": [100, 200, 300, 400, 500]  # posi√ß√µes fixas\n",
        "    },\n",
        "    {\n",
        "        \"sample\": \"Sample 1\",\n",
        "        \"match\": \"||||||||||||||||||||||||||||||||\",\n",
        "    },\n",
        "    {\n",
        "        \"sample\": \"Sample 2\",\n",
        "        \"match\": \"|||||| ||||||| |||||| |||||||||\",\n",
        "    },\n",
        "    {\n",
        "        \"sample\": \"Sample 3\",\n",
        "        \"match\": \"||||| ||||||| |||||| ||||||||||\",\n",
        "    },\n",
        "    {\n",
        "        \"sample\": \"Sample 4\",\n",
        "        \"match\": \"|||||||   ||||||||||            \",\n",
        "    },\n",
        "    {\n",
        "        \"sample\": \"Sample 5\",\n",
        "        \"match\": \"||||||||||||||||||||            \",\n",
        "    },\n",
        "]\n",
        "\n",
        "# Criar figura\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.set_facecolor('black')\n",
        "\n",
        "# Desenhar bandas\n",
        "for i, aln in enumerate(alinhamentos):\n",
        "    x = i * 1.5\n",
        "    sample = aln[\"sample\"]\n",
        "\n",
        "    if sample == \"Ladder\":\n",
        "        for band in aln[\"bands\"]:\n",
        "            ax.add_patch(plt.Rectangle((x, band), 1.0, 10, color='white'))\n",
        "            ax.text(x + 0.5, band + 15, f'{band} bp', ha='center', color='white', fontsize=8)\n",
        "    else:\n",
        "        match_count = aln[\"match\"].count('|')\n",
        "        align_len = len(aln[\"match\"])\n",
        "        brilho = match_count / align_len\n",
        "        cor = (brilho, brilho, brilho)\n",
        "        band_pos = match_count * 10  # escala fict√≠cia\n",
        "\n",
        "        ax.add_patch(plt.Rectangle((x, band_pos), 1.0, 10, color=cor))\n",
        "        ax.text(x + 0.5, band_pos + 15, f'{match_count} bp', ha='center', color='white', fontsize=8)\n",
        "\n",
        "    ax.text(x + 0.5, 20, sample, ha='center', color='white', fontsize=10)\n",
        "\n",
        "# Est√©tica\n",
        "ax.set_xlim(-0.5, len(alinhamentos) * 1.5)\n",
        "ax.set_ylim(0, 600)\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([100, 200, 300, 400, 500])\n",
        "ax.set_yticklabels([f'{y} bp' for y in [100, 200, 300, 400, 500]], color='white')\n",
        "ax.set_title(\"üß¨ Simula√ß√£o de Gel PCR com Refer√™ncia e Alinhamentos\", fontsize=14, color='white')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRehr90apDqe"
      },
      "outputs": [],
      "source": [
        "#  Reuniao 20-10-2025 - BLASTN\n",
        "!pip install biopython\n",
        "\n",
        "path_file = pasta_analise + 'RESULT_FILTRO.csv'\n",
        "df = pd.read_csv(path_file, delimiter = \";\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXtYjveWqbKT"
      },
      "outputs": [],
      "source": [
        "df_filtro = df\n",
        "df_filtro['align_length'] = pd.to_numeric(df_filtro['align_length'], errors='coerce')\n",
        "\n",
        "# df_filtro = df.query('percent_identity == 100 & alignment_length > 199')\n",
        "df_filtro = df.query('align_length > 199.99')\n",
        "print(len(df_filtro))\n",
        "# df_filtro['align_length']\n",
        "# df_filtro.iloc[0]['amplicon']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMhgXMg4OXBC"
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b4mClbKnAmg"
      },
      "outputs": [],
      "source": [
        "#  Reuniao 20-10-2025 - BLASTN\n",
        "!pip install biopython\n",
        "# Database: Whole-genome shotgun contigs (wgs)\n",
        "# Organism: Aspergillus niger (taxid:5061)\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "from Bio.Blast import NCBIWWW, NCBIXML\n",
        "# Bio.Blast.email = \"lucio.martins@gmail.com\"\n",
        "\n",
        "# =====================#\n",
        "row = df_filtro.iloc[23]\n",
        "# =====================#\n",
        "\n",
        "sequencia_teste = row['amplicon']\n",
        "sequencia_teste = 'CGCTAGCGGTAGGAACAGCAGGGACAGAGCCGGACGGACCGGCGGGAACAACGCTACCAGAGGGAGCAGCACCTCCAGAGCCAGAGCCAGAGCCGGACCAGGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCAGAGCCGCCAGTAGAACCACCAGTAGAGCCGCCAGCACCACCACCGACGATGGGGGTAACGCTAGGAGCACCAGACTGAGCAGGGGCACTAGGAGCAGAACCGCCCTCGGTCATAGTAGCAACGGGAACAACAGTCTCGCCGGTGATAGTGTCGCAGGGAGTAGTGGAGGCACCGGCACTGGCGCCTTCCGGAACAGAGGCAG'\n",
        "print(sequencia_teste)\n",
        "\n",
        "data1 = datetime.now()\n",
        "\n",
        "# execu√ß√£o b√°sica do blast\n",
        "\n",
        "# execu√ß√£o avan√ßada do blast\n",
        "# result_handle = NCBIWWW.qblast(\"blastn\", \"nt\", sequencia_teste)\n",
        "result_handle = NCBIWWW.qblast(\n",
        "    program=\"blastn\", # ‚Üí usa megablast por padr√£o\n",
        "    database=\"nt\",\n",
        "    hitlist_size=200,\n",
        "    # expect=0.001,\n",
        "    sequence=sequencia_teste,\n",
        "    # aki testando as configuracoes\n",
        "    # https://www.ncbi.nlm.nih.gov/datasets/taxonomy/5061/\n",
        "    entrez_query = \"txid5061[Organism]\"\n",
        "    # entrez_query = \"txid5061[Organism]\"\n",
        "    # entrez_query = \"wgs[Filter]\"\n",
        "    # entrez_query = \"txid5061[Organism] AND wgs[Filter]\"\n",
        "    # entrez_query=\"Aspergillus[Organism] AND wgs[Filter]\"\n",
        "    # entrez_query=\"Aspergillus niger [Organism] AND wgs [Filter]\"\n",
        "    # entrez_query = \"Aspergillus niger [Organism] AND Ascomycota [Organism]\"\n",
        ")\n",
        "\n",
        "# resultado do blast\n",
        "blast_records = NCBIXML.read(result_handle)\n",
        "\n",
        "data2 = datetime.now()\n",
        "print(f\"> BlastN Processado - {data2} ({data2 - data1})\")\n",
        "\n",
        "sequencial_alinhamento = 0\n",
        "alinhamentos = []\n",
        "for alignment in blast_records.alignments:\n",
        "  for hsp in alignment.hsps:\n",
        "    linha_completa = row.to_dict()\n",
        "    linha_completa['sequencial_alinhamento'] = sequencial_alinhamento\n",
        "    linha_completa['accession_number'] = alignment.accession\n",
        "    linha_completa['alignment_title'] = alignment.title\n",
        "    linha_completa['alignment_hit_id'] = alignment.hit_id\n",
        "    linha_completa['alignment_hit_def'] = alignment.hit_def\n",
        "    linha_completa['alignment_length'] = alignment.length\n",
        "    linha_completa['score'] = hsp.score\n",
        "    linha_completa['bits'] = hsp.bits\n",
        "    linha_completa['expect'] = hsp.expect\n",
        "    linha_completa['query'] = hsp.query\n",
        "    linha_completa['match'] = hsp.match\n",
        "    linha_completa['sbjct'] = hsp.sbjct\n",
        "    linha_completa['query_start'] = hsp.query_start\n",
        "    linha_completa['query_end'] = hsp.query_end\n",
        "    linha_completa['sbjct_start'] = hsp.sbjct_start\n",
        "    linha_completa['sbjct_end'] = hsp.sbjct_end\n",
        "    linha_completa['identities'] = hsp.identities\n",
        "    linha_completa['positives'] = hsp.positives\n",
        "    linha_completa['gaps'] = hsp.gaps\n",
        "    linha_completa['align_length'] = hsp.align_length\n",
        "    linha_completa['percent_identity'] = (hsp.identities / hsp.align_length) * 100\n",
        "\n",
        "    sequencial_alinhamento = sequencial_alinhamento + 1\n",
        "    alinhamentos.append(linha_completa)\n",
        "texto = f'> total de alinhamentos: {len(alinhamentos)}'\n",
        "if len(alinhamentos) == 0:\n",
        "  texto += \"* sem alinhamentos para este sequencia_teste\"\n",
        "print(texto)\n",
        "if len(alinhamentos) > 0:\n",
        "  df_alinhamentos = pd.DataFrame(alinhamentos)\n",
        "  dicionario = df_amostras.set_index(\"id\")[\"Cepa (strain)\"].to_dict()\n",
        "  df_alinhamentos['cepa'] = df_alinhamentos['origem'].map(dicionario)\n",
        "\n",
        "  url_arquivo = f'{pasta_analise}AVALIACAO_1.csv'\n",
        "  df_alinhamentos.to_csv(url_arquivo, sep=';', index=False)\n",
        "  print(f'  > gravado: {url_arquivo} ({len(df_alinhamentos)})')\n",
        "else:\n",
        "  print(f'99999 > sem alinhamentos para este sequencia_teste')\n",
        "print(f'>>> fim')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzyvcmr6f6io"
      },
      "outputs": [],
      "source": [
        "url_arquivo = f'{pasta_analise}AVALIACAO_1.csv'\n",
        "df_alinhamentos = pd.read_csv(url_arquivo, delimiter = \";\")\n",
        "\n",
        "# colunas = ['cepa', 'origem', 'unidade', 'copias', 'align_length', 'alignment_hit_id', 'alignment_title', 'parte_localizada', 'percent_identity']\n",
        "colunas = ['cepa', 'origem', 'unidade', 'copias', 'align_length', 'alignment_hit_id', 'alignment_title']\n",
        "# df_query = df_alinhamentos.query('percent_identity > 90')\n",
        "# df_query[colunas]\n",
        "df_alinhamentos[colunas]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3he5HToeIS82"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def numero_inteiro_encontrado(row):\n",
        "    cepa = str(row['cepa'])\n",
        "    titulo = str(row['alignment_title']).lower()\n",
        "\n",
        "    # Extrai todos os n√∫meros da cepa (inteiros e decimais)\n",
        "    numeros = re.findall(r'\\d+(?:\\.\\d+)?', cepa)\n",
        "\n",
        "    # Mant√©m apenas a parte inteira de cada n√∫mero\n",
        "    partes_inteiras = [num.split('.')[0] for num in numeros]\n",
        "\n",
        "    # Verifica se alguma parte inteira est√° no t√≠tulo\n",
        "    for parte in partes_inteiras:\n",
        "        if parte in titulo:\n",
        "            return parte  # Retorna a primeira parte encontrada\n",
        "\n",
        "    return None  # Se nada for encontrado\n",
        "\n",
        "# df_alinhamentos.at[0, 'alignment_title'] += df_alinhamentos.at[0, 'cepa'] # teste - for√ßa achado\n",
        "# Aplicar ao DataFrame\n",
        "df_alinhamentos['parte_localizada'] = df_alinhamentos.apply(numero_inteiro_encontrado, axis=1)\n",
        "\n",
        "# Filtrar registros com correspond√™ncia\n",
        "df_com_match_num = df_alinhamentos[df_alinhamentos['parte_localizada'].notna()]\n",
        "\n",
        "\n",
        "df_com_match_num[colunas]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mG-8JvIBGkfu"
      },
      "outputs": [],
      "source": [
        "def palavra_cepa_encontrada(row):\n",
        "    cepa = str(row['cepa']).lower().strip()\n",
        "    titulo = str(row['alignment_title']).lower().strip()\n",
        "\n",
        "    # Divide a cepa em palavras e verifica se alguma est√° no t√≠tulo\n",
        "    for palavra in cepa.split():\n",
        "        if palavra in titulo:\n",
        "            return palavra  # Retorna a primeira palavra encontrada\n",
        "    return None  # Nenhuma correspond√™ncia\n",
        "\n",
        "# Aplica a fun√ß√£o ao DataFrame\n",
        "df_alinhamentos_pos = pd.DataFrame(df_alinhamentos)\n",
        "df_alinhamentos_pos['match_cepa'] = df_alinhamentos_pos.apply(palavra_cepa_encontrada, axis=1)\n",
        "\n",
        "# Filtra os registros com correspond√™ncia\n",
        "df_com_match = df_alinhamentos_pos[df_alinhamentos_pos['match_cepa'].notna()]\n",
        "\n",
        "# Exibe apenas as colunas desejada\n",
        "colunas = ['cepa', 'origem', 'unidade', 'copias', 'align_length', 'alignment_hit_id', 'alignment_title', 'match_cepa', 'percent_identity']\n",
        "df_com_match[colunas]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEx0P1UZFiPs"
      },
      "outputs": [],
      "source": [
        "# reunir os arquivos de imagem em diretorio especifico\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "arquivos_str = busca_arquivos('schema', extensao='png', ordenar_por=\"nome\", reverso=True)\n",
        "print(f'quantidade_arquivos: {len(arquivos_str)}')\n",
        "\n",
        "for idx_arquivo, arquivo in enumerate(arquivos_str):\n",
        "    origem = os.path.basename(arquivo).split('_')[0]\n",
        "    pasta_imagens = os.path.join(pasta_analise, 'imagens', origem)\n",
        "    print(f'pasta_imagens: {pasta_imagens}')\n",
        "\n",
        "    if not os.path.exists(pasta_imagens):\n",
        "        os.makedirs(pasta_imagens)\n",
        "\n",
        "    shutil.move(arquivo, pasta_imagens)\n",
        "print('>>> fim')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Mpyx1l2ru9w"
      },
      "outputs": [],
      "source": [
        "colunas = ['cepa', 'origem', 'unidade', 'copias', 'align_length', 'alignment_hit_id', 'alignment_title', 'parte_localizada', 'percent_identity']\n",
        "df_query = df_alinhamentos.query('percent_identity == 100')\n",
        "df_query[colunas]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNm4grsLLlL9"
      },
      "outputs": [],
      "source": [
        "df_alinhamentos.at[0, 'alignment_title'] += df_alinhamentos.at[0, 'cepa']\n",
        "df_alinhamentos.iloc[0]['alignment_title']"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}